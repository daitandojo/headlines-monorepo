# ðŸ“ PROJECT DIRECTORY STRUCTURE

Total: 54 files, 10 directories

```
headlines/
â”œâ”€â”€ ðŸ“ src/
â”‚   â”œâ”€â”€ ðŸ“ chains/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ articleChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ articlePreAssessmentChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ batchHeadlineChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ clusteringChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ contactFinderChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ contactResolverChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ countryCorrectionChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ disambiguationChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emailIntroChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emailSubjectChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ entityCanonicalizerChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ entityExtractorChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ executiveSummaryChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ headlineChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ index.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ judgeChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ opportunityChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ sectionClassifierChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ selectorRepairChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ synthesisChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ translateChain.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ watchlistSuggestionChain.js
â”‚   â”œâ”€â”€ ðŸ“ embeddings/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ embeddings.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ vectorSearch.js
â”‚   â”œâ”€â”€ ðŸ“ lib/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ AIAgent.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ langchain.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ safeInvoke.js
â”‚   â”œâ”€â”€ ðŸ“ node/
â”‚   â”‚   â””â”€â”€ ðŸ“ agents/
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ articleAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ articlePreAssessmentAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ batchArticleAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ clusteringAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ headlineAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ judgeAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ sectionClassifierAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ selectorRepairAgent.js
â”‚   â”‚       â””â”€â”€ ðŸ“„ watchlistAgent.js
â”‚   â”œâ”€â”€ ðŸ“ rag/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ generation.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ orchestrator.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ planner.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ prompts.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ retrieval.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ validation.js
â”‚   â”œâ”€â”€ ðŸ“ search/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ search.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ serpapi.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ wikipedia.js
â”‚   â”œâ”€â”€ ðŸ“ shared/
â”‚   â”‚   â””â”€â”€ ðŸ“ agents/
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ contactAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ emailAgents.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ entityAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ executiveSummaryAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ opportunityAgent.js
â”‚   â”‚       â””â”€â”€ ðŸ“„ synthesisAgent.js
â”‚   â”œâ”€â”€ ðŸ“„ index.js
â”‚   â””â”€â”€ ðŸ“„ next.js
â””â”€â”€ ðŸ“„ package.json
```

# ðŸ“‹ PROJECT METADATA

**Generated**: 2025-10-05T19:41:33.273Z
**Repository Path**: /home/mark/Repos/projects/headlines/packages/ai-services
**Total Files**: 54
**Package**: @headlines/ai-services@1.0.0
**Description**: Centralized, LangChain-powered AI and external service logic.



---


## ðŸ“„ package.json
*Lines: 36, Size: 973 Bytes*

```json
{
  "name": "@headlines/ai-services",
  "version": "1.0.0",
  "description": "Centralized, LangChain-powered AI and external service logic.",
  "main": "src/index.js",
  "type": "module",
  "license": "ISC",
  "exports": {
    ".": "./src/index.js",
    "./node": "./src/index.js",
    "./next": "./src/next.js"
  },
  "dependencies": {
    "@headlines/config": "workspace:*",
    "@headlines/models": "workspace:*",
    "@headlines/prompts": "workspace:*",
    "@headlines/utils-server": "workspace:*",
    "@headlines/utils-shared": "workspace:*",
    "@langchain/community": "*",
    "@langchain/core": "*",
    "@langchain/openai": "*",
    "@langchain/pinecone": "*",
    "@pinecone-database/pinecone": "^2.2.2",
    "@xenova/transformers": "^2.17.2",
    "axios": "^1.7.2",
    "langchain": "*",
    "mongoose": "^8.19.0",
    "newsapi": "^2.4.1",
    "openai": "^5.22.0",
    "p-limit": "^5.0.0",
    "serpapi": "^2.1.0",
    "sharp": "0.33.4",
    "zod": "*"
  }
}

```

## ðŸ“„ src/chains/articleChain.js
*Lines: 64, Size: 2.12 KB*

```javascript
// packages/ai-services/src/chains/articleChain.js (version 2.0.0)
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { RunnableSequence } from '@langchain/core/runnables'
import { settings } from '@headlines/config'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { logger } from '@headlines/utils-shared'
// REFACTOR: Import Zod schema from the new canonical location.
import { articleAssessmentSchema } from '@headlines/models/schemas'

const instructions = getInstructionArticle(settings)
const systemPrompt = [
  instructions.whoYouAre,
  instructions.whatYouDo,
  instructions.primaryMandate,
  instructions.analyticalFramework,
  instructions.scoring,
  instructions.outputFormatDescription,
  instructions.reiteration,
].join('\n\n')

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputArticle.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputArticle[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{article_text}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

async function invoke(input) {
  const result = await safeInvoke(chain, input, 'articleChain', articleAssessmentSchema)
  if (result.error) return result
  if (result.key_individuals?.length > 0) {
    const articleTextLower = input.article_text.toLowerCase()
    result.key_individuals = result.key_individuals.filter((ind) => {
      if (!ind.name) return false
      const isPresent = ind.name
        .split(' ')
        .filter((p) => p.length > 2)
        .some((p) => articleTextLower.includes(p.toLowerCase()))
      if (!isPresent)
        logger.warn({ individual: ind.name }, 'Discarding hallucinated key individual.')
      return isPresent
    })
  }
  return result
}

export const articleChain = { invoke }

```

## ðŸ“„ src/chains/articlePreAssessmentChain.js
*Lines: 31, Size: 1.21 KB*

```javascript
// packages/ai-services/src/chains/articlePreAssessmentChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionArticlePreAssessment } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { articlePreAssessmentSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionArticlePreAssessment.whoYouAre,
  instructionArticlePreAssessment.whatYouDo,
  instructionArticlePreAssessment.classificationFramework,
  instructionArticlePreAssessment.outputFormatDescription,
  instructionArticlePreAssessment.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{input}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const articlePreAssessmentChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'articlePreAssessmentChain', articlePreAssessmentSchema),
}

```

## ðŸ“„ src/chains/batchHeadlineChain.js
*Lines: 33, Size: 1.37 KB*

```javascript
// packages/ai-services/src/chains/batchHeadlineChain.js (version 1.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionBatchHeadlineAssessment } from '@headlines/prompts'
import { getHeadlineModel } from '../lib/langchain.js' // Use the specific model for headlines
import { safeInvoke } from '../lib/safeInvoke.js'
import { batchHeadlineAssessmentSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionBatchHeadlineAssessment.whoYouAre,
  instructionBatchHeadlineAssessment.whatYouDo,
  instructionBatchHeadlineAssessment.primaryMandate,
  instructionBatchHeadlineAssessment.analyticalFramework,
  instructionBatchHeadlineAssessment.outputFormatDescription,
  instructionBatchHeadlineAssessment.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{headlines_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
// The safeInvoke function will be responsible for all parsing.
const chain = RunnableSequence.from([prompt, getHeadlineModel()])

export const batchHeadlineChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'batchHeadlineChain', batchHeadlineAssessmentSchema),
}

```

## ðŸ“„ src/chains/clusteringChain.js
*Lines: 29, Size: 1.05 KB*

```javascript
// packages/ai-services/src/chains/clusteringChain.js (version 3.2 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCluster } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { clusterSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionCluster.whoYouAre,
  instructionCluster.whatYouDo,
  ...instructionCluster.guidelines,
  instructionCluster.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{articles_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const clusteringChain = {
  invoke: (input) => safeInvoke(chain, input, 'clusteringChain', clusterSchema),
}

```

## ðŸ“„ src/chains/contactFinderChain.js
*Lines: 29, Size: 1.06 KB*

```javascript
// packages/ai-services/src/chains/contactFinderChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionContacts } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { findContactSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionContacts.whoYouAre,
  instructionContacts.whatYouDo,
  ...instructionContacts.guidelines,
  instructionContacts.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{snippets}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const contactFinderChain = {
  invoke: (input) => safeInvoke(chain, input, 'contactFinderChain', findContactSchema),
}

```

## ðŸ“„ src/chains/contactResolverChain.js
*Lines: 30, Size: 1.09 KB*

```javascript
// packages/ai-services/src/chains/contactResolverChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEnrichContact } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { enrichContactSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionEnrichContact.whoYouAre,
  instructionEnrichContact.whatYouDo,
  ...instructionEnrichContact.guidelines,
  instructionEnrichContact.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const contactResolverChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'contactResolverChain', enrichContactSchema),
}

```

## ðŸ“„ src/chains/countryCorrectionChain.js
*Lines: 40, Size: 1.98 KB*

```javascript
// packages/ai-services/src/chains/countryCorrectionChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { countryCorrectionSchema } from '@headlines/models/schemas'

const systemPrompt = `You are a data cleaning expert. Your sole task is to analyze a given text string that is supposed to represent a country and extract the single, correct, UN-recognized sovereign country name from it.

**CRITICAL INSTRUCTIONS:**
1.  Analyze the input string.
2.  Identify the most likely country. For example, "Denmark (Aarhus)" should be "Denmark". "London" should be "United Kingdom".
4.  Anything starting with "Central Europe" should be "Europe".
5.  "Denmark & Sweden" should be "Scandinavia"
6.  "International" should be "Global"
7. "Nordic Region" should be "Scandinavia" (also if followed by something between brackets)
8. "Pan-Europe" should be "Europe"
9. "Sweden & Norway" should be "Scandinavia"
10. "United States" should be "United States of America"
11. "UK" should be "United Kingdom"
12. anything starting with "Unknown" should simply be "Unknown"
13.  If a valid country name can be determined, return it.
14.  If the input is ambiguous or does not contain a clear country, you MUST return null.
15.  You MUST respond ONLY with a valid JSON object in this format: {{"country": "Correct Country Name"}} or {{"country": null}}`

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Location String: "{location_string}"'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const countryCorrectionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'countryCorrectionChain', countryCorrectionSchema),
}

```

## ðŸ“„ src/chains/disambiguationChain.js
*Lines: 28, Size: 1 KB*

```javascript
// packages/ai-services/src/chains/disambiguationChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionDisambiguation } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionDisambiguation.whoYouAre,
  instructionDisambiguation.whatYouDo,
  ...instructionDisambiguation.guidelines,
  instructionDisambiguation.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{inputText}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const disambiguationChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'disambiguationChain', disambiguationSchema),
}

```

## ðŸ“„ src/chains/emailIntroChain.js
*Lines: 30, Size: 1.12 KB*

```javascript
// packages/ai-services/src/chains/emailIntroChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailIntro } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailIntroSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionEmailIntro.whoYouAre,
  instructionEmailIntro.whatYouDo,
  ...instructionEmailIntro.guidelines,
  instructionEmailIntro.outputFormatDescription,
  instructionEmailIntro.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Client and Event Data: {payload_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const emailIntroChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailIntroChain', emailIntroSchema),
}

```

## ðŸ“„ src/chains/emailSubjectChain.js
*Lines: 30, Size: 1.13 KB*

```javascript
// packages/ai-services/src/chains/emailSubjectChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailSubject } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailSubjectSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionEmailSubject.whoYouAre,
  instructionEmailSubject.whatYouDo,
  ...instructionEmailSubject.guidelines,
  instructionEmailSubject.outputFormatDescription,
  instructionEmailSubject.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const emailSubjectChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailSubjectChain', emailSubjectSchema),
}

```

## ðŸ“„ src/chains/entityCanonicalizerChain.js
*Lines: 30, Size: 1.1 KB*

```javascript
// packages/ai-services/src/chains/entityCanonicalizerChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCanonicalizer } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { canonicalizerSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionCanonicalizer.whoYouAre,
  instructionCanonicalizer.whatYouDo,
  ...instructionCanonicalizer.guidelines,
  instructionCanonicalizer.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{entity_name}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const entityCanonicalizerChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'entityCanonicalizerChain', canonicalizerSchema),
}

```

## ðŸ“„ src/chains/entityExtractorChain.js
*Lines: 29, Size: 1.04 KB*

```javascript
// packages/ai-services/src/chains/entityExtractorChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEntity } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { entitySchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionEntity.whoYouAre,
  instructionEntity.whatYouDo,
  ...instructionEntity.guidelines,
  instructionEntity.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{article_text}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const entityExtractorChain = {
  invoke: (input) => safeInvoke(chain, input, 'entityExtractorChain', entitySchema),
}

```

## ðŸ“„ src/chains/executiveSummaryChain.js
*Lines: 32, Size: 1.24 KB*

```javascript
// packages/ai-services/src/chains/executiveSummaryChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionExecutiveSummary } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { executiveSummarySchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionExecutiveSummary.whoYouAre,
  instructionExecutiveSummary.whatYouDo,
  ...instructionExecutiveSummary.guidelines,
  instructionExecutiveSummary.outputFormatDescription,
  instructionExecutiveSummary.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Run Data: {payload_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
// The safeInvoke function will be responsible for all parsing.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const executiveSummaryChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'executiveSummaryChain', executiveSummarySchema),
}

```

## ðŸ“„ src/chains/headlineChain.js
*Lines: 86, Size: 2.8 KB*

```javascript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '@headlines/prompts' // Correct: Import from the monorepo package
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { headlineAssessmentSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node' // Correct: Import from the /node entry point

const systemPrompt = [
  instructionHeadlines.whoYouAre,
  instructionHeadlines.whatYouDo,
  instructionHeadlines.primaryMandate,
  instructionHeadlines.analyticalFramework,
  instructionHeadlines.outputFormatDescription,
].join('\n\n')

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputHeadlines.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputHeadlines[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{headlineWithContext}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

function prepareInput({ article, hits }) {
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`
  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }
  return { headlineWithContext }
}

async function invoke({ article, hits }) {
  const input = prepareInput({ article, hits })
  const result = await safeInvoke(chain, input, 'headlineChain', headlineAssessmentSchema)

  if (result.error) {
    return {
      relevance_headline: 0,
      assessment_headline: 'AI assessment failed.',
      headline_en: article.headline,
    }
  }

  const assessment = result.assessment?.[0]
  if (assessment && hits.length > 0) {
    let score = assessment.relevance_headline
    if (settings.WATCHLIST_SCORE_BOOST > 0) {
      score = Math.min(100, score + settings.WATCHLIST_SCORE_BOOST)
      assessment.assessment_headline = `Watchlist boost (+${settings.WATCHLIST_SCORE_BOOST}). ${assessment.assessment_headline}`
    }
    assessment.relevance_headline = score
  }

  return (
    assessment || {
      relevance_headline: 0,
      assessment_headline: 'AI assessment failed.',
      headline_en: article.headline,
    }
  )
}

export const headlineChain = { invoke }

```

## ðŸ“„ src/chains/index.js
*Lines: 46, Size: 2.95 KB*

```javascript
// packages/ai-services/src/chains/index.js (version 3.1 - Final)
import { articleChain as ac } from './articleChain.js'
import { articlePreAssessmentChain as apac } from './articlePreAssessmentChain.js'
import { clusteringChain as cc } from './clusteringChain.js'
import { contactFinderChain as cfc } from './contactFinderChain.js'
import { contactResolverChain as crc } from './contactResolverChain.js'
import { disambiguationChain as dc } from './disambiguationChain.js'
import { emailIntroChain as eic } from './emailIntroChain.js'
import { emailSubjectChain as esc } from './emailSubjectChain.js'
import { entityCanonicalizerChain as ecc } from './entityCanonicalizerChain.js'
import { entityExtractorChain as eec } from './entityExtractorChain.js'
import { executiveSummaryChain as exsc } from './executiveSummaryChain.js'
import { headlineChain as hc } from './headlineChain.js'
import { judgeChain as jc } from './judgeChain.js'
import { opportunityChain as oc } from './opportunityChain.js'
import { sectionClassifierChain as scc } from './sectionClassifierChain.js'
import { selectorRepairChain as src } from './selectorRepairChain.js'
import { synthesisChain as sc } from './synthesisChain.js'
import { watchlistSuggestionChain as wsc } from './watchlistSuggestionChain.js'
import { batchHeadlineChain as bhc } from './batchHeadlineChain.js'
import { translateChain as tc } from './translateChain.js'
import { countryCorrectionChain as ccc } from './countryCorrectionChain.js'

// DEFINITIVE FIX: Export each chain's invoke method as a standalone async function
export const articleChain = async (input) => ac.invoke(input)
export const articlePreAssessmentChain = async (input) => apac.invoke(input)
export const clusteringChain = async (input) => cc.invoke(input)
export const contactFinderChain = async (input) => cfc.invoke(input)
export const contactResolverChain = async (input) => crc.invoke(input)
export const disambiguationChain = async (input) => dc.invoke(input)
export const emailIntroChain = async (input) => eic.invoke(input)
export const emailSubjectChain = async (input) => esc.invoke(input)
export const entityCanonicalizerChain = async (input) => ecc.invoke(input)
export const entityExtractorChain = async (input) => eec.invoke(input)
export const executiveSummaryChain = async (input) => exsc.invoke(input)
export const headlineChain = async (input) => hc.invoke(input)
export const judgeChain = async (input) => jc.invoke(input)
export const opportunityChain = async (input) => oc.invoke(input)
export const sectionClassifierChain = async (input) => scc.invoke(input)
export const selectorRepairChain = async (input) => src.invoke(input)
export const synthesisChain = async (input) => sc.invoke(input)
export const watchlistSuggestionChain = async (input) => wsc.invoke(input)
export const batchHeadlineChain = async (input) => bhc.invoke(input)
export const translateChain = async (input) => tc.invoke(input)
export const countryCorrectionChain = async (input) => ccc.invoke(input)

```

## ðŸ“„ src/chains/judgeChain.js
*Lines: 30, Size: 1.06 KB*

```javascript
// packages/ai-services/src/chains/judgeChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionJudge } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { judgeSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionJudge.whoYouAre,
  instructionJudge.whatYouDo,
  ...instructionJudge.guidelines,
  instructionJudge.outputFormatDescription,
  instructionJudge.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Data for review: {payload_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const judgeChain = {
  invoke: (input) => safeInvoke(chain, input, 'judgeChain', judgeSchema),
}

```

## ðŸ“„ src/chains/opportunityChain.js
*Lines: 31, Size: 1.15 KB*

```javascript
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getInstructionOpportunities } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { opportunitySchema } from '@headlines/models/schemas'
// Correct: Import from the /node entry point for the pipeline/Node.js environment
import { settings } from '@headlines/config/node'

const instructions = getInstructionOpportunities(settings)
const systemPrompt = [
  instructions.whoYouAre,
  instructions.whatYouDo,
  ...instructions.guidelines,
  instructions.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_text}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const opportunityChain = {
  invoke: (input) => safeInvoke(chain, input, 'opportunityChain', opportunitySchema),
}

```

## ðŸ“„ src/chains/sectionClassifierChain.js
*Lines: 36, Size: 2.03 KB*

```javascript
// packages/ai-services/src/chains/sectionClassifierChain.js (version 2.3.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js' // Correctly import the getter function
import { safeInvoke } from '../lib/safeInvoke.js'
import { sectionClassifierSchema } from '@headlines/models/schemas'

const INSTRUCTION = {
  whoYouAre:
    'You are a master website navigation analyst. Your task is to analyze a list of hyperlinks (anchor text and href) from a webpage and classify each one into one of four categories.',
  guidelines: [
    '**Categories:**',
    '1.  **"news_section"**: A link to a major category or section of news (e.g., "Business", "Technology", "World News", "/erhverv", "/Ã¸konomi").',
    '2.  **"article_headline"**: A link to a specific news article or story. The text is usually a full sentence or a descriptive title.',
    '3.  **"navigation"**: A link to a functional page on the site (e.g., "About Us", "Contact", "Login").',
    '4.  **"other"**: Any other type of link (advertisements, privacy policies, etc.).',
    '**Instructions:**',
    '-   You will receive a JSON array of link objects.',
    '-   You MUST return a JSON object with a single key, "classifications".',
    '-   The "classifications" array MUST contain one classification object for EACH link in the input, in the EXACT SAME ORDER.',
  ],
}

const systemPrompt = [INSTRUCTION.whoYouAre, ...INSTRUCTION.guidelines].join('\n\n')
const fullPrompt = `${systemPrompt}\n\nUser Input:\n{links_json_string}`
const prompt = ChatPromptTemplate.fromTemplate(fullPrompt)
// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()]) // Call the getter function

export const sectionClassifierChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'sectionClassifierChain', sectionClassifierSchema),
}

```

## ðŸ“„ src/chains/selectorRepairChain.js
*Lines: 31, Size: 1.14 KB*

```javascript
// packages/ai-services/src/chains/selectorRepairChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSelectorRepair } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { selectorRepairSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionSelectorRepair.whoYouAre,
  instructionSelectorRepair.whatYouDo,
  ...instructionSelectorRepair.guidelines,
  instructionSelectorRepair.outputFormatDescription,
  instructionSelectorRepair.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{payload_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const selectorRepairChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'selectorRepairChain', selectorRepairSchema),
}

```

## ðŸ“„ src/chains/synthesisChain.js
*Lines: 29, Size: 1.06 KB*

```javascript
// packages/ai-services/src/chains/synthesisChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSynthesize } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { synthesisSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionSynthesize.whoYouAre,
  instructionSynthesize.whatYouDo,
  ...instructionSynthesize.guidelines,
  instructionSynthesize.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const synthesisChain = {
  invoke: (input) => safeInvoke(chain, input, 'synthesisChain', synthesisSchema),
}

```

## ðŸ“„ src/chains/translateChain.js
*Lines: 30, Size: 1.13 KB*

```javascript
// packages/ai-services/src/chains/translateChain.js (version 1.0.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionTranslate } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { translateSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionTranslate.whoYouAre,
  instructionTranslate.whatYouDo,
  ...instructionTranslate.guidelines,
  instructionTranslate.outputFormatDescription,
  instructionTranslate.reiteration,
].join('\\n\\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Target Language: {language}\\n\\nHTML Content:\\n```{html_content}```'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const translateChain = {
  invoke: (input) => safeInvoke(chain, input, 'translateChain', translateSchema),
}

```

## ðŸ“„ src/chains/watchlistSuggestionChain.js
*Lines: 31, Size: 1.21 KB*

```javascript
// packages/ai-services/src/chains/watchlistSuggestionChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionWatchlistSuggestion } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { watchlistSuggestionSchema } from '@headlines/models/schemas'

const systemPrompt = [
  instructionWatchlistSuggestion.whoYouAre,
  instructionWatchlistSuggestion.whatYouDo,
  ...instructionWatchlistSuggestion.guidelines,
  instructionWatchlistSuggestion.outputFormatDescription,
  instructionWatchlistSuggestion.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser is removed.
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const watchlistSuggestionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'watchlistSuggestionChain', watchlistSuggestionSchema),
}

```

## ðŸ“„ src/embeddings/embeddings.js
*Lines: 233, Size: 7.92 KB*

```javascript
// src/lib/embeddings.js (Enhanced version with query expansion and caching)

// In-memory cache for embeddings (consider Redis for production)
const embeddingCache = new Map();
const MAX_CACHE_SIZE = 1000;

// Singleton pattern to ensure we only load the model once per server instance
class EmbeddingPipeline {
    static task = 'feature-extraction';
    static model = 'Xenova/all-MiniLM-L6-v2';
    static instance = null;
    
    static async getInstance() {
        if (this.instance === null) {
            const { pipeline } = await import('@xenova/transformers');
            this.instance = await pipeline(this.task, this.model);
        }
        return this.instance;
    }
}

/**
 * Creates a cache key from text
 * @param {string} text 
 * @returns {string}
 */
function createCacheKey(text) {
    return `embed_${text.toLowerCase().trim().replace(/\s+/g, '_')}`;
}

/**
 * Manages cache size to prevent memory bloat
 */
function manageCacheSize() {
    if (embeddingCache.size >= MAX_CACHE_SIZE) {
        // Remove oldest 20% of entries (FIFO-ish)
        const keysToRemove = Array.from(embeddingCache.keys()).slice(0, Math.floor(MAX_CACHE_SIZE * 0.2));
        keysToRemove.forEach(key => embeddingCache.delete(key));
        console.log(`[Embedding Cache] Cleaned ${keysToRemove.length} entries`);
    }
}

/**
 * Generates an embedding for a given text with caching
 * @param {string} text The text to embed
 * @returns {Promise<Array<number>>} A promise that resolves to the embedding vector
 */
export async function generateEmbedding(text) {
    if (!text || text.trim().length === 0) {
        throw new Error('Text cannot be empty for embedding generation');
    }
    
    const cleanText = text.trim();
    const cacheKey = createCacheKey(cleanText);
    
    // Check cache first
    if (embeddingCache.has(cacheKey)) {
        console.log(`[Embedding Cache] Hit for text: "${cleanText.substring(0, 50)}..."`);
        return embeddingCache.get(cacheKey);
    }
    
    try {
        const extractor = await EmbeddingPipeline.getInstance();
        const output = await extractor(cleanText, { pooling: 'mean', normalize: true });
        const embedding = Array.from(output.data);
        
        // Cache the result
        manageCacheSize();
        embeddingCache.set(cacheKey, embedding);
        
        console.log(`[Embedding] Generated embedding for text: "${cleanText.substring(0, 50)}..." (${embedding.length} dimensions)`);
        return embedding;
        
    } catch (error) {
        console.error(`[Embedding Error] Failed to generate embedding: ${error.message}`);
        throw new Error(`Failed to generate embedding: ${error.message}`);
    }
}

/**
 * Generates multiple query variations to improve RAG recall
 * @param {string} originalQuery 
 * @returns {Promise<Array<Array<number>>>} Array of embeddings for different query variations
 */
export async function generateQueryEmbeddings(originalQuery) {
    const variations = generateQueryVariations(originalQuery);
    const embeddingPromises = variations.map(query => generateEmbedding(query));
    
    try {
        const embeddings = await Promise.all(embeddingPromises);
        console.log(`[Query Expansion] Generated ${embeddings.length} query variations for: "${originalQuery}" ->`, variations);
        return embeddings;
    } catch (error) {
        console.error(`[Query Expansion Error] ${error.message}`);
        // Fallback to original query only
        return [await generateEmbedding(originalQuery)];
    }
}

/**
 * Creates query variations to improve semantic search recall
 * @param {string} query 
 * @returns {Array<string>}
 */
function generateQueryVariations(query) {
    const originalQuery = query.trim();
    const variations = new Set([originalQuery]);

    // CORRECTED: Smartly strip disambiguation tags for broader searches
    const coreEntity = originalQuery.replace(/\s*\((company|person)\)$/, '').trim();
    if (coreEntity !== originalQuery) {
        variations.add(coreEntity);
    }

    // Pattern for "Who founded X?"
    const à¤¹à¥‚à¤‚FounderMatch = coreEntity.toLowerCase().match(/^(?:who|what)\s+(?:is|was|founded|created)\s+(.+)/);
    if ( à¤¹à¥‚à¤‚FounderMatch) {
        let subject = à¤¹à¥‚à¤‚FounderMatch[1].replace(/\?/g, '').replace(/^(the|a|an)\s/,'').trim();
        variations.add(subject);
        variations.add(`${subject} founder`);
        variations.add(`founder of ${subject}`);
        variations.add(`${subject} history`);
    } else {
        // General question pattern
        const questionMatch = coreEntity.toLowerCase().match(/^(who|what|when|where|why|how)\s(is|are|was|were|did|does|do)\s(.+)/);
        if (questionMatch) {
            let subject = questionMatch[3].replace(/\?/g, '').trim();
            variations.add(subject);
            
            const simplified = subject.replace(/^(the|a|an)\s/,'').split(' of ');
            if (simplified.length > 1) {
                variations.add(`${simplified[1].trim()} ${simplified[0].trim()}`);
            }
        }
    }
    
    // Add generic variations for the core entity
    if (hasProperNouns(coreEntity)) {
        variations.add(`${coreEntity} background details`);
        variations.add(`Information about ${coreEntity}`);
    }
    
    // Return the top 4 most distinct variations
    return Array.from(variations).slice(0, 4);
}


/**
 * Simple check for proper nouns (capitalized words not at the start of a sentence)
 * @param {string} text 
 * @returns {boolean}
 */
function hasProperNouns(text) {
    // Looks for words starting with an uppercase letter
    return /\b[A-Z][a-z]+/.test(text);
}

/**
 * Batch embedding generation for efficiency
 * @param {Array<string>} texts 
 * @returns {Promise<Array<Array<number>>>}
 */
export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) {
        return [];
    }
    
    const embeddings = [];
    const extractor = await EmbeddingPipeline.getInstance();
    
    // Process in batches to avoid memory issues
    const BATCH_SIZE = 10;
    for (let i = 0; i < texts.length; i += BATCH_SIZE) {
        const batch = texts.slice(i, i + BATCH_SIZE);
        const batchPromises = batch.map(text => {
            const cacheKey = createCacheKey(text);
            if (embeddingCache.has(cacheKey)) {
                return Promise.resolve(embeddingCache.get(cacheKey));
            }
            return extractor(text, { pooling: 'mean', normalize: true })
                .then(output => {
                    const embedding = Array.from(output.data);
                    embeddingCache.set(cacheKey, embedding);
                    return embedding;
                });
        });
        
        const batchEmbeddings = await Promise.all(batchPromises);
        embeddings.push(...batchEmbeddings);
        
        console.log(`[Batch Embedding] Processed batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(texts.length/BATCH_SIZE)}`);
    }
    
    return embeddings;
}

/**
 * Calculate cosine similarity between two embeddings
 * @param {Array<number>} embedding1 
 * @param {Array<number>} embedding2 
 * @returns {Promise<number>} Similarity score between 0 and 1
 */
export async function calculateSimilarity(embedding1, embedding2) {
    if (embedding1.length !== embedding2.length) {
        throw new Error('Embeddings must have the same dimensions');
    }
    
    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;
    
    for (let i = 0; i < embedding1.length; i++) {
        dotProduct += embedding1[i] * embedding2[i];
        norm1 += embedding1[i] * embedding1[i];
        norm2 += embedding2[i] * embedding2[i];
    }
    
    if (norm1 === 0 || norm2 === 0) return 0;
    
    return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
}

/**
 * Get cache statistics for monitoring
 * @returns {Promise<Object>}
 */
export async function getCacheStats() {
    return {
        size: embeddingCache.size,
        maxSize: MAX_CACHE_SIZE,
        utilizationPercent: Math.round((embeddingCache.size / MAX_CACHE_SIZE) * 100)
    };
}
```

## ðŸ“„ src/embeddings/vectorSearch.js
*Lines: 76, Size: 2.48 KB*

```javascript
// packages/ai-services/src/embeddings/vectorSearch.js
import { Pinecone } from '@pinecone-database/pinecone'
import { logger } from '@headlines/utils-shared'
import { generateEmbedding } from './embeddings.js'
import { env } from '@headlines/config'

const { PINECONE_API_KEY, PINECONE_INDEX_NAME } = env

const SIMILARITY_THRESHOLD = 0.65
const MAX_CONTEXT_ARTICLES = 3
const MAX_RETRIES = 2 // Add retry configuration

let pineconeIndex
if (PINECONE_API_KEY) {
  const pc = new Pinecone({ apiKey: PINECONE_API_KEY })
  pineconeIndex = pc.index(PINECONE_INDEX_NAME)
} else {
  logger.warn(
    'Pinecone API Key not found. RAG/vector search functionality will be disabled.'
  )
}

export async function findSimilarArticles(queryText) {
  if (!pineconeIndex) return []
  logger.info('RAG: Searching for historical context in Pinecone...')
  if (!queryText || typeof queryText !== 'string' || queryText.trim().length === 0)
    return []

  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      const queryEmbedding = await generateEmbedding(queryText)

      const queryResponse = await pineconeIndex.query({
        topK: MAX_CONTEXT_ARTICLES,
        vector: queryEmbedding,
        includeMetadata: true,
      })

      const relevantMatches = queryResponse.matches.filter(
        (match) => match.score >= SIMILARITY_THRESHOLD
      )

      if (relevantMatches.length > 0) {
        const retrievedArticlesForLogging = relevantMatches
          .map(
            (match) =>
              `  - [Score: ${match.score.toFixed(3)}] "${match.metadata.headline}"`
          )
          .join('\n')
        logger.info(
          `RAG: Found ${relevantMatches.length} relevant historical articles:\n${retrievedArticlesForLogging}`
        )
        return relevantMatches.map((match) => ({
          headline: match.metadata.headline,
          newspaper: match.metadata.newspaper,
          assessment_article: match.metadata.summary,
        }))
      } else {
        logger.info('RAG: Found no relevant historical articles in Pinecone.')
        return []
      }
    } catch (error) {
      logger.error({ err: error }, `RAG: Pinecone query attempt ${attempt} failed.`)
      if (attempt === MAX_RETRIES) {
        logger.error(
          { err: error },
          'RAG: Pinecone query or embedding generation failed after all retries.'
        )
        return []
      }
      await new Promise((res) => setTimeout(res, 1000 * attempt)) // Exponential backoff
    }
  }
  return [] // Should be unreachable
}

```

## ðŸ“„ src/index.js
*Lines: 147, Size: 5.83 KB*

```javascript
// packages/ai-services/src/index.js
// This is the default, Node.js-safe entry point. It exports everything.

// --- SHARED EXPORTS ---
export * from './lib/langchain.js'
export * from './chains/index.js'
export * from './search/search.js'
export * from './search/wikipedia.js'
export * from './embeddings/embeddings.js'
export * from './embeddings/vectorSearch.js'
export * from './rag/orchestrator.js'
export * from './shared/agents/synthesisAgent.js'
export * from './shared/agents/opportunityAgent.js'
export * from './shared/agents/contactAgent.js'
export * from './shared/agents/entityAgent.js'
export * from './shared/agents/emailAgents.js'
export * from './shared/agents/executiveSummaryAgent.js'

// --- NODE-ONLY EXPORTS ---
export * from './node/agents/articleAgent.js'
export * from './node/agents/articlePreAssessmentAgent.js'
export * from './node/agents/batchArticleAgent.js'
export * from './node/agents/clusteringAgent.js'
export * from './node/agents/headlineAgent.js'
export * from './node/agents/judgeAgent.js'
export * from './node/agents/sectionClassifierAgent.js'
export * from './node/agents/selectorRepairAgent.js'
export * from './node/agents/watchlistAgent.js'

// --- MOVED LOGIC FROM DATA-ACCESS ---
import { logger } from '@headlines/utils-shared'
import { settings } from '@headlines/config/node'
import { callLanguageModel } from './lib/langchain.js'
import { SynthesizedEvent, Opportunity } from '@headlines/models/node'
import { synthesizeEvent } from './shared/agents/synthesisAgent.js'
import { generateOpportunitiesFromEvent } from './shared/agents/opportunityAgent.js'
import { instructionSourceDiscovery } from '@headlines/prompts'
import { generateEmbedding } from './embeddings/embeddings.js'
import { Article } from '@headlines/models'
import mongoose from 'mongoose'

const TITLE_GENERATOR_PROMPT = `You are a title generation AI. Your task is to read a conversation and create a concise, 5-word-or-less title that accurately summarizes the main topic. Example Title: "Anders Holch Povlsen's Bestseller"`

export async function generateChatTitle(messages) {
  if (!messages || messages.length < 2) {
    return { success: false, error: 'Not enough messages to generate a title.' }
  }
  try {
    const conversationText = messages.map((m) => `${m.role}: ${m.content}`).join('\n')
    const title = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: TITLE_GENERATOR_PROMPT,
      userContent: conversationText,
      isJson: false,
    })
    const cleanedTitle = title.trim().replace(/"/g, '')
    return { success: true, title: cleanedTitle }
  } catch (error) {
    return { success: false, error: 'Failed to generate title.' }
  }
}

export async function processUploadedArticle(item, userId) {
  if (!userId) {
    return { success: false, error: 'Authentication required' }
  }
  try {
    const enrichedArticle = {
      ...item,
      relevance_article: 100,
      assessment_article: item.article,
      articleContent: { contents: [item.article] },
      newspaper: 'Manual Upload',
      country: ['Denmark'],
      key_individuals: [],
    }

    const synthesizedResult = await synthesizeEvent([enrichedArticle], [], '', '')
    if (!synthesizedResult || !synthesizedResult.events || synthesizedResult.events.length === 0) {
      throw new Error('AI failed to synthesize an event from the provided text.')
    }
    const eventData = synthesizedResult.events[0]

    const eventToSave = new SynthesizedEvent({
      ...eventData,
      event_key: `manual-${new Date().toISOString()}`,
      highest_relevance_score: 100,
      source_articles: [{ headline: item.headline, link: '#manual', newspaper: 'Manual Upload' }],
    })

    const opportunitiesToSave = await generateOpportunitiesFromEvent(eventToSave, [enrichedArticle])

    await eventToSave.save()
    if (opportunitiesToSave.length > 0) {
      await Opportunity.insertMany(opportunitiesToSave.map((opp) => ({ ...opp, events: [eventToSave._id] })))
    }

    return { success: true, event: eventToSave.synthesized_headline }
  } catch (e) {
    console.error('[Upload Action Error]:', e)
    return { success: false, error: e.message }
  }
}

export async function addKnowledge(data) {
    const { headline, business_summary, source, country, link } = data
    if (!headline || !business_summary || !source || !country || !link) {
        return { success: false, message: 'All fields are required.' }
    }
    try {
        const textToEmbed = `${headline}\n${business_summary}`
        const embedding = await generateEmbedding(textToEmbed)
        const newArticle = new Article({
            _id: new mongoose.Types.ObjectId(),
            headline, link, newspaper: source, source: 'Manual Upload', country: [country],
            relevance_headline: 100, assessment_headline: 'Manually uploaded by user.',
            relevance_article: 100, assessment_article: business_summary,
            embedding: embedding, key_individuals: [],
        })
        await newArticle.save()
        // Pinecone logic would go here
        return { success: true, message: 'Knowledge successfully added and embedded.' }
    } catch (error) {
        console.error('[Add Knowledge Error]', error)
        return { success: false, message: 'Failed to add knowledge.' }
    }
}

export async function suggestSections(url) {
  const scrapeResult = { success: true, content: '<div>Mock Content</div>' }
  try {
    const data = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: instructionSourceDiscovery,
      userContent: `Analyze the HTML from ${url}:\n\n${scrapeResult.content}`,
      isJson: true,
    })
    return { success: true, data: data.suggestions }
  } catch (e) {
    return { success: false, error: 'AI agent failed to suggest sections.' }
  }
}

// --- Sanity Check Function ---
export async function performAiSanityCheck() {
    // ... implementation ...
}
```

## ðŸ“„ src/lib/AIAgent.js
*Lines: 65, Size: 1.82 KB*

```javascript
// packages/ai-services/src/lib/AIAgent.js (version 2.0.0)
import { callLanguageModel } from './langchain.js'
import { logger } from '@headlines/utils-shared'

export class AIAgent {
  constructor({
    model,
    systemPrompt,
    isJson = true,
    fewShotInputs = [],
    fewShotOutputs = [],
    zodSchema,
  }) {
    if (!model || !systemPrompt) {
      throw new Error('AIAgent requires a model and systemPrompt.')
    }
    this.model = model
    this.systemPrompt = systemPrompt
    this.isJson = isJson
    this.fewShotInputs = fewShotInputs
    this.fewShotOutputs = fewShotOutputs
    this.zodSchema = zodSchema

    logger.trace(
      { agentConfig: { model, isJson, hasSchema: !!zodSchema } },
      'Initialized new AIAgent.'
    )
  }

  async execute(userContent) {
    let systemPromptContent = this.systemPrompt
    // If the provided prompt is a function (for dynamic settings), execute it.
    if (typeof systemPromptContent === 'function') {
      systemPromptContent = systemPromptContent()
    }

    const response = await callLanguageModel({
      modelName: this.model,
      systemPrompt: systemPromptContent,
      userContent,
      isJson: this.isJson,
      fewShotInputs: this.fewShotInputs,
      fewShotOutputs: this.fewShotOutputs,
    })

    if (this.isJson && this.zodSchema && !response.error) {
      const validationResult = this.zodSchema.safeParse(response)
      if (!validationResult.success) {
        logger.error(
          {
            details: validationResult.error.flatten(),
            model: this.model,
            rawResponse: response, // Log the raw response for debugging
          },
          `AI response failed Zod validation.`
        )
        return { error: 'Zod validation failed', details: validationResult.error }
      }
      return validationResult.data
    }

    return response
  }
}

```

## ðŸ“„ src/lib/langchain.js
*Lines: 91, Size: 2.92 KB*

```javascript
import { ChatOpenAI } from '@langchain/openai'
import { env, settings } from '@headlines/config/node'
import { logger } from '@headlines/utils-shared'
import { tokenTracker } from '@headlines/utils-server/node'
import { safeExecute } from '@headlines/utils-server/helpers'
import OpenAI from 'openai'

const modelConfig = { response_format: { type: 'json_object' } }

// Temperature parameters have been removed to rely on provider defaults, which is more robust.
export const getHeadlineModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_HEADLINE_ASSESSMENT }).bind(modelConfig)
export const getHighPowerModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_SYNTHESIS }).bind(modelConfig)
export const getUtilityModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_UTILITY }).bind(modelConfig)

const baseClient = new OpenAI({
  apiKey: env.OPENAI_API_KEY,
  timeout: 120 * 1000, // This is a TCP-level timeout, good to have.
  maxRetries: 3,
})

export async function callLanguageModel({
  modelName,
  prompt,
  systemPrompt,
  userContent,
  isJson = true,
  fewShotInputs = [],
  fewShotOutputs = [],
}) {
  const messages = []
  if (systemPrompt) {
    const systemContent =
      typeof systemPrompt === 'object' ? JSON.stringify(systemPrompt) : systemPrompt
    messages.push({ role: 'system', content: systemContent })
  }
  fewShotInputs.forEach((input, i) => {
    const shotContent = typeof input === 'string' ? input : JSON.stringify(input)
    if (shotContent) {
      messages.push({ role: 'user', content: shotContent })
      messages.push({ role: 'assistant', content: fewShotOutputs[i] })
    }
  })
  const finalUserContent = userContent || prompt
  messages.push({ role: 'user', content: finalUserContent })
  logger.trace(
    { payload: { model: modelName, messages_count: messages.length } },
    'Sending payload to LLM.'
  )

  const apiPayload = {
    model: modelName,
    messages: messages,
  }
  if (isJson) {
    apiPayload.response_format = { type: 'json_object' }
  }

  // Use the robust safeExecute with its built-in timeout race.
  // We'll give it an 85-second timeout, slightly less than the safeExecute default.
  const result = await safeExecute(() => baseClient.chat.completions.create(apiPayload), {
    timeout: 85000,
  })

  if (!result) {
    // safeExecute will have already logged the error (timeout or otherwise).
    return { error: 'API call failed or timed out' }
  }

  if (result.usage) {
    tokenTracker.recordUsage(modelName, result.usage)
  }
  const responseContent = result.choices[0].message.content
  logger.trace({ chars: responseContent.length }, 'Received LLM response.')

  if (isJson) {
    try {
      return JSON.parse(responseContent)
    } catch (parseError) {
      logger.error(
        { err: parseError, details: responseContent },
        `LLM response JSON Parse Error for model ${modelName}`
      )
      return { error: 'JSON Parsing Error' }
    }
  }
  return responseContent
}

```

## ðŸ“„ src/lib/safeInvoke.js
*Lines: 90, Size: 3.06 KB*

```javascript
import { logger } from '@headlines/utils-shared'
import { getRedisClient } from '@headlines/utils-server/node'
import { createHash } from 'crypto'
import { StringOutputParser } from '@langchain/core/output_parsers'

const MAX_RETRIES = 1
const CACHE_TTL_SECONDS = 60 * 60 * 24
const inMemoryCache = new Map()

function createCacheKey(agentName, input) {
  const hash = createHash('sha256')
  hash.update(JSON.stringify(input))
  return `ai_cache:${agentName}:${hash.digest('hex')}`
}

export async function safeInvoke(chain, input, agentName, zodSchema) {
  const redis = await getRedisClient()
  const cacheKey = createCacheKey(agentName, input)

  if (redis) {
    try {
      const cachedResult = await redis.get(cacheKey)
      if (typeof cachedResult === 'string' && cachedResult.length > 0) {
        logger.trace({ agent: agentName }, `[Redis Cache HIT] for ${agentName}.`)
        return JSON.parse(cachedResult)
      }
    } catch (err) {
      logger.error({ err, agent: agentName, key: cacheKey }, `Redis GET or PARSE failed.`)
    }
  } else if (inMemoryCache.has(cacheKey)) {
    logger.trace({ agent: agentName }, `[In-Memory Cache HIT] for ${agentName}.`)
    return inMemoryCache.get(cacheKey)
  }

  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
    try {
      // --- START OF THE DEFINITIVE FIX ---
      // The chain now outputs a raw BaseMessage. We pipe it to a StringOutputParser
      // here to get the raw string content from the AI.
      const stringParser = new StringOutputParser()
      const stringResult = await chain.pipe(stringParser).invoke(input)

      // This is more robust because it finds the JSON even if the AI adds extra text.
      const jsonMatch = stringResult.match(/\{[\s\S]*\}/)
      if (!jsonMatch) {
        throw new Error("No valid JSON object found in the LLM's string response.")
      }
      const result = JSON.parse(jsonMatch[0])
      // --- END OF THE DEFINITIVE FIX ---

      const validation = zodSchema.safeParse(result)
      if (!validation.success) {
        logger.error(
          { details: validation.error.flatten(), agent: agentName, output: result },
          `Zod validation failed for ${agentName}.`
        )
        throw new Error('Zod validation failed')
      }

      const dataToCache = validation.data

      if (redis) {
        try {
          await redis.set(cacheKey, JSON.stringify(dataToCache), {
            EX: CACHE_TTL_SECONDS,
          })
        } catch (err) {
          logger.error({ err, agent: agentName }, `Redis SET failed for ${agentName}.`)
        }
      } else {
        inMemoryCache.set(cacheKey, dataToCache)
      }
      return dataToCache
    } catch (error) {
      if (attempt < MAX_RETRIES) {
        logger.warn(
          { agent: agentName, attempt: attempt + 1, error: error.message },
          `Invocation failed for ${agentName}. Retrying...`
        )
        continue
      }
      logger.error(
        { err: error, agent: agentName },
        `LangChain invocation failed for ${agentName}.`
      )
      return { error: `Agent ${agentName} failed: ${error.message}` }
    }
  }
}

```

## ðŸ“„ src/next.js
*Lines: 54, Size: 1.6 KB*

```javascript
// packages/ai-services/src/next.js
import 'server-only'

export * from './chains/index.js'
export * from './search/search.js'
export * from './search/wikipedia.js'
export * from './embeddings/embeddings.js'
export * from './embeddings/vectorSearch.js'
export * from './rag/orchestrator.js'
export * from './shared/agents/synthesisAgent.js'
export * from './shared/agents/opportunityAgent.js'
export * from './shared/agents/contactAgent.js'
export * from './shared/agents/entityAgent.js'
export * from './shared/agents/emailAgents.js'
export * from './shared/agents/executiveSummaryAgent.js'

import {
  generateChatTitle as coreGenTitle,
  processUploadedArticle as coreUpload,
  addKnowledge as coreAddKnowledge,
  suggestSections as coreSuggestSections,
} from './index.js'
import dbConnect from '@headlines/data-access/dbConnect/next'
import { revalidatePath } from 'next/cache'

// Wrap core functions with dbConnect for the Next.js environment
export const generateChatTitle = async (...args) => {
  await dbConnect()
  return coreGenTitle(...args)
}

export const processUploadedArticle = async (...args) => {
  await dbConnect()
  const result = await coreUpload(...args)
  if (result.success) {
    revalidatePath('/events')
    revalidatePath('/opportunities')
  }
  return result
}

export const addKnowledge = async (...args) => {
  await dbConnect()
  return coreAddKnowledge(...args)
}

export const suggestSections = async (...args) => {
  await dbConnect()
  return coreSuggestSections(...args)
}

import { performAiSanityCheck as coreSanityCheck } from './index.js'
export const performAiSanityCheck = coreSanityCheck

```

## ðŸ“„ src/node/agents/articleAgent.js
*Lines: 75, Size: 2.68 KB*

```javascript
// packages/ai-services/src/node/agents/articleAgent.js (version 1.0.0)
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { articleAssessmentSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionArticle,
    fewShotInputs: shotsInputArticle,
    fewShotOutputs: shotsOutputArticle,
    zodSchema: articleAssessmentSchema,
  })

// The agent now accepts `hits` as a parameter, breaking the circular dependency.
// It is no longer responsible for fetching data.
export async function assessArticleContent(article, hits = [], isSalvaged = false) {
  const articleAssessmentAgent = getAgent()
  const fullContent = (article.articleContent?.contents || []).join('\n')
  const truncatedContent = truncateString(fullContent, settings.LLM_CONTEXT_MAX_CHARS)

  if (fullContent.length > settings.LLM_CONTEXT_MAX_CHARS) {
    logger.warn(
      {
        originalLength: fullContent.length,
        truncatedLength: truncatedContent.length,
        limit: settings.LLM_CONTEXT_MAX_CHARS,
      },
      `Article content for LLM was truncated to prevent context overload.`
    )
  }

  let articleText = `HEADLINE: ${article.headline}\n\nBODY:\n${truncatedContent}`

  if (hits.length > 0) {
    const hitStrings = hits.map(
      (hit) => `[WATCHLIST HIT: ${hit.name} | CONTEXT: ${hit.context || 'N/A'}]`
    )
    const hitPrefix = hitStrings.join(' ')
    articleText = `${hitPrefix} ${articleText}`
    logger.info({ hits: hits.map((h) => h.name) }, 'Watchlist entities found in article.')
  }

  if (isSalvaged) {
    articleText = `[SALVAGE CONTEXT: The original source for this headline failed to scrape. This content is from an alternative source. Please assess based on this new context.]\n\n${articleText}`
  }

  const response = await articleAssessmentAgent.execute(articleText)

  if (response.error) {
    logger.error(
      { article: { link: article.link }, details: response },
      `Article assessment failed for ${article.link}.`
    )
    return { ...article, error: `AI Error: ${response.error}` }
  }

  if (
    response.amount > 0 &&
    response.amount < settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  ) {
    response.relevance_article = 10
    response.assessment_article = `Dropped: Amount ($${response.amount}M) is below the financial threshold of $${settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS}M.`
  }

  return { ...article, ...response, error: null }
}

```

## ðŸ“„ src/node/agents/articlePreAssessmentAgent.js
*Lines: 22, Size: 784 Bytes*

```javascript
// packages/ai-services/src/agents/articlePreAssessmentAgent.js
import { AIAgent } from '../../lib/AIAgent.js'
import { articlePreAssessmentSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings } from '@headlines/config/node'
import { instructionArticlePreAssessment } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionArticlePreAssessment,
    zodSchema: articlePreAssessmentSchema,
  })

export async function preAssessArticle(articleContent) {
  const articlePreAssessmentAgent = getAgent()
  const response = await articlePreAssessmentAgent.execute(articleContent)
  if (response.error) {
    return { classification: null, error: response.error }
  }
  return response
}

```

## ðŸ“„ src/node/agents/batchArticleAgent.js
*Lines: 64, Size: 2.01 KB*

```javascript
// packages/ai-services/src/agents/batchArticleAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { batchArticleAssessmentSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings, AI_BATCH_SIZE } from '@headlines/config/node'
import { getInstructionBatchArticleAssessment } from '@headlines/prompts'
import { assessArticleContent } from './articleAgent.js'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionBatchArticleAssessment,
    zodSchema: batchArticleAssessmentSchema,
  })

export async function batchAssessArticles(articles) {
  if (!articles || articles.length === 0) return []

  const batchAgent = getAgent()
  const articleBatches = []
  for (let i = 0; i < articles.length; i += AI_BATCH_SIZE) {
    articleBatches.push(articles.slice(i, i + AI_BATCH_SIZE))
  }

  const allResults = []

  for (const batch of articleBatches) {
    const payload = batch.map((article) => ({
      headline: article.headline,
      content: (article.articleContent?.contents || []).join('\n'),
    }))

    const response = await batchAgent.execute(JSON.stringify(payload))

    if (
      response.error ||
      !response.assessments ||
      response.assessments.length !== batch.length
    ) {
      logger.error(
        {
          details: response,
          expectedCount: batch.length,
          receivedCount: response.assessments?.length,
        },
        'Batch assessment failed or returned mismatched count. Falling back to single-article processing for this batch.'
      )

      const fallbackPromises = batch.map((article) => assessArticleContent(article))
      const fallbackResults = await Promise.all(fallbackPromises)
      allResults.push(...fallbackResults)
      continue
    }

    const mergedResults = batch.map((originalArticle, index) => ({
      ...originalArticle,
      ...response.assessments[index],
    }))
    allResults.push(...mergedResults)
  }

  return allResults
}

```

## ðŸ“„ src/node/agents/clusteringAgent.js
*Lines: 75, Size: 2.3 KB*

```javascript
// packages/ai-services/src/node/agents/clusteringAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { clusterSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionCluster } from '@headlines/prompts'

const CLUSTER_BATCH_SIZE = 25

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionCluster,
    zodSchema: clusterSchema,
  })

export async function clusterArticlesIntoEvents(articles) {
  const articleClusterAgent = getAgent()
  logger.info(`Clustering ${articles.length} articles into unique events...`)

  if (!articles || articles.length === 0) {
    return []
  }

  const batches = []
  for (let i = 0; i < articles.length; i += CLUSTER_BATCH_SIZE) {
    batches.push(articles.slice(i, i + CLUSTER_BATCH_SIZE))
  }
  logger.info(`Processing clusters in ${batches.length} batches.`)

  const allClusters = []
  for (const [index, batch] of batches.entries()) {
    logger.info(`Clustering batch ${index + 1} of ${batches.length}...`)
    const articlePayload = batch.map((a) => ({
      id: a._id.toString(),
      headline: a.headline,
      source: a.newspaper,
      summary: (a.assessment_article || a.assessment_headline || '').substring(0, 400),
    }))
    const userContent = JSON.stringify(articlePayload)
    const response = await articleClusterAgent.execute(userContent)

    if (response.error || !response.events) {
      logger.error(`Failed to cluster articles in batch ${index + 1}.`, {
        response,
      })
      continue
    }
    allClusters.push(...response.events)
  }

  if (allClusters.length === 0) {
    logger.warn('Failed to cluster any articles across all batches.')
    return []
  }

  const finalEventMap = new Map()
  allClusters.forEach((event) => {
    if (finalEventMap.has(event.event_key)) {
      const existing = finalEventMap.get(event.event_key)
      event.article_ids.forEach((id) => existing.article_ids.add(id))
    } else {
      finalEventMap.set(event.event_key, {
        event_key: event.event_key,
        article_ids: new Set(event.article_ids),
      })
    }
  })

  return Array.from(finalEventMap.values()).map((event) => ({
    event_key: event.event_key,
    article_ids: Array.from(event.article_ids),
  }))
}

```

## ðŸ“„ src/node/agents/headlineAgent.js
*Lines: 66, Size: 2.08 KB*

```javascript
// packages/ai-services/src/agents/headlineAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { headlineAssessmentSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings } from '@headlines/config/node'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_HEADLINE_ASSESSMENT,
    systemPrompt: instructionHeadlines,
    fewShotInputs: shotsInputHeadlines,
    fewShotOutputs: shotsOutputHeadlines,
    zodSchema: headlineAssessmentSchema,
  })

async function assessSingleHeadline(article, hits = []) {
  const headlineAssessmentAgent = getAgent()
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`

  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }

  const response = await headlineAssessmentAgent.execute(headlineWithContext)

  let assessment = {
    relevance_headline: 0,
    assessment_headline: 'AI assessment failed.',
    headline_en: article.headline,
  }

  if (response && response.assessment && response.assessment.length > 0) {
    assessment = response.assessment[0]
    let score = assessment.relevance_headline
    const boost = settings.WATCHLIST_SCORE_BOOST

    if (hits.length > 0 && boost > 0) {
      score = Math.min(100, score + boost)
      assessment.assessment_headline = `Watchlist boost (+${boost}). ${assessment.assessment_headline}`
    }
    assessment.relevance_headline = score
  }

  return { ...article, ...assessment }
}

export async function assessHeadlinesInBatches(articles, articlesHits) {
  const assessmentPromises = articles.map((article, index) => {
    const hitsForArticle = articlesHits[index] || []
    return assessSingleHeadline(article, hitsForArticle)
  })

  const results = await Promise.all(assessmentPromises)
  return results
}

```

## ðŸ“„ src/node/agents/judgeAgent.js
*Lines: 62, Size: 1.76 KB*

```javascript
// packages/ai-services/src/node/agents/judgeAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { judgeSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionJudge } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionJudge,
    zodSchema: judgeSchema,
  })

export async function judgePipelineOutput(events, opportunities) {
  const judgeAgent = getAgent() // <-- FIX APPLIED HERE
  if (
    (!events || events.length === 0) &&
    (!opportunities || opportunities.length === 0)
  ) {
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }
  logger.info('âš–ï¸ [Judge Agent] Reviewing final pipeline output for quality control...')

  const lightweightEvents = (events || []).map((e) => ({
    identifier: `Event: ${e.synthesized_headline}`,
    summary: e.synthesized_summary,
    assessment: e.ai_assessment_reason,
    score: e.highest_relevance_score,
  }))

  const lightweightOpportunities = (opportunities || []).map((o) => ({
    identifier: `Opportunity: ${o.reachOutTo}`,
    reason: o.whyContact,
    wealth_estimate_mm: o.likelyMMDollarWealth,
  }))

  const inputText = JSON.stringify({
    events: lightweightEvents,
    opportunities: lightweightOpportunities,
  })

  const response = await judgeAgent.execute(inputText)

  if (response.error) {
    logger.error({ details: response }, 'Judge Agent failed to produce a verdict.')
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }

  logger.info(
    { details: response },
    '[Judge Agent] Successfully produced quality control verdicts.'
  )
  return response
}

```

## ðŸ“„ src/node/agents/sectionClassifierAgent.js
*Lines: 42, Size: 1.29 KB*

```javascript
// packages/ai-services/src/node/agents/sectionClassifierAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { sectionClassifierSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionSectionClassifier } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY, // Using the cheap and fast model
    systemPrompt: [
      instructionSectionClassifier.whoYouAre,
      instructionSectionClassifier.whatYouDo,
      ...instructionSectionClassifier.guidelines,
      instructionSectionClassifier.outputFormatDescription,
    ].join('\n\n'),
    zodSchema: sectionClassifierSchema,
  })

export async function classifyLinks(links) {
  if (!links || links.length === 0) {
    return []
  }

  const agent = getAgent() // <-- FIX APPLIED HERE
  const response = await agent.execute(JSON.stringify(links))

  if (
    response.error ||
    !response.classifications ||
    response.classifications.length !== links.length
  ) {
    logger.error(
      { response, expectedCount: links.length },
      'Section classifier agent failed or returned mismatched count.'
    )
    return null // Return null to indicate failure
  }

  return response.classifications
}

```

## ðŸ“„ src/node/agents/selectorRepairAgent.js
*Lines: 55, Size: 1.64 KB*

```javascript
// packages/ai-services/src/node/agents/selectorRepairAgent.js
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { selectorRepairSchema } from '@headlines/models/schemas'
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { instructionSelectorRepair } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: [
      instructionSelectorRepair.whoYouAre,
      instructionSelectorRepair.whatYouDo,
      ...instructionSelectorRepair.guidelines,
      instructionSelectorRepair.outputFormatDescription,
      instructionSelectorRepair.reiteration,
    ].join('\n\n'),
    zodSchema: selectorRepairSchema,
  })

export async function suggestNewSelector(
  url,
  failedSelector,
  htmlContent,
  heuristicSuggestions = []
) {
  const selectorRepairAgent = getAgent()
  try {
    const payload = {
      url,
      failed_selector: failedSelector,
      heuristic_suggestions: heuristicSuggestions.map((s) => ({
        selector: s.selector,
        samples: s.samples.slice(0, 3),
      })),
      html_content: truncateString(htmlContent, LLM_CONTEXT_MAX_CHARS),
    }

    const response = await selectorRepairAgent.execute(JSON.stringify(payload))

    if (response.error || !response.suggested_selectors) {
      logger.error('Selector repair agent failed to produce a valid suggestion.', {
        response,
      })
      return null
    }

    return response
  } catch (error) {
    logger.error({ err: error }, 'Error in suggestNewSelector')
    return null
  }
}

```

## ðŸ“„ src/node/agents/watchlistAgent.js
*Lines: 48, Size: 1.76 KB*

```javascript
// packages/ai-services/src/node/agents/watchlistAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { watchlistSuggestionSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionWatchlistSuggestion } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: [
      instructionWatchlistSuggestion.whoYouAre,
      instructionWatchlistSuggestion.whatYouDo,
      ...instructionWatchlistSuggestion.guidelines,
      instructionWatchlistSuggestion.outputFormatDescription,
      instructionWatchlistSuggestion.reiteration,
    ].join('\n\n'),
    zodSchema: watchlistSuggestionSchema,
  })

/**
 * Analyzes events to generate new watchlist suggestions.
 * @param {Array<object>} events - High-quality synthesized events.
 * @param {Set<string>} existingWatchlistNames - A set of lowercase names already on the watchlist.
 * @returns {Promise<Array<object>>} An array of new WatchlistSuggestion documents.
 */
export async function generateWatchlistSuggestions(events, existingWatchlistNames) {
  const watchlistSuggestionAgent = getAgent() // <-- FIX APPLIED HERE
  try {
    const payload = { events }
    const response = await watchlistSuggestionAgent.execute(JSON.stringify(payload))

    if (response.error || !Array.isArray(response.suggestions)) {
      logger.warn('AI failed to generate watchlist suggestions.', response)
      return []
    }

    const newSuggestions = response.suggestions.filter(
      (s) => !existingWatchlistNames.has(s.name.toLowerCase())
    )

    return newSuggestions
  } catch (error) {
    logger.error({ err: error }, 'Error in generateWatchlistSuggestions')
    return []
  }
}

```

## ðŸ“„ src/rag/generation.js
*Lines: 137, Size: 4.31 KB*

```javascript
// packages/ai-services/src/rag/generation.js
import { getSynthesizerPrompt } from './prompts.js'
import { checkGroundedness } from './validation.js'
import { callLanguageModel } from '../lib/langchain.js'
import { settings } from '@headlines/config'
import { ragResponseSchema } from '@headlines/models/schemas'
import { logger } from '@headlines/utils-shared'

const SYNTHESIZER_MODEL = settings.LLM_MODEL_SYNTHESIS

function assembleContext(ragResults, wikiResults, searchResults) {
  const dbContext =
    ragResults.length > 0
      ? ragResults
          .map(
            (match) =>
              `- [Similarity: ${match.score.toFixed(3)}] ${match.metadata.headline}: ${match.metadata.summary}`
          )
          .join('\n')
      : 'None'

  const wikiContext =
    wikiResults.length > 0
      ? wikiResults
          .map(
            (res) => `- [Quality: ${res.validation.quality}] ${res.title}: ${res.summary}`
          )
          .join('\n')
      : 'None'

  const searchContext =
    searchResults.length > 0
      ? searchResults
          .map((res) => `- [${res.title}](${res.link}): ${res.snippet}`)
          .join('\n')
      : 'None'

  return `---
Internal Database Context:
${dbContext}
---
Wikipedia Context:
${wikiContext}
---
Search Results Context:
${searchContext}
---`
}

function formatThoughts(plan, context, groundednessResult) {
  const thoughts = `
**THOUGHT PROCESS: THE PLAN**
${plan.plan.map((step) => `- ${step}`).join('\n')}

**REASONING:**
${plan.reasoning}

**RETRIEVED CONTEXT:**
- **Internal RAG Search:** ${context.ragResults.length} item(s) found.
${context.ragResults.map((r) => `  - [Score: ${r.score.toFixed(2)}] ${r.metadata.headline}`).join('\n')}

- **Wikipedia Search:** ${context.wikiResults.length} article(s) found.
${context.wikiResults.map((w) => `  - **Query:** "${w.query}"\n    - **Result:** ${w.title}: ${w.summary.substring(0, 100)}...`).join('\n')}

- **Web Search:** ${context.searchResults.length} result(s) found.
${context.searchResults.map((s) => `  - **Query:** "${plan.user_query}"\n    - **Result:** ${s.title}: ${s.snippet.substring(0, 100)}...`).join('\n')}

**FINAL CHECK:**
- **Groundedness Passed:** ${groundednessResult.is_grounded ? 'CONFIRMED' : 'FAILED'}
`
  return thoughts.trim().replace(/\n\n+/g, '\n\n')
}

function buildHtmlFromAnswerParts(answerParts) {
  if (!answerParts || answerParts.length === 0) return ''
  return answerParts
    .map((part) => {
      const sourceClass = {
        rag: 'rag-source',
        wiki: 'wiki-source',
        search: 'llm-source',
        llm: '',
      }[part.source]
      return sourceClass ? `<span class="${sourceClass}">${part.text}</span>` : part.text
    })
    .join('')
}

export async function generateFinalResponse({ plan, context }) {
  const fullContextString = assembleContext(
    context.ragResults,
    context.wikiResults,
    context.searchResults
  )

  logger.info(`[RAG Generation] Calling Synthesizer Agent with ${SYNTHESIZER_MODEL}...`)
  const synthesizerResponse = await callLanguageModel({
    modelName: SYNTHESIZER_MODEL,
    systemPrompt: getSynthesizerPrompt(),
    userContent: `CONTEXT:\n${fullContextString}\n\nPLAN:\n${JSON.stringify(
      plan.plan,
      null,
      2
    )}\n\nUSER'S QUESTION: "${plan.user_query}"`,
    isJson: true,
  })

  const validation = ragResponseSchema.safeParse(synthesizerResponse)
  if (!validation.success) {
    logger.error(
      { err: validation.error },
      '[RAG Generation] Synthesizer Agent failed to return valid structured JSON.'
    )
    return {
      answer: 'The AI synthesizer failed to generate a structured response.',
      thoughts: 'An error occurred during the final synthesis step.',
    }
  }

  const answerParts = validation.data.answer_parts
  const rawResponseText = answerParts.map((p) => p.text).join(' ')

  const groundednessResult = await checkGroundedness(rawResponseText, fullContextString)
  const thoughts = formatThoughts(plan, context, groundednessResult)

  let finalAnswer
  if (groundednessResult.is_grounded) {
    finalAnswer = buildHtmlFromAnswerParts(answerParts)
  } else {
    logger.warn('[RAG Pipeline] Groundedness check failed. Returning safe response.')
    finalAnswer =
      'I was unable to construct a reliable answer from the available sources. The context may be insufficient or conflicting.'
  }

  return { answer: finalAnswer, thoughts }
}

```

## ðŸ“„ src/rag/orchestrator.js
*Lines: 41, Size: 1.52 KB*

```javascript
// packages/ai-services/src/rag/orchestrator.js
import { retrieveContextForQuery } from './retrieval.js'
import { assessContextQuality } from './validation.js'
import { generateFinalResponse } from './generation.js'
import { runPlannerAgent } from './planner.js'
import { logger } from '@headlines/utils-shared'

export async function processChatRequest(messages) {
  logger.info('--- [RAG Pipeline Start] ---')

  logger.info('[RAG Pipeline] Step 1: Planning Phase Started...')
  const plan = await runPlannerAgent(messages)
  logger.info('[RAG Pipeline] Step 1: Planning Phase Completed.')

  logger.info('[RAG Pipeline] Step 2: Retrieval Phase Started...')
  const initialContext = await retrieveContextForQuery(plan, messages, 'ragOnly')
  const initialQuality = assessContextQuality(initialContext.ragResults, [], [])

  let finalContext = initialContext

  if (initialQuality.hasHighConfidenceRAG) {
    logger.info(
      '[RAG Pipeline] High confidence RAG hit found. Short-circuiting retrieval.'
    )
  } else {
    logger.info('[RAG Pipeline] RAG context insufficient. Proceeding to full retrieval.')
    finalContext = await retrieveContextForQuery(plan, messages, 'full')
  }
  logger.info('[RAG Pipeline] Step 2: Retrieval Phase Completed.')

  logger.info('[RAG Pipeline] Step 3: Synthesis Phase Started...')
  const finalResponse = await generateFinalResponse({
    plan,
    context: finalContext,
  })
  logger.info('[RAG Pipeline] Step 3: Synthesis Phase Completed.')

  logger.info('--- [RAG Pipeline End] ---')
  return finalResponse
}

```

## ðŸ“„ src/rag/planner.js
*Lines: 46, Size: 1.26 KB*

```javascript
// packages/ai-services/src/rag/planner.js
import { callLanguageModel } from '../lib/langchain.js'
import { PLANNER_PROMPT } from './prompts.js'
import { settings } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

const PLANNER_MODEL = settings.LLM_MODEL_UTILITY

export async function runPlannerAgent(messages) {
  const conversationText = messages
    .map((m) => `${m.role.toUpperCase()}: ${m.content}`)
    .join('\n\n')

  logger.info(`[Planner Agent] Generating plan with ${PLANNER_MODEL}...`)

  const response = await callLanguageModel({
    modelName: PLANNER_MODEL,
    systemPrompt: PLANNER_PROMPT,
    userContent: conversationText,
    isJson: true,
  })

  if (response.error) {
    throw new Error(`Planner Agent failed: ${response.error}`)
  }

  // --- START OF THE FIX ---
  // Replaced browser-specific logger.groupCollapsed with a single structured log object.
  // This is safe to run on the server.
  logger.trace(
    {
      agent: 'Planner Agent',
      planDetails: {
        userQuery: response.user_query,
        reasoning: response.reasoning,
        planSteps: response.plan,
        searchQueries: response.search_queries,
      },
    },
    '[Planner Agent] Plan Generated'
  )
  // --- END OF THE FIX ---

  return response
}

```

## ðŸ“„ src/rag/prompts.js
*Lines: 103, Size: 6.08 KB*

```javascript
// File: packages/ai-services/src/rag/prompts.js (Unabridged and Corrected)

export const PLANNER_PROMPT = `You are an expert AI Planner. Your job is to analyze the user's query and conversation history to create a step-by-step plan for an AI Synthesizer Agent to follow. You also create a list of optimized search queries for a Retrieval Agent.

**Conversation History:**
{CONVERSATION_HISTORY}

**Latest User Query:**
"{USER_QUERY}"

**Your Task:**
1.  **Analyze the User's Intent:** Understand what the user is truly asking for.
2.  **Formulate a Plan:** Create a clear, step-by-step plan for the Synthesizer Agent.
3.  **Generate Search Queries:** Create an array of 1-3 optimized, self-contained search queries. **CRITICAL JSON RULE:** If a query within the 'search_queries' array requires double quotes, you MUST escape them with a backslash. For example: ["\\"Troels Holch Povlsen\\" sons", "Bestseller founder"].

**Example 1:**
User Query: "Which Danish Rich List person is involved in Technology?"
History: (empty)
Your JSON Output:
{
  "user_query": "Which Danish Rich List person is involved in Technology?",
  "reasoning": "The user wants a list of wealthy Danes involved in technology. I need to identify these individuals from the context and then filter them based on their tech involvement.",
  "plan": [
    "Scan all context to identify every unique individual mentioned who is on the Danish Rich List.",
    "For each person, look for evidence of direct involvement in the technology sector.",
    "Filter out individuals with no clear connection to technology.",
    "Synthesize the findings into a helpful list of names, citing their connection to technology.",
    "If no one is found, state that clearly."
  ],
  "search_queries": ["Danish Rich List technology involvement", "Wealthy Danish tech investors", "Danish tech company founders"]
}

**Example 2:**
User Query: "Does Troels Holch Povlsen have sons?"
History: (assistant previously mentioned Bestseller's founder)
Your JSON Output:
{
  "user_query": "Does Troels Holch Povlsen have sons?",
  "reasoning": "The user is asking a direct factual question about a specific person's family. The search queries must be precise.",
  "plan": [
      "Scan context for any mention of 'Troels Holch Povlsen' and his family, specifically children or sons.",
      "Extract the names of his sons if mentioned.",
      "Synthesize a complete and helpful answer, stating the names of the sons and any additional relevant context provided."
  ],
  "search_queries": ["\\"Troels Holch Povlsen\\" sons", "\\"Troels Holch Povlsen\\" children", "\\"Bestseller\\" founder family"]
}

Respond ONLY with a valid JSON object with the specified structure.
`

export const getSynthesizerPrompt =
  () => `You are an elite, fact-based intelligence analyst. Your SOLE task is to execute the provided "PLAN" using only the "CONTEXT" to answer the "USER'S QUESTION". You operate under a strict "ZERO HALLUCINATION" protocol. Your response must be confident, direct, and sound like a human expert.

**PRIMARY DIRECTIVE:**
Synthesize information from all sources in the "CONTEXT" into a single, cohesive, and well-written answer. Directly address the user's question and enrich it with relevant surrounding details found in the context.

**EXAMPLE TONE:**
-   **Bad:** "According to the context, Bestseller was founded by Troels Holch Povlsen."
-   **Good:** "Bestseller was founded in 1975 by Troels Holch Povlsen and his wife, Merete Bech Povlsen. The company is now run by their son, Anders Holch Povlsen."

**CRITICAL RULES OF ENGAGEMENT:**
1.  **NO OUTSIDE KNOWLEDGE:** You are forbidden from using any information not present in the provided "CONTEXT".
2.  **DIRECT ATTRIBUTION:** You MUST still cite your sources inline for the UI. Wrap facts from the Internal DB with <rag>tags</rag>, from Wikipedia with <wiki>tags</wiki>, and from Search Results with <search>tags</search>. The user will not see these tags, but they are essential for the system.
3.  **BE CONFIDENT AND DIRECT:** Present the synthesized facts as a definitive answer.
4.  **INSUFFICIENT DATA:** If the context is insufficient to answer the question at all, respond with EXACTLY: "I do not have sufficient information in my sources to answer that question."
5.  **DO NOT OFFER HELP (CRITICAL):** You MUST NOT end your response by offering to search for more information, provide more details, or ask follow-up questions. Your answer should be a complete, self-contained statement of facts.

**DO NOT:**
-   Use phrases like "According to the context provided...", "The sources state...", or "Based on the information...".
-   Apologize for not knowing or mention your limitations.
-   Talk about your process in the final answer.
-   Speculate or infer beyond what is explicitly stated in the context.

Answer the question directly and authoritatively, as if you are a world-class analyst presenting your verified findings.`

export const GROUNDEDNESS_CHECK_PROMPT = `You are a meticulous fact-checker AI. Your task is to determine if the "Proposed Response" is strictly grounded in the "Provided Context". A response is grounded if and only if ALL of its claims can be directly verified from the context.

**Provided Context:**
---
{CONTEXT}
---

**Proposed Response:**
---
{RESPONSE}
---

Analyze the "Proposed Response" sentence by sentence.

**Respond ONLY with a valid JSON object with the following structure:**
{
  "is_grounded": boolean, // true if ALL claims in the response are supported by the context, otherwise false.
  "unsupported_claims": [
    // List any specific claims from the response that are NOT supported by the context.
    "Claim 1 that is not supported.",
    "Claim 2 that is not supported."
  ]
}

If the response is fully supported, "unsupported_claims" should be an empty array. If the "Proposed Response" states that it cannot answer the question, consider it grounded.`

export const FAILED_GROUNDEDNESS_PROMPT = `I could not form a reliable answer based on the available information. The initial response I generated may have contained information not supported by the sources. For accuracy, please ask a more specific question or try rephrasing your request.`

```

## ðŸ“„ src/rag/retrieval.js
*Lines: 134, Size: 3.8 KB*

```javascript
// packages/ai-services/src/rag/retrieval.js
import { OpenAI } from 'openai'
import { Pinecone } from '@pinecone-database/pinecone'
import { generateQueryEmbeddings } from '../embeddings/embeddings.js'
import {
  fetchBatchWikipediaSummaries,
  validateWikipediaContent,
} from '../search/wikipedia.js'
import { getGoogleSearchResults } from '../search/serpapi.js'
import { env } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

let openAIClient, pineconeIndex
function initializeClients() {
  if (!openAIClient) {
    if (env.OPENAI_API_KEY) {
      openAIClient = new OpenAI({ apiKey: env.OPENAI_API_KEY })
    }
    if (env.PINECONE_API_KEY) {
      const pc = new Pinecone({ apiKey: env.PINECONE_API_KEY })
      pineconeIndex = pc.index(env.PINECONE_INDEX_NAME)
    }
  }
}

const SIMILARITY_THRESHOLD = 0.38

async function fetchPineconeContext(queries, exclude_entities = []) {
  initializeClients()
  if (!pineconeIndex) {
    logger.warn(
      '[RAG Retrieval] Pinecone is not configured. Skipping internal DB search.'
    )
    return []
  }

  const queryEmbeddings = await Promise.all(
    queries.map((q) => generateQueryEmbeddings(q))
  )
  const allQueryEmbeddings = queryEmbeddings.flat()

  const pineconePromises = allQueryEmbeddings.map((embedding) =>
    pineconeIndex.query({
      topK: 5,
      vector: embedding,
      includeMetadata: true,
    })
  )
  const pineconeResponses = await Promise.all(pineconePromises)

  const uniqueMatches = new Map()
  pineconeResponses.forEach((response) => {
    response?.matches?.forEach((match) => {
      if (
        !uniqueMatches.has(match.id) ||
        match.score > uniqueMatches.get(match.id).score
      ) {
        uniqueMatches.set(match.id, match)
      }
    })
  })

  const results = Array.from(uniqueMatches.values())
    .filter((match) => match.score >= SIMILARITY_THRESHOLD)
    .sort((a, b) => b.score - a.score)
    .slice(0, 5)

  logger.groupCollapsed(`[RAG Retrieval] Pinecone Results (${results.length})`)
  results.forEach((match) => {
    logger.trace(`- Score: ${match.score.toFixed(4)} | ID: ${match.id}`)
    logger.trace(`  Headline: ${match.metadata.headline}`)
  })
  logger.groupEnd()

  return results
}

async function fetchValidatedWikipediaContext(entities) {
  const wikiResults = await fetchBatchWikipediaSummaries(entities)
  const validWikiResults = []
  for (const res of wikiResults.filter((r) => r.success)) {
    const validation = await validateWikipediaContent(res.summary)
    if (validation.valid) {
      validWikiResults.push({ ...res, validation })
    }
  }

  logger.groupCollapsed(`[RAG Retrieval] Wikipedia Results (${validWikiResults.length})`)
  validWikiResults.forEach((res) => {
    logger.trace(`- Title: ${res.title}`)
    logger.trace(`  Summary: ${res.summary.substring(0, 200)}...`)
  })
  logger.groupEnd()

  return validWikiResults
}

export async function retrieveContextForQuery(plan, messages, mode = 'full') {
  const { search_queries, user_query } = plan

  const pineconeResults = await fetchPineconeContext(search_queries)

  if (mode === 'ragOnly') {
    return {
      ragResults: pineconeResults,
      wikiResults: [],
      searchResults: [],
    }
  }

  const [wikipediaResults, searchResultsObj] = await Promise.all([
    fetchValidatedWikipediaContext(search_queries),
    getGoogleSearchResults(user_query),
  ])

  const searchResults = searchResultsObj.success ? searchResultsObj.results : []

  logger.groupCollapsed(
    `[RAG Retrieval] SerpAPI Google Search Results (${searchResults.length})`
  )
  searchResults.forEach((res) => {
    logger.trace(`- Title: ${res.title}`)
    logger.trace(`  Link: ${res.link}`)
    logger.trace(`  Snippet: ${res.snippet}`)
  })
  logger.groupEnd()

  return {
    ragResults: pineconeResults,
    wikiResults: wikipediaResults,
    searchResults: searchResults,
  }
}

```

## ðŸ“„ src/rag/validation.js
*Lines: 82, Size: 2.9 KB*

```javascript
// packages/ai-services/src/rag/validation.js
import { settings } from '@headlines/config'
import { callLanguageModel } from '../lib/langchain.js'
import { GROUNDEDNESS_CHECK_PROMPT } from './prompts.js'
import { logger } from '@headlines/utils-shared'

const HIGH_CONFIDENCE_THRESHOLD = 0.75
const SIMILARITY_THRESHOLD = 0.38

export function assessContextQuality(ragResults, wikiResults, searchResults) {
  const ragScore = ragResults.length > 0 ? Math.max(...ragResults.map((r) => r.score)) : 0
  const highQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'high'
  ).length
  const mediumQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'medium'
  ).length
  const wikiScore = highQualityWiki > 0 ? 0.7 : mediumQualityWiki > 0 ? 0.5 : 0
  const searchScore = searchResults.length > 0 ? 0.6 : 0

  const combinedScore = Math.max(ragScore, wikiScore, searchScore)

  return {
    hasHighConfidenceRAG: ragScore >= HIGH_CONFIDENCE_THRESHOLD,
    hasSufficientContext: combinedScore >= SIMILARITY_THRESHOLD,
    ragResultCount: ragResults.length,
    wikiResultCount: wikiResults.length,
    searchResultCount: searchResults.length,
    highQualityWikiCount: highQualityWiki,
    maxSimilarity: ragScore,
    combinedConfidence: combinedScore,
    hasMultipleSources:
      (ragResults.length > 0 ? 1 : 0) +
        (wikiResults.length > 0 ? 1 : 0) +
        (searchResults.length > 0 ? 1 : 0) >
      1,
    hasHighQualityContent: ragScore >= HIGH_CONFIDENCE_THRESHOLD || highQualityWiki > 0,
  }
}

export async function checkGroundedness(responseText, contextString) {
  logger.info('[RAG Validation] Performing Groundedness Check...')
  if (
    responseText.trim() ===
    'I do not have sufficient information in my sources to answer that question.'
  ) {
    logger.info('[RAG Validation] PASSED: Bot correctly stated insufficient info.')
    return { is_grounded: true, unsupported_claims: [] }
  }

  try {
    const prompt = GROUNDEDNESS_CHECK_PROMPT.replace('{CONTEXT}', contextString).replace(
      '{RESPONSE}',
      responseText
    )

    const result = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: prompt,
      userContent: 'Perform the groundedness check based on the system prompt.',
      isJson: true,
    })

    if (result.error) {
      throw new Error(result.error)
    }

    if (result.is_grounded) {
      logger.info('[RAG Validation] PASSED: Response is grounded in sources.')
    } else {
      logger.warn('[RAG Validation] FAILED: Response contains unsupported claims.')
      logger.groupCollapsed('Unsupported Claims Details')
      result.unsupported_claims.forEach((claim) => logger.warn(`- ${claim}`))
      logger.groupEnd()
    }
    return result
  } catch (error) {
    logger.error({ err: error }, '[RAG Validation] Error during verification:')
    return { is_grounded: false, unsupported_claims: ['Fact-checking system failed.'] }
  }
}

```

## ðŸ“„ src/search/search.js
*Lines: 85, Size: 3.09 KB*

```javascript
import axios from 'axios'
import NewsAPI from 'newsapi'
import { env } from '@headlines/config'
import { logger, apiCallTracker } from '@headlines/utils-shared'

const { SERPER_API_KEY, NEWSAPI_API_KEY } = env
const serperClient = SERPER_API_KEY
  ? axios.create({
      baseURL: 'https://google.serper.dev',
      headers: { 'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json' },
    })
  : null
const newsapi = NEWSAPI_API_KEY ? new NewsAPI(NEWSAPI_API_KEY) : null

if (!serperClient)
  logger.warn(
    'SERPER_API_KEY not found. Google Search dependent functions will be disabled.'
  )
if (!newsapi)
  logger.warn('NEWSAPI_API_KEY not found. NewsAPI dependent functions will be disabled.')

async function withRetry(apiCall, serviceName, maxRetries = 2) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await apiCall()
    } catch (error) {
      const isRetryable = error.response && error.response.status >= 500
      if (!isRetryable || attempt === maxRetries) {
        logger.error(
          { err: error?.response?.data || error },
          `${serviceName} search failed.`
        )
        return { success: false, error: error.message, results: [] }
      }
      const delay = 1000 * Math.pow(2, attempt - 1)
      logger.warn(`[${serviceName}] Attempt ${attempt} failed. Retrying in ${delay}ms...`)
      await new Promise((res) => setTimeout(res, delay))
    }
  }
}

export async function findAlternativeSources(headline) {
  if (!serperClient) return { success: false, results: [] }
  return withRetry(async () => {
    apiCallTracker.recordCall('serper_news')
    const response = await serperClient.post('/news', { q: headline })
    return { success: true, results: response.data.news || [] }
  }, 'Serper News')
}
export async function performGoogleSearch(query) {
  if (!serperClient) return { success: false, snippets: 'SERPER_API_KEY not configured.' }
  return withRetry(async () => {
    apiCallTracker.recordCall('serper_search')
    const response = await serperClient.post('/search', { q: query })
    const organicResults = response.data.organic || []
    if (organicResults.length > 0) {
      const snippets = organicResults
        .slice(0, 5)
        .map((res) => `- ${res.title}: ${res.snippet}`)
        .join('\n')
      return { success: true, snippets }
    }
    return { success: false, snippets: 'No search results found.' }
  }, 'Serper Search')
}
export async function findNewsApiArticlesForEvent(headline) {
  if (!newsapi) return { success: false, snippets: 'NewsAPI key not configured.' }
  return withRetry(async () => {
    apiCallTracker.recordCall('newsapi_search')
    const response = await newsapi.v2.everything({
      q: `"${headline}"`,
      pageSize: 5,
      sortBy: 'relevancy',
      language: 'en,da,sv,no',
    })
    if (response.articles && response.articles.length > 0) {
      const snippets = response.articles
        .map((a) => `- ${a.title} (${a.source.name}): ${a.description || ''}`)
        .join('\n')
      return { success: true, snippets }
    }
    return { success: false, snippets: 'No related articles found.' }
  }, 'NewsAPI')
}

```

## ðŸ“„ src/search/serpapi.js
*Lines: 61, Size: 1.8 KB*

```javascript
// packages/ai-services/src/search/serpapi.js
import { getJson } from 'serpapi'
import { env } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

const searchCache = new Map()
const CACHE_TTL = 1000 * 60 * 60 // 1 hour

export async function getGoogleSearchResults(query) {
  if (!env.SERPAPI_API_KEY) {
    logger.warn('[SerpAPI] SERPAPI_API_KEY is not configured. Skipping web search.')
    return { success: true, results: [] }
  }

  if (!query) {
    return { success: false, error: 'Query is required.' }
  }

  const cacheKey = `serpapi_${query.toLowerCase().trim()}`
  if (searchCache.has(cacheKey)) {
    const cached = searchCache.get(cacheKey)
    if (Date.now() - cached.timestamp < CACHE_TTL) {
      logger.info(`[SerpAPI Cache] Hit for query: "${query}"`)
      return cached.data
    }
  }

  logger.info(`[SerpAPI] Performing live search for: "${query}"`)

  try {
    const response = await getJson({
      api_key: env.SERPAPI_API_KEY,
      engine: 'google',
      q: query,
      location: 'United States',
      gl: 'us',
      hl: 'en',
    })

    const organicResults = response.organic_results || []
    const answerBox = response.answer_box ? [response.answer_box] : []

    const formattedResults = [...answerBox, ...organicResults]
      .map((item) => ({
        title: item.title,
        link: item.link,
        snippet: item.snippet || item.answer || item.result,
        source: 'Google Search',
      }))
      .filter((item) => item.snippet)
      .slice(0, 5)

    const result = { success: true, results: formattedResults }
    searchCache.set(cacheKey, { data: result, timestamp: Date.now() })
    return result
  } catch (error) {
    logger.error({ err: error }, '[SerpAPI Error]')
    return { success: false, error: `Failed to fetch search results: ${error.message}` }
  }
}

```

## ðŸ“„ src/search/wikipedia.js
*Lines: 131, Size: 4.35 KB*

```javascript
import { logger, apiCallTracker } from '@headlines/utils-shared'
import { settings } from '@headlines/config'
// --- START: DEFINITIVE FIX ---
// Import the centrally defined, corrected chain instead of recreating it locally.
import { disambiguationChain } from '../chains/index.js'
// --- END: DEFINITIVE FIX ---
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '@headlines/models/schemas'

const WIKI_API_ENDPOINT = 'https://en.wikipedia.org/w/api.php'
const WIKI_SUMMARY_LENGTH = 750

// --- START: DEFINITIVE FIX ---
// The entire local definition of the chain is now removed.
// --- END: DEFINITIVE FIX ---

async function fetchWithRetry(url, options, maxRetries = 2) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url, options)
      if (!response.ok) throw new Error(`API returned status ${response.status}`)
      return response
    } catch (error) {
      if (attempt === maxRetries) throw error
      const delay = 1000 * Math.pow(2, attempt - 1)
      logger.warn(
        `[Wikipedia Fetch] Attempt ${attempt} failed for ${url}. Retrying in ${delay}ms...`
      )
      await new Promise((res) => setTimeout(res, delay))
    }
  }
}

export async function fetchWikipediaSummary(query) {
  if (!query) return { success: false, error: 'Query cannot be empty.' }
  try {
    apiCallTracker.recordCall('wikipedia')
    const searchParams = new URLSearchParams({
      action: 'query',
      list: 'search',
      srsearch: query,
      srlimit: '5',
      format: 'json',
    })
    const searchResponse = await fetchWithRetry(
      `${WIKI_API_ENDPOINT}?${searchParams.toString()}`
    )
    const searchData = await searchResponse.json()
    const searchResults = searchData.query.search
    if (!searchResults || searchResults.length === 0)
      throw new Error(`No search results for "${query}".`)

    let best_title = null
    try {
      const userContent = `Original Query: "${query}"\n\nSearch Results:\n${JSON.stringify(searchResults.map((r) => ({ title: r.title, snippet: r.snippet })))}`

      // --- START: DEFINITIVE FIX ---
      // Call the imported, corrected chain.
      const disambiguationResponse = await disambiguationChain({ inputText: userContent })
      // --- END: DEFINITIVE FIX ---

      if (
        disambiguationResponse &&
        !disambiguationResponse.error &&
        disambiguationResponse.best_title
      ) {
        best_title = disambiguationResponse.best_title
      }
    } catch (e) {
      logger.warn({ err: e }, `Disambiguation chain failed for query "${query}".`)
    }

    if (!best_title) {
      best_title = searchResults[0].title
      logger.info(
        `[Wikipedia] AI disambiguation failed or returned null. Falling back to top search result: "${best_title}"`
      )
    }

    const summaryParams = new URLSearchParams({
      action: 'query',
      prop: 'extracts',
      exintro: 'true',
      explaintext: 'true',
      titles: best_title,
      format: 'json',
      redirects: '1',
    })
    const summaryResponse = await fetchWithRetry(
      `${WIKI_API_ENDPOINT}?${summaryParams.toString()}`
    )
    const summaryData = await summaryResponse.json()
    const pages = summaryData.query.pages
    const pageId = Object.keys(pages)[0]
    const summary = pages[pageId]?.extract
    if (!summary) throw new Error(`Could not extract summary for page "${best_title}".`)

    const conciseSummary =
      summary.length > WIKI_SUMMARY_LENGTH
        ? summary.substring(0, WIKI_SUMMARY_LENGTH) + '...'
        : summary
    return { success: true, summary: conciseSummary, title: best_title, query }
  } catch (error) {
    logger.warn(`Wikipedia lookup for "${query}" failed: ${error.message}`)
    return { success: false, error: error.message, query }
  }
}

export async function fetchBatchWikipediaSummaries(queries) {
  const promises = queries.map((q) => fetchWikipediaSummary(q))
  return Promise.all(promises)
}

export async function validateWikipediaContent(text) {
  const lowerText = text.toLowerCase()
  const isDisambiguation =
    lowerText.includes('may refer to:') || lowerText.includes('is a list of')
  if (isDisambiguation) {
    return {
      valid: false,
      quality: 'low',
      reason: 'Disambiguation page content detected.',
    }
  }
  return {
    valid: true,
    quality: 'high',
    reason: 'Content appears to be a valid summary.',
  }
}

```

## ðŸ“„ src/shared/agents/contactAgent.js
*Lines: 54, Size: 1.71 KB*

```javascript
// packages/ai-services/src/shared/agents/contactAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { findContactSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionContacts } from '@headlines/prompts'
import { performGoogleSearch } from '../../search/search.js'

const getFinderAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionContacts,
    zodSchema: findContactSchema,
  })

export async function findContactDetails(person) {
  const contactFinderAgent = getFinderAgent()
  logger.info(`[Contact Research Agent] Initiated for: ${person.reachOutTo}`)

  const company = person.contactDetails?.company || ''
  const queries = [
    `"${person.reachOutTo}" ${company} email address`,
    `"${person.reachOutTo}" contact information`,
  ]

  let combinedSnippets = ''
  for (const query of queries) {
    const searchResult = await performGoogleSearch(query)
    if (searchResult.success && searchResult.snippets) {
      combinedSnippets += `\n--- Results for query: "${query}" ---\n${searchResult.snippets}`
    }
  }

  if (!combinedSnippets) {
    logger.warn(`[Contact Research Agent] No search results for "${person.reachOutTo}".`)
    return { email: null }
  }

  const response = await contactFinderAgent.execute(combinedSnippets)

  if (response.error || !response.email) {
    logger.warn(
      `[Contact Research Agent] LLM failed to extract details for "${person.reachOutTo}".`
    )
    return { email: null }
  }

  logger.info(
    { details: response },
    `[Contact Research Agent] Found details for "${person.reachOutTo}".`
  )
  return response
}

```

## ðŸ“„ src/shared/agents/emailAgents.js
*Lines: 77, Size: 2.68 KB*

```javascript
// packages/ai-services/src/shared/agents/emailAgents.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { emailSubjectSchema, emailIntroSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionEmailSubject, instructionEmailIntro } from '@headlines/prompts'

const getAgent = (systemPrompt, zodSchema) =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt,
    zodSchema,
  })

export async function generateEmailSubjectLine(events) {
  const subjectLineAgent = getAgent(instructionEmailSubject, emailSubjectSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const response = await subjectLineAgent.execute(JSON.stringify(eventPayload))
    if (response.error || !response.subject_headline) {
      logger.warn('AI failed to generate a custom email subject line.', response)
      return 'Key Developments' // Fallback
    }
    return response.subject_headline
  } catch (error) {
    logger.error({ err: error }, 'Error in generateEmailSubjectLine')
    return 'Key Developments' // Fallback
  }
}

export async function generatePersonalizedIntro(user, events) {
  const introAgent = getAgent(instructionEmailIntro, emailIntroSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const payload = {
      firstName: user.firstName,
      events: eventPayload,
    }
    const response = await introAgent.execute(JSON.stringify(payload))

    if (response.error || !response.greeting) {
      logger.warn('AI failed to generate a personalized intro.', response)
      return {
        greeting: `Dear ${user.firstName},`,
        body: 'Here are the latest relevant wealth events we have identified for your review.',
        bullets: events
          .slice(0, 2)
          .map(
            (e) =>
              `A key development regarding ${e.synthesized_headline.substring(0, 40)}...`
          ),
        signoff: ['We wish you a fruitful day!', 'The team at Wealth Watch'],
      }
    }
    return response
  } catch (error) {
    logger.error({ err: error }, 'Error in generatePersonalizedIntro')
    return {
      greeting: `Dear ${user.firstName},`,
      body: 'Here are the latest relevant wealth events we have identified for your review.',
      bullets: events
        .slice(0, 2)
        .map(
          (e) =>
            `A key development regarding ${e.synthesized_headline.substring(0, 40)}...`
        ),
      signoff: ['We wish you a fruitful day!', 'The team at Wealth Watch'],
    }
  }
}

```

## ðŸ“„ src/shared/agents/entityAgent.js
*Lines: 67, Size: 2.36 KB*

```javascript
// packages/ai-services/src/shared/agents/entityAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { entitySchema, canonicalizerSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionEntity, instructionCanonicalizer } from '@headlines/prompts'

const getEntityExtractorAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionEntity,
    zodSchema: entitySchema,
  })

const getEntityCanonicalizerAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionCanonicalizer,
    zodSchema: canonicalizerSchema,
  })

// DEFINITIVE FIX:
// The agent is no longer instantiated at the module level.
// We now export the function that creates it, preventing the side-effect during import.
export const entityCanonicalizerAgent = getEntityCanonicalizerAgent

export async function extractEntities(text) {
  const entityExtractorAgent = getEntityExtractorAgent()
  // Now we call the function to get the agent instance when we need it.
  const canonicalizer = getEntityCanonicalizerAgent()

  if (!text) return []

  try {
    const response = await entityExtractorAgent.execute(`Article Text:\n${text}`)

    if (response.error) {
      throw new Error(response.error)
    }

    const { reasoning, entities } = response
    logger.info(`[Query Planner Agent] Reasoning: ${reasoning}`)
    if (!entities || !Array.isArray(entities)) return []

    const canonicalizationPromises = entities
      .map((entity) => entity.replace(/\s*\(.*\)\s*/g, '').trim())
      .filter(Boolean)
      .map(async (entity) => {
        const canonResponse = await canonicalizer.execute(entity)
        if (canonResponse && !canonResponse.error && canonResponse.canonical_name) {
          logger.trace(`Canonicalized "${entity}" -> "${canonResponse.canonical_name}"`)
          return canonResponse.canonical_name
        }
        return null
      })

    const canonicalEntities = await Promise.all(canonicalizationPromises)
    const uniqueEntities = [...new Set(canonicalEntities.filter(Boolean))]

    logger.info({ entities: uniqueEntities }, `Final list of canonical entities for RAG.`)
    return uniqueEntities
  } catch (error) {
    logger.warn({ err: error }, 'Wikipedia query planning (entity extraction) failed.')
    return []
  }
}

```

## ðŸ“„ src/shared/agents/executiveSummaryAgent.js
*Lines: 36, Size: 1.25 KB*

```javascript
// packages/ai-services/src/agents/executiveSummaryAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { executiveSummarySchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings } from '@headlines/config/node'
import { instructionExecutiveSummary } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionExecutiveSummary,
    zodSchema: executiveSummarySchema,
  })

export async function generateExecutiveSummary(judgeVerdict, runStats) {
  const agent = getAgent()
  try {
    const payload = {
      freshHeadlinesFound: runStats.freshHeadlinesFound,
      judgeVerdict: judgeVerdict || { event_judgements: [], opportunity_judgements: [] },
    }

    const response = await agent.execute(JSON.stringify(payload))

    if (response.error || !response.summary) {
      logger.warn('AI failed to generate an executive summary.', response)
      return 'AI failed to generate a summary for this run.'
    }

    return response.summary
  } catch (error) {
    logger.error({ err: error }, 'Error in generateExecutiveSummary')
    return 'An unexpected error occurred while generating the executive summary.'
  }
}

```

## ðŸ“„ src/shared/agents/opportunityAgent.js
*Lines: 70, Size: 2.34 KB*

```javascript
// packages/ai-services/src/agents/opportunityAgent.js
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { opportunitySchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { getInstructionOpportunities } from '@headlines/prompts'

const getOppAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionOpportunities,
    zodSchema: opportunitySchema,
  })

export async function generateOpportunitiesFromEvent(
  synthesizedEvent,
  articlesInCluster
) {
  const opportunityGeneratorAgent = getOppAgent()

  const highestRelevanceArticle = articlesInCluster.reduce((max, current) =>
    (current.relevance_article || 0) > (max.relevance_article || 0) ? current : max
  )

  const fullText = articlesInCluster
    .map((a) => (a.articleContent?.contents || []).join('\n'))
    .join('\n\n')

  const inputText = `
        Synthesized Event Headline: ${synthesizedEvent.synthesized_headline}
        Synthesized Event Summary: ${synthesizedEvent.synthesized_summary}
        Key Individuals already identified: ${JSON.stringify(synthesizedEvent.key_individuals)}
        Source Article Snippets: ${truncateString(fullText, LLM_CONTEXT_MAX_CHARS)}
    `

  const response = await opportunityGeneratorAgent.execute(inputText)

  if (response.error || !response.opportunities) {
    logger.warn(
      { event: synthesizedEvent.synthesized_headline, details: response },
      `Opportunity generation failed.`
    )
    return []
  }

  const validOpportunities = (response.opportunities || []).filter(
    (opp) =>
      opp.likelyMMDollarWealth === null || // Keep opportunities where wealth is unknown
      opp.likelyMMDollarWealth >= settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  )

  const opportunitiesWithSource = validOpportunities.map((opp) => ({
    ...opp,
    event_key: synthesizedEvent.event_key,
    sourceArticleId: highestRelevanceArticle._id,
  }))

  logger.info(
    { details: opportunitiesWithSource },
    `[Opportunity Agent] Generated ${
      opportunitiesWithSource.length
    } opportunity/ies from event "${truncateString(
      synthesizedEvent.synthesized_headline,
      50
    )}"`
  )
  return opportunitiesWithSource
}

```

## ðŸ“„ src/shared/agents/synthesisAgent.js
*Lines: 97, Size: 2.96 KB*

```javascript
// packages/ai-services/src/shared/agents/synthesisAgent.js
import { truncateString, logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { synthesisSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config'
import { instructionSynthesize } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: [
      instructionSynthesize.whoYouAre,
      instructionSynthesize.whatYouDo,
      ...instructionSynthesize.guidelines,
      instructionSynthesize.outputFormatDescription,
    ].join('\n\n'),
    zodSchema: synthesisSchema,
  })

export async function synthesizeEvent(
  articlesInCluster,
  historicalContext,
  wikipediaContext,
  newsApiContext
) {
  const eventSynthesizerAgent = getAgent()

  const todayPayload = articlesInCluster.map((a) => ({
    headline: a.headline,
    source: a.newspaper,
    full_text: truncateString(
      (a.articleContent?.contents || []).join('\n'),
      LLM_CONTEXT_MAX_CHARS / (articlesInCluster.length || 1)
    ),
    key_individuals: a.key_individuals || [],
  }))

  const historyPayload = (historicalContext || []).map((h) => ({
    headline: h.headline,
    source: h.newspaper,
    published: h.createdAt,
    summary: h.assessment_article || '',
  }))

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT (Internal Database) ]': historyPayload,
    '[ PUBLIC WIKIPEDIA CONTEXT ]': wikipediaContext || 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': newsApiContext || 'Not available.',
  }

  logger.trace({ synthesis_context: userContent }, '--- SYNTHESIS CONTEXT ---')

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    logger.error('Failed to synthesize event.', { response })
    return { error: 'Synthesis failed' }
  }
  return response
}

export async function synthesizeFromHeadline(article) {
  const eventSynthesizerAgent = getAgent()
  logger.warn(
    { headline: article.headline },
    `Salvaging high-signal headline with failed enrichment...`
  )

  const todayPayload = [
    {
      headline: article.headline,
      source: article.newspaper,
      full_text:
        "NOTE: Full article text could not be retrieved. Synthesize based on the headline's explicit claims and your general knowledge.",
      key_individuals: article.key_individuals || [],
    },
  ]

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT ]': [],
    '[ PUBLIC WIKIPEDIA CONTEXT ]': 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': 'Not available.',
  }

  logger.trace({ synthesis_context: userContent }, '--- SALVAGE SYNTHESIS CONTEXT ---')

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    logger.error('Failed to salvage headline.', { response })
    return null
  }
  return response
}

```
