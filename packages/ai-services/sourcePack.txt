# 📁 PROJECT DIRECTORY STRUCTURE

Total: 78 files, 8 directories

```
headlines/
├── 📁 src/
│   ├── 📁 agents/
│   │   ├── 📄 articleAgent.js
│   │   ├── 📄 articlePreAssessmentAgent.js
│   │   ├── 📄 batchArticleAgent.js
│   │   ├── 📄 clusteringAgent.js
│   │   ├── 📄 contactAgent.js
│   │   ├── 📄 emailAgents.js
│   │   ├── 📄 entityAgent.js
│   │   ├── 📄 executiveSummaryAgent.js
│   │   ├── 📄 headlineAgent.js
│   │   ├── 📄 judgeAgent.js
│   │   ├── 📄 opportunityAgent.js
│   │   ├── 📄 sectionClassifierAgent.js
│   │   ├── 📄 selectorRepairAgent.js
│   │   ├── 📄 synthesisAgent.js
│   │   └── 📄 watchlistAgent.js
│   ├── 📁 chains/
│   │   ├── 📄 articleChain.js
│   │   ├── 📄 articlePreAssessmentChain.js
│   │   ├── 📄 batchHeadlineChain.js
│   │   ├── 📄 clusteringChain.js
│   │   ├── 📄 contactFinderChain.js
│   │   ├── 📄 contactResolverChain.js
│   │   ├── 📄 countryCorrectionChain.js
│   │   ├── 📄 disambiguationChain.js
│   │   ├── 📄 emailIntroChain.js
│   │   ├── 📄 emailSubjectChain.js
│   │   ├── 📄 entityCanonicalizerChain.js
│   │   ├── 📄 entityExtractorChain.js
│   │   ├── 📄 executiveSummaryChain.js
│   │   ├── 📄 headlineChain.js
│   │   ├── 📄 index.js
│   │   ├── 📄 judgeChain.js
│   │   ├── 📄 opportunityChain.js
│   │   ├── 📄 sectionClassifierChain.js
│   │   ├── 📄 selectorRepairChain.js
│   │   ├── 📄 synthesisChain.js
│   │   ├── 📄 translateChain.js
│   │   └── 📄 watchlistSuggestionChain.js
│   ├── 📁 embeddings/
│   │   ├── 📄 embeddings.js
│   │   └── 📄 vectorSearch.js
│   ├── 📁 lib/
│   │   ├── 📄 AIAgent.js
│   │   ├── 📄 langchain.js
│   │   └── 📄 safeInvoke.js
│   ├── 📁 rag/
│   │   ├── 📄 generation.js
│   │   ├── 📄 orchestrator.js
│   │   ├── 📄 planner.js
│   │   ├── 📄 prompts.js
│   │   ├── 📄 retrieval.js
│   │   └── 📄 validation.js
│   ├── 📁 schemas/
│   │   ├── 📄 articleAssessmentSchema.js
│   │   ├── 📄 articlePreAssessmentSchema.js
│   │   ├── 📄 batchArticleAssessmentSchema.js
│   │   ├── 📄 batchHeadlineAssessmentSchema.js
│   │   ├── 📄 canonicalizerSchema.js
│   │   ├── 📄 clusterSchema.js
│   │   ├── 📄 countryCorrectionSchema.js
│   │   ├── 📄 disambiguationSchema.js
│   │   ├── 📄 emailIntroSchema.js
│   │   ├── 📄 emailSubjectSchema.js
│   │   ├── 📄 enrichContactSchema.js
│   │   ├── 📄 entitySchema.js
│   │   ├── 📄 executiveSummarySchema.js
│   │   ├── 📄 findContactSchema.js
│   │   ├── 📄 headlineAssessmentSchema.js
│   │   ├── 📄 index.js
│   │   ├── 📄 judgeSchema.js
│   │   ├── 📄 opportunitySchema.js
│   │   ├── 📄 sectionClassifierSchema.js
│   │   ├── 📄 selectorRepairSchema.js
│   │   ├── 📄 synthesisSchema.js
│   │   ├── 📄 translateSchema.js
│   │   └── 📄 watchlistSuggestionSchema.js
│   ├── 📁 search/
│   │   ├── 📄 search.js
│   │   ├── 📄 serpapi.js
│   │   └── 📄 wikipedia.js
│   ├── 📄 core.js
│   ├── 📄 index.js
│   └── 📄 next.js
└── 📄 package.json
```

# 📋 PROJECT METADATA

**Generated**: 2025-09-29T13:20:36.398Z
**Repository Path**: /home/mark/Repos/projects/headlines/packages/ai-services
**Total Files**: 78
**Package**: @headlines/ai-services@1.0.0
**Description**: Centralized, LangChain-powered AI and external service logic.



---


## 📄 package.json
*Lines: 35, Size: 946 Bytes*

```json
{
  "name": "@headlines/ai-services",
  "version": "1.0.0",
  "description": "Centralized, LangChain-powered AI and external service logic.",
  "main": "src/index.js",
  "type": "module",
  "license": "ISC",
  "exports": {
    ".": "./src/index.js",
    "./node": "./src/index.js",
    "./next": "./src/next.js"
  },
  "dependencies": {
    "@headlines/config": "workspace:*",
    "@headlines/models": "workspace:*",
    "@headlines/prompts": "workspace:*",
    "@headlines/utils-shared": "workspace:*",
    "@headlines/utils-server": "workspace:*",
    "@langchain/community": "*",
    "@langchain/core": "*",
    "@langchain/openai": "*",
    "@langchain/pinecone": "*",
    "@pinecone-database/pinecone": "^2.2.2",
    "@xenova/transformers": "^2.17.2",
    "axios": "^1.7.2",
    "langchain": "*",
    "newsapi": "^2.4.1",
    "openai": "^5.22.0",
    "p-limit": "^5.0.0",
    "serpapi": "^2.1.0",
    "sharp": "0.33.4",
    "zod": "*"
  }
}

```

## 📄 src/agents/articleAgent.js
*Lines: 76, Size: 2.64 KB*

```javascript
'use server'

import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { articleAssessmentSchema } from '../schemas/articleAssessmentSchema.js'
import { settings } from '@headlines/config/node'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionArticle,
    fewShotInputs: shotsInputArticle,
    fewShotOutputs: shotsOutputArticle,
    zodSchema: articleAssessmentSchema,
  })

// The agent now accepts `hits` as a parameter, breaking the circular dependency.
// It is no longer responsible for fetching data.
export async function assessArticleContent(article, hits = [], isSalvaged = false) {
  const articleAssessmentAgent = getAgent()
  const fullContent = (article.articleContent?.contents || []).join('\n')
  const truncatedContent = truncateString(fullContent, settings.LLM_CONTEXT_MAX_CHARS)

  if (fullContent.length > settings.LLM_CONTEXT_MAX_CHARS) {
    logger.warn(
      {
        originalLength: fullContent.length,
        truncatedLength: truncatedContent.length,
        limit: settings.LLM_CONTEXT_MAX_CHARS,
      },
      `Article content for LLM was truncated to prevent context overload.`
    )
  }

  let articleText = `HEADLINE: ${article.headline}\n\nBODY:\n${truncatedContent}`

  if (hits.length > 0) {
    const hitStrings = hits.map(
      (hit) => `[WATCHLIST HIT: ${hit.name} | CONTEXT: ${hit.context || 'N/A'}]`
    )
    const hitPrefix = hitStrings.join(' ')
    articleText = `${hitPrefix} ${articleText}`
    logger.info({ hits: hits.map((h) => h.name) }, 'Watchlist entities found in article.')
  }

  if (isSalvaged) {
    articleText = `[SALVAGE CONTEXT: The original source for this headline failed to scrape. This content is from an alternative source. Please assess based on this new context.]\n\n${articleText}`
  }

  const response = await articleAssessmentAgent.execute(articleText)

  if (response.error) {
    logger.error(
      { article: { link: article.link }, details: response },
      `Article assessment failed for ${article.link}.`
    )
    return { ...article, error: `AI Error: ${response.error}` }
  }

  if (
    response.amount > 0 &&
    response.amount < settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  ) {
    response.relevance_article = 10
    response.assessment_article = `Dropped: Amount ($${response.amount}M) is below the financial threshold of $${settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS}M.`
  }

  return { ...article, ...response, error: null }
}

```

## 📄 src/agents/articlePreAssessmentAgent.js
*Lines: 24, Size: 829 Bytes*

```javascript
// This file is a standard server-side JavaScript module.
// It does NOT contain environment-specific directives.

import { AIAgent } from '../lib/AIAgent.js'
import { articlePreAssessmentSchema } from '../schemas/articlePreAssessmentSchema.js'
import { settings } from '@headlines/config/node'
import { instructionArticlePreAssessment } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionArticlePreAssessment,
    zodSchema: articlePreAssessmentSchema,
  })

export async function preAssessArticle(articleContent) {
  const articlePreAssessmentAgent = getAgent()
  const response = await articlePreAssessmentAgent.execute(articleContent)
  if (response.error) {
    return { classification: null, error: response.error }
  }
  return response
}

```

## 📄 src/agents/batchArticleAgent.js
*Lines: 67, Size: 2.2 KB*

```javascript
// This is a standard server-side module, do not add environment-specific directives.

import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { batchArticleAssessmentSchema } from '../schemas/batchArticleAssessmentSchema.js'
import { settings, AI_BATCH_SIZE } from '@headlines/config/node'
import { getInstructionBatchArticleAssessment } from '@headlines/prompts'
import { assessArticleContent } from './articleAgent.js' // The fallback single-article agent

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionBatchArticleAssessment,
    zodSchema: batchArticleAssessmentSchema,
  })

export async function batchAssessArticles(articles) {
  if (!articles || articles.length === 0) return []

  const batchAgent = getAgent()
  const articleBatches = []
  // Use the centralized batch size from config
  for (let i = 0; i < articles.length; i += AI_BATCH_SIZE) {
    articleBatches.push(articles.slice(i, i + AI_BATCH_SIZE))
  }

  const allResults = []

  for (const batch of articleBatches) {
    const payload = batch.map((article) => ({
      headline: article.headline,
      content: (article.articleContent?.contents || []).join('\n'),
    }))

    const response = await batchAgent.execute(JSON.stringify(payload))

    if (
      response.error ||
      !response.assessments ||
      response.assessments.length !== batch.length
    ) {
      logger.error(
        {
          details: response,
          expectedCount: batch.length,
          receivedCount: response.assessments?.length,
        },
        'Batch assessment failed or returned mismatched count. Falling back to single-article processing for this batch.'
      )

      // The fallback now correctly calls the refactored single-article agent
      const fallbackPromises = batch.map((article) => assessArticleContent(article))
      const fallbackResults = await Promise.all(fallbackPromises)
      allResults.push(...fallbackResults)
      continue
    }

    const mergedResults = batch.map((originalArticle, index) => ({
      ...originalArticle,
      ...response.assessments[index],
    }))
    allResults.push(...mergedResults)
  }

  return allResults
}

```

## 📄 src/agents/clusteringAgent.js
*Lines: 77, Size: 2.6 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { clusterSchema } from '../schemas/clusterSchema.js'
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { instructionCluster } from '@headlines/prompts'

const CLUSTER_BATCH_SIZE = 25 // This is a processing batch, not a model limit, so it can be defined locally.

const getAgent = () =>
  new AIAgent({
    // Correct: Use the settings object for the model name
    model: settings.LLM_MODEL_SYNTHESIS, // Clustering is a high-level synthesis task
    systemPrompt: instructionCluster,
    zodSchema: clusterSchema,
  })

export async function clusterArticlesIntoEvents(articles) {
  const articleClusterAgent = getAgent()
  logger.info(`Clustering ${articles.length} articles into unique events...`)

  if (!articles || articles.length === 0) {
    return []
  }

  const batches = []
  for (let i = 0; i < articles.length; i += CLUSTER_BATCH_SIZE) {
    batches.push(articles.slice(i, i + CLUSTER_BATCH_SIZE))
  }
  logger.info(`Processing clusters in ${batches.length} batches.`)

  const allClusters = []
  for (const [index, batch] of batches.entries()) {
    logger.info(`Clustering batch ${index + 1} of ${batches.length}...`)
    const articlePayload = batch.map((a) => ({
      id: a._id.toString(),
      headline: a.headline,
      source: a.newspaper,
      // Use a more robust summary, preferring the AI's assessment
      summary: (a.assessment_article || a.assessment_headline || '').substring(0, 400),
    }))
    const userContent = JSON.stringify(articlePayload)
    const response = await articleClusterAgent.execute(userContent)

    if (response.error || !response.events) {
      logger.error(`Failed to cluster articles in batch ${index + 1}.`, {
        response,
      })
      continue
    }
    allClusters.push(...response.events)
  }

  if (allClusters.length === 0) {
    logger.warn('Failed to cluster any articles across all batches.')
    return []
  }

  // De-duplicate clusters that might have been created across batches with the same key
  const finalEventMap = new Map()
  allClusters.forEach((event) => {
    if (finalEventMap.has(event.event_key)) {
      const existing = finalEventMap.get(event.event_key)
      event.article_ids.forEach((id) => existing.article_ids.add(id))
    } else {
      finalEventMap.set(event.event_key, {
        event_key: event.event_key,
        article_ids: new Set(event.article_ids),
      })
    }
  })

  return Array.from(finalEventMap.values()).map((event) => ({
    event_key: event.event_key,
    article_ids: Array.from(event.article_ids),
  }))
}

```

## 📄 src/agents/contactAgent.js
*Lines: 61, Size: 2.09 KB*

```javascript
import { logger } from '@headlines/utils-server/node';
import { AIAgent } from '../lib/AIAgent.js';
import { findContactSchema } from '../schemas/findContactSchema.js';
import { settings } from '@headlines/config/node';
import { instructionContacts } from '@headlines/prompts';
import { performGoogleSearch } from '../search/search.js'; // Import search function directly

const getFinderAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionContacts,
    zodSchema: findContactSchema,
  });

/**
 * Uses Google search and an AI agent to find a contact email for a given person.
 * @param {object} person - An object representing the person to research.
 *  - {string} reachOutTo - The person's full name.
 *  - {object} contactDetails - An object with a `company` property.
 * @returns {Promise<{email: string|null}>} An object containing the found email or null.
 */
export async function findContactDetails(person) {
  const contactFinderAgent = getFinderAgent();
  logger.info(`[Contact Research Agent] Initiated for: ${person.reachOutTo}`);
  
  const company = person.contactDetails?.company || '';
  const queries = [
    `"${person.reachOutTo}" ${company} email address`,
    `"${person.reachOutTo}" contact information`,
  ];

  let combinedSnippets = '';
  for (const query of queries) {
    const searchResult = await performGoogleSearch(query);
    if (searchResult.success && searchResult.snippets) {
      combinedSnippets += `\n--- Results for query: "${query}" ---\n${searchResult.snippets}`;
    }
  }

  if (!combinedSnippets) {
    logger.warn(
      `[Contact Research Agent] No search results for "${person.reachOutTo}".`
    );
    return { email: null };
  }

  const response = await contactFinderAgent.execute(combinedSnippets);

  if (response.error || !response.email) {
    logger.warn(
      `[Contact Research Agent] LLM failed to extract details for "${person.reachOutTo}".`
    );
    return { email: null };
  }

  logger.info(
    { details: response },
    `[Contact Research Agent] Found details for "${person.reachOutTo}".`
  );
  return response;
}
```

## 📄 src/agents/emailAgents.js
*Lines: 80, Size: 2.92 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { emailSubjectSchema } from '../schemas/emailSubjectSchema.js'
import { emailIntroSchema } from '../schemas/emailIntroSchema.js'
import { settings } from '@headlines/config/node'
import { instructionEmailSubject, instructionEmailIntro } from '@headlines/prompts'

const getAgent = (systemPrompt, zodSchema) =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt,
    zodSchema,
  })

export async function generateEmailSubjectLine(events) {
  const subjectLineAgent = getAgent(instructionEmailSubject, emailSubjectSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const response = await subjectLineAgent.execute(JSON.stringify(eventPayload))
    if (response.error || !response.subject_headline) {
      logger.warn('AI failed to generate a custom email subject line.', response)
      return 'Key Developments' // Fallback
    }
    return response.subject_headline
  } catch (error) {
    logger.error({ err: error }, 'Error in generateEmailSubjectLine')
    return 'Key Developments' // Fallback
  }
}

export async function generatePersonalizedIntro(user, events) {
  const introAgent = getAgent(instructionEmailIntro, emailIntroSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const payload = {
      firstName: user.firstName,
      events: eventPayload,
    }
    const response = await introAgent.execute(JSON.stringify(payload))

    // The schema returns a structured object, not a simple string.
    if (response.error || !response.greeting) {
      logger.warn('AI failed to generate a personalized intro.', response)
      // Return a fallback object that matches the schema's structure
      return {
        greeting: `Dear ${user.firstName},`,
        body: 'Here are the latest relevant wealth events we have identified for your review.',
        bullets: events
          .slice(0, 2)
          .map(
            (e) =>
              `A key development regarding ${e.synthesized_headline.substring(0, 40)}...`
          ),
        signoff: 'We wish you a fruitful day!\\n\\nThe team at Wealth Watch',
      }
    }
    return response // Return the full structured object
  } catch (error) {
    logger.error({ err: error }, 'Error in generatePersonalizedIntro')
    // Return a fallback object that matches the schema's structure
    return {
      greeting: `Dear ${user.firstName},`,
      body: 'Here are the latest relevant wealth events we have identified for your review.',
      bullets: events
        .slice(0, 2)
        .map(
          (e) =>
            `A key development regarding ${e.synthesized_headline.substring(0, 40)}...`
        ),
      signoff: 'We wish you a fruitful day!\\n\\nThe team at Wealth Watch',
    }
  }
}

```

## 📄 src/agents/entityAgent.js
*Lines: 65, Size: 2.22 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { entitySchema } from '../schemas/entitySchema.js'
import { canonicalizerSchema } from '../schemas/canonicalizerSchema.js'
import { settings } from '@headlines/config/node'
import { instructionEntity, instructionCanonicalizer } from '@headlines/prompts'

const getEntityExtractorAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionEntity,
    zodSchema: entitySchema,
  })

const getEntityCanonicalizerAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionCanonicalizer,
    zodSchema: canonicalizerSchema,
  })

// Export the agent instance directly for potential reuse
export const entityCanonicalizerAgent = getEntityCanonicalizerAgent()

export async function extractEntities(text) {
  const entityExtractorAgent = getEntityExtractorAgent()
  // Use the exported agent instance
  const canonicalizer = entityCanonicalizerAgent

  if (!text) return []

  try {
    const response = await entityExtractorAgent.execute(`Article Text:\n${text}`)

    if (response.error) {
      throw new Error(response.error)
    }

    const { reasoning, entities } = response
    logger.info(`[Query Planner Agent] Reasoning: ${reasoning}`)
    if (!entities || !Array.isArray(entities)) return []

    const canonicalizationPromises = entities
      .map((entity) => entity.replace(/\s*\(.*\)\s*/g, '').trim())
      .filter(Boolean)
      .map(async (entity) => {
        const canonResponse = await canonicalizer.execute(entity)
        if (canonResponse && !canonResponse.error && canonResponse.canonical_name) {
          logger.trace(`Canonicalized "${entity}" -> "${canonResponse.canonical_name}"`)
          return canonResponse.canonical_name
        }
        return null
      })

    const canonicalEntities = await Promise.all(canonicalizationPromises)
    const uniqueEntities = [...new Set(canonicalEntities.filter(Boolean))]

    logger.info({ entities: uniqueEntities }, `Final list of canonical entities for RAG.`)
    return uniqueEntities
  } catch (error) {
    logger.warn({ err: error }, 'Wikipedia query planning (entity extraction) failed.')
    return []
  }
}

```

## 📄 src/agents/executiveSummaryAgent.js
*Lines: 36, Size: 1.24 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { executiveSummarySchema } from '../schemas/executiveSummarySchema.js'
import { settings } from '@headlines/config/node'
import { instructionExecutiveSummary } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionExecutiveSummary,
    zodSchema: executiveSummarySchema,
  })

export async function generateExecutiveSummary(judgeVerdict, runStats) {
  const agent = getAgent()
  try {
    // Create a payload with the crucial context for the AI
    const payload = {
      freshHeadlinesFound: runStats.freshHeadlinesFound,
      judgeVerdict: judgeVerdict || { event_judgements: [], opportunity_judgements: [] },
    }

    const response = await agent.execute(JSON.stringify(payload))

    if (response.error || !response.summary) {
      logger.warn('AI failed to generate an executive summary.', response)
      return 'AI failed to generate a summary for this run.'
    }

    return response.summary
  } catch (error) {
    logger.error({ err: error }, 'Error in generateExecutiveSummary')
    return 'An unexpected error occurred while generating the executive summary.'
  }
}

```

## 📄 src/agents/headlineAgent.js
*Lines: 69, Size: 2.26 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { headlineAssessmentSchema } from '../schemas/headlineAssessmentSchema.js'
import { settings } from '@headlines/config/node'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_HEADLINE_ASSESSMENT,
    systemPrompt: instructionHeadlines,
    fewShotInputs: shotsInputHeadlines,
    fewShotOutputs: shotsOutputHeadlines,
    zodSchema: headlineAssessmentSchema,
  })

// This agent now accepts pre-calculated hits. It no longer fetches data.
async function assessSingleHeadline(article, hits = []) {
  const headlineAssessmentAgent = getAgent()
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`

  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }

  const response = await headlineAssessmentAgent.execute(headlineWithContext)

  // Define a default assessment object for fallback
  let assessment = {
    relevance_headline: 0,
    assessment_headline: 'AI assessment failed.',
    headline_en: article.headline,
  }

  if (response && response.assessment && response.assessment.length > 0) {
    assessment = response.assessment[0]
    let score = assessment.relevance_headline
    const boost = settings.WATCHLIST_SCORE_BOOST

    if (hits.length > 0 && boost > 0) {
      score = Math.min(100, score + boost)
      assessment.assessment_headline = `Watchlist boost (+${boost}). ${assessment.assessment_headline}`
    }
    assessment.relevance_headline = score
  }

  return { ...article, ...assessment }
}

// The batching function now needs to accept hits for each article.
// We'll pass an array of `hits` arrays.
export async function assessHeadlinesInBatches(articles, articlesHits) {
  const assessmentPromises = articles.map((article, index) => {
    const hitsForArticle = articlesHits[index] || []
    return assessSingleHeadline(article, hitsForArticle)
  })

  const results = await Promise.all(assessmentPromises)
  return results
}

```

## 📄 src/agents/judgeAgent.js
*Lines: 62, Size: 1.76 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { judgeSchema } from '../schemas/judgeSchema.js'
import { settings } from '@headlines/config/node'
import { instructionJudge } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionJudge,
    zodSchema: judgeSchema,
  })

export async function judgePipelineOutput(events, opportunities) {
  const judgeAgent = getAgent()
  if (
    (!events || events.length === 0) &&
    (!opportunities || opportunities.length === 0)
  ) {
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }
  logger.info('⚖️ [Judge Agent] Reviewing final pipeline output for quality control...')

  const lightweightEvents = (events || []).map((e) => ({
    identifier: `Event: ${e.synthesized_headline}`,
    summary: e.synthesized_summary,
    assessment: e.ai_assessment_reason,
    score: e.highest_relevance_score,
  }))

  const lightweightOpportunities = (opportunities || []).map((o) => ({
    identifier: `Opportunity: ${o.reachOutTo}`,
    reason: o.whyContact,
    wealth_estimate_mm: o.likelyMMDollarWealth,
  }))

  const inputText = JSON.stringify({
    events: lightweightEvents,
    opportunities: lightweightOpportunities,
  })

  const response = await judgeAgent.execute(inputText)

  if (response.error) {
    logger.error({ details: response }, 'Judge Agent failed to produce a verdict.')
    // Return a default "empty" verdict on failure to avoid crashing the pipeline
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }

  logger.info(
    { details: response },
    '[Judge Agent] Successfully produced quality control verdicts.'
  )
  return response
}

```

## 📄 src/agents/opportunityAgent.js
*Lines: 69, Size: 2.28 KB*

```javascript
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { opportunitySchema } from '../schemas/opportunitySchema.js'
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { getInstructionOpportunities } from '@headlines/prompts'

const getOppAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionOpportunities,
    zodSchema: opportunitySchema,
  })

export async function generateOpportunitiesFromEvent(
  synthesizedEvent,
  articlesInCluster
) {
  const opportunityGeneratorAgent = getOppAgent()

  const highestRelevanceArticle = articlesInCluster.reduce((max, current) =>
    (current.relevance_article || 0) > (max.relevance_article || 0) ? current : max
  )

  const fullText = articlesInCluster
    .map((a) => (a.articleContent?.contents || []).join('\n'))
    .join('\n\n')

  const inputText = `
        Synthesized Event Headline: ${synthesizedEvent.synthesized_headline}
        Synthesized Event Summary: ${synthesizedEvent.synthesized_summary}
        Key Individuals already identified: ${JSON.stringify(synthesizedEvent.key_individuals)}
        Source Article Snippets: ${truncateString(fullText, LLM_CONTEXT_MAX_CHARS)}
    `

  const response = await opportunityGeneratorAgent.execute(inputText)

  if (response.error || !response.opportunities) {
    logger.warn(
      { event: synthesizedEvent.synthesized_headline, details: response },
      `Opportunity generation failed.`
    )
    return []
  }

  const validOpportunities = (response.opportunities || []).filter(
    (opp) =>
      opp.likelyMMDollarWealth === null || // Keep opportunities where wealth is unknown
      opp.likelyMMDollarWealth >= settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  )

  const opportunitiesWithSource = validOpportunities.map((opp) => ({
    ...opp,
    event_key: synthesizedEvent.event_key,
    sourceArticleId: highestRelevanceArticle._id,
  }))

  logger.info(
    { details: opportunitiesWithSource },
    `[Opportunity Agent] Generated ${
      opportunitiesWithSource.length
    } opportunity/ies from event "${truncateString(
      synthesizedEvent.synthesized_headline,
      50
    )}"`
  )
  return opportunitiesWithSource
}

```

## 📄 src/agents/sectionClassifierAgent.js
*Lines: 41, Size: 1.21 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { sectionClassifierSchema } from '../schemas/sectionClassifierSchema.js'
import { settings } from '@headlines/config/node'
import { instructionSectionClassifier } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY, // Using the cheap and fast model
    systemPrompt: [
      instructionSectionClassifier.whoYouAre,
      instructionSectionClassifier.whatYouDo,
      ...instructionSectionClassifier.guidelines,
      instructionSectionClassifier.outputFormatDescription,
    ].join('\n\n'),
    zodSchema: sectionClassifierSchema,
  })

export async function classifyLinks(links) {
  if (!links || links.length === 0) {
    return []
  }

  const agent = getAgent()
  const response = await agent.execute(JSON.stringify(links))

  if (
    response.error ||
    !response.classifications ||
    response.classifications.length !== links.length
  ) {
    logger.error(
      { response, expectedCount: links.length },
      'Section classifier agent failed or returned mismatched count.'
    )
    return null // Return null to indicate failure
  }

  return response.classifications
}

```

## 📄 src/agents/selectorRepairAgent.js
*Lines: 54, Size: 1.58 KB*

```javascript
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { selectorRepairSchema } from '../schemas/selectorRepairSchema.js'
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { instructionSelectorRepair } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: [
      instructionSelectorRepair.whoYouAre,
      instructionSelectorRepair.whatYouDo,
      ...instructionSelectorRepair.guidelines,
      instructionSelectorRepair.outputFormatDescription,
      instructionSelectorRepair.reiteration,
    ].join('\n\n'),
    zodSchema: selectorRepairSchema,
  })

export async function suggestNewSelector(
  url,
  failedSelector,
  htmlContent,
  heuristicSuggestions = []
) {
  const selectorRepairAgent = getAgent()
  try {
    const payload = {
      url,
      failed_selector: failedSelector,
      heuristic_suggestions: heuristicSuggestions.map((s) => ({
        selector: s.selector,
        samples: s.samples.slice(0, 3),
      })),
      html_content: truncateString(htmlContent, LLM_CONTEXT_MAX_CHARS),
    }

    const response = await selectorRepairAgent.execute(JSON.stringify(payload))

    if (response.error || !response.suggested_selectors) {
      logger.error('Selector repair agent failed to produce a valid suggestion.', {
        response,
      })
      return null
    }

    return response
  } catch (error) {
    logger.error({ err: error }, 'Error in suggestNewSelector')
    return null
  }
}

```

## 📄 src/agents/synthesisAgent.js
*Lines: 99, Size: 3.04 KB*

```javascript
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { synthesisSchema } from '../schemas/synthesisSchema.js'
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { instructionSynthesize } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: [
      instructionSynthesize.whoYouAre,
      instructionSynthesize.whatYouDo,
      ...instructionSynthesize.guidelines,
      instructionSynthesize.outputFormatDescription,
    ].join('\n\n'),
    zodSchema: synthesisSchema,
  })

export async function synthesizeEvent(
  articlesInCluster,
  historicalContext,
  wikipediaContext,
  newsApiContext
  // gdeltContext is no longer used, so it's removed.
) {
  const eventSynthesizerAgent = getAgent()

  const todayPayload = articlesInCluster.map((a) => ({
    headline: a.headline,
    source: a.newspaper,
    full_text: truncateString(
      (a.articleContent?.contents || []).join('\n'),
      // Use a safe division, prevent division by zero.
      LLM_CONTEXT_MAX_CHARS / (articlesInCluster.length || 1)
    ),
    key_individuals: a.key_individuals || [],
  }))

  const historyPayload = (historicalContext || []).map((h) => ({
    headline: h.headline,
    source: h.newspaper,
    published: h.createdAt,
    summary: h.assessment_article || '',
  }))

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT (Internal Database) ]': historyPayload,
    '[ PUBLIC WIKIPEDIA CONTEXT ]': wikipediaContext || 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': newsApiContext || 'Not available.',
  }

  logger.trace({ synthesis_context: userContent }, '--- SYNTHESIS CONTEXT ---')

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    logger.error('Failed to synthesize event.', { response })
    return { error: 'Synthesis failed' }
  }
  return response
}

export async function synthesizeFromHeadline(article) {
  const eventSynthesizerAgent = getAgent()
  logger.warn(
    { headline: article.headline },
    `Salvaging high-signal headline with failed enrichment...`
  )

  const todayPayload = [
    {
      headline: article.headline,
      source: article.newspaper,
      full_text:
        "NOTE: Full article text could not be retrieved. Synthesize based on the headline's explicit claims and your general knowledge.",
      key_individuals: article.key_individuals || [],
    },
  ]

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT ]': [],
    '[ PUBLIC WIKIPEDIA CONTEXT ]': 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': 'Not available.',
  }

  logger.trace({ synthesis_context: userContent }, '--- SALVAGE SYNTHESIS CONTEXT ---')

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    logger.error('Failed to salvage headline.', { response })
    return null
  }
  return response
}

```

## 📄 src/agents/watchlistAgent.js
*Lines: 48, Size: 1.77 KB*

```javascript
import { logger } from '@headlines/utils-server/node'
import { AIAgent } from '../lib/AIAgent.js'
import { watchlistSuggestionSchema } from '../schemas/watchlistSuggestionSchema.js'
import { settings } from '@headlines/config/node'
import { instructionWatchlistSuggestion } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: [
      instructionWatchlistSuggestion.whoYouAre,
      instructionWatchlistSuggestion.whatYouDo,
      ...instructionWatchlistSuggestion.guidelines,
      instructionWatchlistSuggestion.outputFormatDescription,
      instructionWatchlistSuggestion.reiteration,
    ].join('\n\n'),
    zodSchema: watchlistSuggestionSchema,
  })

/**
 * Analyzes events to generate new watchlist suggestions.
 * @param {Array<object>} events - High-quality synthesized events.
 * @param {Set<string>} existingWatchlistNames - A set of lowercase names already on the watchlist.
 * @returns {Promise<Array<object>>} An array of new WatchlistSuggestion documents.
 */
export async function generateWatchlistSuggestions(events, existingWatchlistNames) {
  const watchlistSuggestionAgent = getAgent()
  try {
    const payload = { events }
    const response = await watchlistSuggestionAgent.execute(JSON.stringify(payload))

    if (response.error || !Array.isArray(response.suggestions)) {
      logger.warn('AI failed to generate watchlist suggestions.', response)
      return []
    }

    // Post-filter to ensure we don't suggest entities that already exist
    const newSuggestions = response.suggestions.filter(
      (s) => !existingWatchlistNames.has(s.name.toLowerCase())
    )

    return newSuggestions
  } catch (error) {
    logger.error({ err: error }, 'Error in generateWatchlistSuggestions')
    return []
  }
}

```

## 📄 src/chains/articleChain.js
*Lines: 63, Size: 2.08 KB*

```javascript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { settings } from '@headlines/config'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { logger } from '@headlines/utils-server/logger'
import { articleAssessmentSchema } from '../schemas/index.js'

const instructions = getInstructionArticle(settings)
const systemPrompt = [
  instructions.whoYouAre,
  instructions.whatYouDo,
  instructions.primaryMandate,
  instructions.analyticalFramework,
  instructions.scoring,
  instructions.outputFormatDescription,
  instructions.reiteration,
].join('\n\n')

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputArticle.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputArticle[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{article_text}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

async function invoke(input) {
  const result = await safeInvoke(chain, input, 'articleChain', articleAssessmentSchema)
  if (result.error) return result
  if (result.key_individuals?.length > 0) {
    const articleTextLower = input.article_text.toLowerCase()
    result.key_individuals = result.key_individuals.filter((ind) => {
      if (!ind.name) return false
      const isPresent = ind.name
        .split(' ')
        .filter((p) => p.length > 2)
        .some((p) => articleTextLower.includes(p.toLowerCase()))
      if (!isPresent)
        logger.warn({ individual: ind.name }, 'Discarding hallucinated key individual.')
      return isPresent
    })
  }
  return result
}

export const articleChain = { invoke }

```

## 📄 src/chains/articlePreAssessmentChain.js
*Lines: 29, Size: 1.13 KB*

```javascript
// packages/ai-services/src/chains/articlePreAssessmentChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionArticlePreAssessment } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { articlePreAssessmentSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionArticlePreAssessment.whoYouAre,
  instructionArticlePreAssessment.whatYouDo,
  instructionArticlePreAssessment.classificationFramework,
  instructionArticlePreAssessment.outputFormatDescription,
  instructionArticlePreAssessment.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{input}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const articlePreAssessmentChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'articlePreAssessmentChain', articlePreAssessmentSchema),
}

```

## 📄 src/chains/batchHeadlineChain.js
*Lines: 30, Size: 1.23 KB*

```javascript
// packages/ai-services/src/chains/batchHeadlineChain.js (version 1.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionBatchHeadlineAssessment } from '@headlines/prompts'
import { getHeadlineModel } from '../lib/langchain.js' // Use the specific model for headlines
import { safeInvoke } from '../lib/safeInvoke.js'
import { batchHeadlineAssessmentSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionBatchHeadlineAssessment.whoYouAre,
  instructionBatchHeadlineAssessment.whatYouDo,
  instructionBatchHeadlineAssessment.primaryMandate,
  instructionBatchHeadlineAssessment.analyticalFramework,
  instructionBatchHeadlineAssessment.outputFormatDescription,
  instructionBatchHeadlineAssessment.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{headlines_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHeadlineModel(), new JsonOutputParser()])

export const batchHeadlineChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'batchHeadlineChain', batchHeadlineAssessmentSchema),
}

```

## 📄 src/chains/clusteringChain.js
*Lines: 28, Size: 1 KB*

```javascript
// packages/ai-services/src/chains/clusteringChain.js (version 3.2 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCluster } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { clusterSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionCluster.whoYouAre,
  instructionCluster.whatYouDo,
  ...instructionCluster.guidelines,
  instructionCluster.outputFormatDescription,
  instructionCluster.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{articles_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const clusteringChain = {
  invoke: (input) => safeInvoke(chain, input, 'clusteringChain', clusterSchema),
}

```

## 📄 src/chains/contactFinderChain.js
*Lines: 28, Size: 1.01 KB*

```javascript
// packages/ai-services/src/chains/contactFinderChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionContacts } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { findContactSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionContacts.whoYouAre,
  instructionContacts.whatYouDo,
  ...instructionContacts.guidelines,
  instructionContacts.outputFormatDescription,
  instructionContacts.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{snippets}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const contactFinderChain = {
  invoke: (input) => safeInvoke(chain, input, 'contactFinderChain', findContactSchema),
}

```

## 📄 src/chains/contactResolverChain.js
*Lines: 28, Size: 1.02 KB*

```javascript
// packages/ai-services/src/chains/contactResolverChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEnrichContact } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { enrichContactSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEnrichContact.whoYouAre,
  instructionEnrichContact.whatYouDo,
  ...instructionEnrichContact.guidelines,
  instructionEnrichContact.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const contactResolverChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'contactResolverChain', enrichContactSchema),
}

```

## 📄 src/chains/countryCorrectionChain.js
*Lines: 40, Size: 2.07 KB*

```javascript
// packages/ai-services/src/chains/countryCorrectionChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { countryCorrectionSchema } from '../schemas/index.js'

const systemPrompt = `You are a data cleaning expert. Your sole task is to analyze a given text string that is supposed to represent a country and extract the single, correct, UN-recognized sovereign country name from it.

**CRITICAL INSTRUCTIONS:**
1.  Analyze the input string.
2.  Identify the most likely country. For example, "Denmark (Aarhus)" should be "Denmark". "London" should be "United Kingdom".
4.  Anything starting with "Central Europe" should be "Europe".
5.  "Denmark & Sweden" should be "Scandinavia"
6.  "International" should be "Global"
7. "Nordic Region" should be "Scandinavia" (also if followed by something between brackets)
8. "Pan-Europe" should be "Europe"
9. "Sweden & Norway" should be "Scandinavia"
10. "United States" should be "United States of America"
11. "UK" should be "United Kingdom"
12. anything starting with "Unknown" should simply be "Unknown"
13.  If a valid country name can be determined, return it.
14.  If the input is ambiguous or does not contain a clear country, you MUST return null.
15.  You MUST respond ONLY with a valid JSON object in this format: {{"country": "Correct Country Name"}} or {{"country": null}}`;

// DEFINITIVE FIX: The template variable must match the key used in the input object.
// Langchain expects the input variable to be directly in the template string.
const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Location String: "{location_string}"'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const countryCorrectionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'countryCorrectionChain', countryCorrectionSchema),
}

```

## 📄 src/chains/disambiguationChain.js
*Lines: 28, Size: 1.02 KB*

```javascript
// packages/ai-services/src/chains/disambiguationChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionDisambiguation } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionDisambiguation.whoYouAre,
  instructionDisambiguation.whatYouDo,
  ...instructionDisambiguation.guidelines,
  instructionDisambiguation.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{inputText}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const disambiguationChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'disambiguationChain', disambiguationSchema),
}

```

## 📄 src/chains/emailIntroChain.js
*Lines: 28, Size: 1.05 KB*

```javascript
// packages/ai-services/src/chains/emailIntroChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailIntro } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailIntroSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEmailIntro.whoYouAre,
  instructionEmailIntro.whatYouDo,
  ...instructionEmailIntro.guidelines,
  instructionEmailIntro.outputFormatDescription,
  instructionEmailIntro.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Client and Event Data: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const emailIntroChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailIntroChain', emailIntroSchema),
}

```

## 📄 src/chains/emailSubjectChain.js
*Lines: 28, Size: 1.06 KB*

```javascript
// packages/ai-services/src/chains/emailSubjectChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailSubject } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailSubjectSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEmailSubject.whoYouAre,
  instructionEmailSubject.whatYouDo,
  ...instructionEmailSubject.guidelines,
  instructionEmailSubject.outputFormatDescription,
  instructionEmailSubject.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const emailSubjectChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailSubjectChain', emailSubjectSchema),
}

```

## 📄 src/chains/entityCanonicalizerChain.js
*Lines: 28, Size: 1.03 KB*

```javascript
// packages/ai-services/src/chains/entityCanonicalizerChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCanonicalizer } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { canonicalizerSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionCanonicalizer.whoYouAre,
  instructionCanonicalizer.whatYouDo,
  ...instructionCanonicalizer.guidelines,
  instructionCanonicalizer.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{entity_name}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const entityCanonicalizerChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'entityCanonicalizerChain', canonicalizerSchema),
}

```

## 📄 src/chains/entityExtractorChain.js
*Lines: 27, Size: 988 Bytes*

```javascript
// packages/ai-services/src/chains/entityExtractorChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEntity } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { entitySchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEntity.whoYouAre,
  instructionEntity.whatYouDo,
  ...instructionEntity.guidelines,
  instructionEntity.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{article_text}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const entityExtractorChain = {
  invoke: (input) => safeInvoke(chain, input, 'entityExtractorChain', entitySchema),
}

```

## 📄 src/chains/executiveSummaryChain.js
*Lines: 29, Size: 1.1 KB*

```javascript
// packages/ai-services/src/chains/executiveSummaryChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionExecutiveSummary } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { executiveSummarySchema } from '../schemas/index.js'

const systemPrompt = [
  instructionExecutiveSummary.whoYouAre,
  instructionExecutiveSummary.whatYouDo,
  ...instructionExecutiveSummary.guidelines,
  instructionExecutiveSummary.outputFormatDescription,
  instructionExecutiveSummary.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Run Data: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const executiveSummaryChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'executiveSummaryChain', executiveSummarySchema),
}

```

## 📄 src/chains/headlineChain.js
*Lines: 84, Size: 2.72 KB*

```javascript
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '@headlines/prompts' // Correct: Import from the monorepo package
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { headlineAssessmentSchema } from '../schemas/index.js'
import { settings } from '@headlines/config/node' // Correct: Import from the /node entry point

const systemPrompt = [
  instructionHeadlines.whoYouAre,
  instructionHeadlines.whatYouDo,
  instructionHeadlines.primaryMandate,
  instructionHeadlines.analyticalFramework,
  instructionHeadlines.outputFormatDescription,
].join('\n\n')

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputHeadlines.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputHeadlines[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{headlineWithContext}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

function prepareInput({ article, hits }) {
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`
  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }
  return { headlineWithContext }
}

async function invoke({ article, hits }) {
  const input = prepareInput({ article, hits })
  const result = await safeInvoke(chain, input, 'headlineChain', headlineAssessmentSchema)

  if (result.error) {
    return {
      relevance_headline: 0,
      assessment_headline: 'AI assessment failed.',
      headline_en: article.headline,
    }
  }

  const assessment = result.assessment?.[0]
  if (assessment && hits.length > 0) {
    let score = assessment.relevance_headline
    if (settings.WATCHLIST_SCORE_BOOST > 0) {
      score = Math.min(100, score + settings.WATCHLIST_SCORE_BOOST)
      assessment.assessment_headline = `Watchlist boost (+${settings.WATCHLIST_SCORE_BOOST}). ${assessment.assessment_headline}`
    }
    assessment.relevance_headline = score
  }

  return (
    assessment || {
      relevance_headline: 0,
      assessment_headline: 'AI assessment failed.',
      headline_en: article.headline,
    }
  )
}

export const headlineChain = { invoke }

```

## 📄 src/chains/index.js
*Lines: 47, Size: 2.99 KB*

```javascript
// packages/ai-services/src/chains/index.js (version 3.1 - Final)
import { articleChain as ac } from './articleChain.js'
import { articlePreAssessmentChain as apac } from './articlePreAssessmentChain.js'
import { clusteringChain as cc } from './clusteringChain.js'
import { contactFinderChain as cfc } from './contactFinderChain.js'
import { contactResolverChain as crc } from './contactResolverChain.js'
import { disambiguationChain as dc } from './disambiguationChain.js'
import { emailIntroChain as eic } from './emailIntroChain.js'
import { emailSubjectChain as esc } from './emailSubjectChain.js'
import { entityCanonicalizerChain as ecc } from './entityCanonicalizerChain.js'
import { entityExtractorChain as eec } from './entityExtractorChain.js'
import { executiveSummaryChain as exsc } from './executiveSummaryChain.js'
import { headlineChain as hc } from './headlineChain.js'
import { judgeChain as jc } from './judgeChain.js'
import { opportunityChain as oc } from './opportunityChain.js'
import { sectionClassifierChain as scc } from './sectionClassifierChain.js'
import { selectorRepairChain as src } from './selectorRepairChain.js'
import { synthesisChain as sc } from './synthesisChain.js'
import { watchlistSuggestionChain as wsc } from './watchlistSuggestionChain.js'
import { batchHeadlineChain as bhc } from './batchHeadlineChain.js'
import { translateChain as tc } from './translateChain.js'
import { countryCorrectionChain as ccc } from './countryCorrectionChain.js'

// DEFINITIVE FIX: Export each chain's invoke method as a standalone async function
// to comply with "use server" constraints.
export const articleChain = async (input) => ac.invoke(input)
export const articlePreAssessmentChain = async (input) => apac.invoke(input)
export const clusteringChain = async (input) => cc.invoke(input)
export const contactFinderChain = async (input) => cfc.invoke(input)
export const contactResolverChain = async (input) => crc.invoke(input)
export const disambiguationChain = async (input) => dc.invoke(input)
export const emailIntroChain = async (input) => eic.invoke(input)
export const emailSubjectChain = async (input) => esc.invoke(input)
export const entityCanonicalizerChain = async (input) => ecc.invoke(input)
export const entityExtractorChain = async (input) => eec.invoke(input)
export const executiveSummaryChain = async (input) => exsc.invoke(input)
export const headlineChain = async (input) => hc.invoke(input)
export const judgeChain = async (input) => jc.invoke(input)
export const opportunityChain = async (input) => oc.invoke(input)
export const sectionClassifierChain = async (input) => scc.invoke(input)
export const selectorRepairChain = async (input) => src.invoke(input)
export const synthesisChain = async (input) => sc.invoke(input)
export const watchlistSuggestionChain = async (input) => wsc.invoke(input)
export const batchHeadlineChain = async (input) => bhc.invoke(input)
export const translateChain = async (input) => tc.invoke(input)
export const countryCorrectionChain = async (input) => ccc.invoke(input)

```

## 📄 src/chains/judgeChain.js
*Lines: 28, Size: 1011 Bytes*

```javascript
// packages/ai-services/src/chains/judgeChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionJudge } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { judgeSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionJudge.whoYouAre,
  instructionJudge.whatYouDo,
  ...instructionJudge.guidelines,
  instructionJudge.outputFormatDescription,
  instructionJudge.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Data for review: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const judgeChain = {
  invoke: (input) => safeInvoke(chain, input, 'judgeChain', judgeSchema),
}

```

## 📄 src/chains/opportunityChain.js
*Lines: 29, Size: 1.07 KB*

```javascript
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getInstructionOpportunities } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { opportunitySchema } from '../schemas/index.js'
// Correct: Import from the /node entry point for the pipeline/Node.js environment
import { settings } from '@headlines/config/node'

const instructions = getInstructionOpportunities(settings)
const systemPrompt = [
  instructions.whoYouAre,
  instructions.whatYouDo,
  ...instructions.guidelines,
  instructions.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_text}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const opportunityChain = {
  invoke: (input) => safeInvoke(chain, input, 'opportunityChain', opportunitySchema),
}

```

## 📄 src/chains/sectionClassifierChain.js
*Lines: 34, Size: 1.95 KB*

```javascript
// packages/ai-services/src/chains/sectionClassifierChain.js (version 2.3.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js' // Correctly import the getter function
import { safeInvoke } from '../lib/safeInvoke.js'
import { sectionClassifierSchema } from '../schemas/index.js'

const INSTRUCTION = {
  whoYouAre:
    'You are a master website navigation analyst. Your task is to analyze a list of hyperlinks (anchor text and href) from a webpage and classify each one into one of four categories.',
  guidelines: [
    '**Categories:**',
    '1.  **"news_section"**: A link to a major category or section of news (e.g., "Business", "Technology", "World News", "/erhverv", "/økonomi").',
    '2.  **"article_headline"**: A link to a specific news article or story. The text is usually a full sentence or a descriptive title.',
    '3.  **"navigation"**: A link to a functional page on the site (e.g., "About Us", "Contact", "Login").',
    '4.  **"other"**: Any other type of link (advertisements, privacy policies, etc.).',
    '**Instructions:**',
    '-   You will receive a JSON array of link objects.',
    '-   You MUST return a JSON object with a single key, "classifications".',
    '-   The "classifications" array MUST contain one classification object for EACH link in the input, in the EXACT SAME ORDER.',
  ],
}

const systemPrompt = [INSTRUCTION.whoYouAre, ...INSTRUCTION.guidelines].join('\n\n')
const fullPrompt = `${systemPrompt}\n\nUser Input:\n{links_json_string}`
const prompt = ChatPromptTemplate.fromTemplate(fullPrompt)
const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()]) // Call the getter function

export const sectionClassifierChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'sectionClassifierChain', sectionClassifierSchema),
}

```

## 📄 src/chains/selectorRepairChain.js
*Lines: 29, Size: 1.07 KB*

```javascript
// packages/ai-services/src/chains/selectorRepairChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSelectorRepair } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { selectorRepairSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionSelectorRepair.whoYouAre,
  instructionSelectorRepair.whatYouDo,
  ...instructionSelectorRepair.guidelines,
  instructionSelectorRepair.outputFormatDescription,
  instructionSelectorRepair.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const selectorRepairChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'selectorRepairChain', selectorRepairSchema),
}

```

## 📄 src/chains/synthesisChain.js
*Lines: 27, Size: 1007 Bytes*

```javascript
// packages/ai-services/src/chains/synthesisChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSynthesize } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { synthesisSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionSynthesize.whoYouAre,
  instructionSynthesize.whatYouDo,
  ...instructionSynthesize.guidelines,
  instructionSynthesize.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const synthesisChain = {
  invoke: (input) => safeInvoke(chain, input, 'synthesisChain', synthesisSchema),
}

```

## 📄 src/chains/translateChain.js
*Lines: 28, Size: 1.05 KB*

```javascript
// packages/ai-services/src/chains/translateChain.js (version 1.0.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionTranslate } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { translateSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionTranslate.whoYouAre,
  instructionTranslate.whatYouDo,
  ...instructionTranslate.guidelines,
  instructionTranslate.outputFormatDescription,
  instructionTranslate.reiteration,
].join('\\n\\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Target Language: {language}\\n\\nHTML Content:\\n```{html_content}```'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const translateChain = {
  invoke: (input) => safeInvoke(chain, input, 'translateChain', translateSchema),
}

```

## 📄 src/chains/watchlistSuggestionChain.js
*Lines: 29, Size: 1.14 KB*

```javascript
// packages/ai-services/src/chains/watchlistSuggestionChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionWatchlistSuggestion } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { watchlistSuggestionSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionWatchlistSuggestion.whoYouAre,
  instructionWatchlistSuggestion.whatYouDo,
  ...instructionWatchlistSuggestion.guidelines,
  instructionWatchlistSuggestion.outputFormatDescription,
  instructionWatchlistSuggestion.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const watchlistSuggestionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'watchlistSuggestionChain', watchlistSuggestionSchema),
}

```

## 📄 src/core.js
*Lines: 132, Size: 4.58 KB*

```javascript
// This is the core, shared module for the ai-services package.
// It exports all AI chains, agents, and utility functions.

// --- Low-Level LangChain Primitives & Helpers ---
import { callLanguageModel } from './lib/langchain.js'
import * as chains from './chains/index.js'
import * as search from './search/search.js'
import * as wikipedia from './search/wikipedia.js'
import * as embeddings from './embeddings/embeddings.js'
import * as vectorSearch from './embeddings/vectorSearch.js'
import { processChatRequest } from './rag/orchestrator.js'
import { logger } from '@headlines/utils-server/node' // Use the node-safe entry point
import { settings } from '@headlines/config/node' // Use the node-safe entry point

// --- High-Level Agents (Moved from scraper-logic) ---
// Note: You will need to ensure the agent files themselves are moved into `packages/ai-services/src/agents/`
// and their internal imports are updated to be relative to their new location.
import { assessArticleContent } from './agents/articleAgent.js';
import { preAssessArticle } from './agents/articlePreAssessmentAgent.js';
import { batchAssessArticles } from './agents/batchArticleAgent.js';
import { clusterArticlesIntoEvents } from './agents/clusteringAgent.js';
import { findContactDetails } from './agents/contactAgent.js';
import { generateEmailSubjectLine, generatePersonalizedIntro } from './agents/emailAgents.js';
import { extractEntities, entityCanonicalizerAgent } from './agents/entityAgent.js';
import { generateExecutiveSummary } from './agents/executiveSummaryAgent.js';
import { assessHeadlinesInBatches } from './agents/headlineAgent.js';
import { judgePipelineOutput } from './agents/judgeAgent.js';
import { generateOpportunitiesFromEvent } from './agents/opportunityAgent.js';
import { classifyLinks as sectionClassifierAgent } from './agents/sectionClassifierAgent.js'; // aliased to match old export
import { suggestNewSelector } from './agents/selectorRepairAgent.js';
import { synthesizeEvent, synthesizeFromHeadline } from './agents/synthesisAgent.js';
import { generateWatchlistSuggestions } from './agents/watchlistAgent.js';

// --- Sanity Check Function ---
export async function performAiSanityCheck() {
  try {
    logger.info('🔬 Performing AI service sanity check (OpenAI)...')
    const answer = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      prompt: 'What is in one word the name of the capital of France',
      isJson: false,
    })
    if (
      answer &&
      typeof answer === 'string' &&
      answer.trim().toLowerCase().includes('paris')
    ) {
      logger.info('✅ AI service sanity check passed.')
      return true
    } else {
      logger.fatal(
        { details: { expected: 'paris', received: answer } },
        `OpenAI sanity check failed.`
      )
      return false
    }
  } catch (error) {
    if (error.status === 401 || error.message?.includes('Incorrect API key')) {
      logger.fatal(`OpenAI sanity check failed due to INVALID API KEY (401).`)
    } else {
      logger.fatal(
        { err: error },
        'OpenAI sanity check failed with an unexpected API error.'
      )
    }
    return false
  }
}

// --- EXPORT ALL FUNCTIONALITY ---

// Foundational exports
export { processChatRequest, callLanguageModel }

// Low-level chains (re-exported from chains/index.js)
export const {
  articleChain,
  articlePreAssessmentChain,
  clusteringChain,
  contactFinderChain,
  contactResolverChain,
  disambiguationChain,
  emailIntroChain,
  emailSubjectChain,
  entityCanonicalizerChain,
  entityExtractorChain,
  executiveSummaryChain,
  headlineChain,
  batchHeadlineChain,
  judgeChain,
  opportunityChain,
  sectionClassifierChain,
  selectorRepairChain,
  synthesisChain,
  watchlistSuggestionChain,
  translateChain,
  countryCorrectionChain,
} = chains

// External search and data retrieval services
export const {
  findAlternativeSources,
  performGoogleSearch,
  findNewsApiArticlesForEvent,
} = search
export const { fetchWikipediaSummary } = wikipedia

// Vector embedding and search services
export const { generateEmbedding } = embeddings
export const { findSimilarArticles } = vectorSearch

// High-level agent functions
export {
  assessArticleContent,
  preAssessArticle,
  batchAssessArticles,
  clusterArticlesIntoEvents,
  findContactDetails,
  generateEmailSubjectLine,
  generatePersonalizedIntro,
  extractEntities,
  entityCanonicalizerAgent,
  assessHeadlinesInBatches,
  judgePipelineOutput,
  generateOpportunitiesFromEvent,
  sectionClassifierAgent,
  suggestNewSelector,
  synthesizeEvent,
  synthesizeFromHeadline,
  generateWatchlistSuggestions,
  generateExecutiveSummary,
}
```

## 📄 src/embeddings/embeddings.js
*Lines: 235, Size: 7.98 KB*

```javascript
// src/lib/embeddings.js (Enhanced version with query expansion and caching)
"use server";
import { pipeline } from '@xenova/transformers';

// In-memory cache for embeddings (consider Redis for production)
const embeddingCache = new Map();
const MAX_CACHE_SIZE = 1000;

// Singleton pattern to ensure we only load the model once per server instance
class EmbeddingPipeline {
    static task = 'feature-extraction';
    static model = 'Xenova/all-MiniLM-L6-v2';
    static instance = null;
    
    static async getInstance() {
        if (this.instance === null) {
            const { pipeline } = await import('@xenova/transformers');
            this.instance = await pipeline(this.task, this.model);
        }
        return this.instance;
    }
}

/**
 * Creates a cache key from text
 * @param {string} text 
 * @returns {string}
 */
function createCacheKey(text) {
    return `embed_${text.toLowerCase().trim().replace(/\s+/g, '_')}`;
}

/**
 * Manages cache size to prevent memory bloat
 */
function manageCacheSize() {
    if (embeddingCache.size >= MAX_CACHE_SIZE) {
        // Remove oldest 20% of entries (FIFO-ish)
        const keysToRemove = Array.from(embeddingCache.keys()).slice(0, Math.floor(MAX_CACHE_SIZE * 0.2));
        keysToRemove.forEach(key => embeddingCache.delete(key));
        console.log(`[Embedding Cache] Cleaned ${keysToRemove.length} entries`);
    }
}

/**
 * Generates an embedding for a given text with caching
 * @param {string} text The text to embed
 * @returns {Promise<Array<number>>} A promise that resolves to the embedding vector
 */
export async function generateEmbedding(text) {
    if (!text || text.trim().length === 0) {
        throw new Error('Text cannot be empty for embedding generation');
    }
    
    const cleanText = text.trim();
    const cacheKey = createCacheKey(cleanText);
    
    // Check cache first
    if (embeddingCache.has(cacheKey)) {
        console.log(`[Embedding Cache] Hit for text: "${cleanText.substring(0, 50)}..."`);
        return embeddingCache.get(cacheKey);
    }
    
    try {
        const extractor = await EmbeddingPipeline.getInstance();
        const output = await extractor(cleanText, { pooling: 'mean', normalize: true });
        const embedding = Array.from(output.data);
        
        // Cache the result
        manageCacheSize();
        embeddingCache.set(cacheKey, embedding);
        
        console.log(`[Embedding] Generated embedding for text: "${cleanText.substring(0, 50)}..." (${embedding.length} dimensions)`);
        return embedding;
        
    } catch (error) {
        console.error(`[Embedding Error] Failed to generate embedding: ${error.message}`);
        throw new Error(`Failed to generate embedding: ${error.message}`);
    }
}

/**
 * Generates multiple query variations to improve RAG recall
 * @param {string} originalQuery 
 * @returns {Promise<Array<Array<number>>>} Array of embeddings for different query variations
 */
export async function generateQueryEmbeddings(originalQuery) {
    const variations = generateQueryVariations(originalQuery);
    const embeddingPromises = variations.map(query => generateEmbedding(query));
    
    try {
        const embeddings = await Promise.all(embeddingPromises);
        console.log(`[Query Expansion] Generated ${embeddings.length} query variations for: "${originalQuery}" ->`, variations);
        return embeddings;
    } catch (error) {
        console.error(`[Query Expansion Error] ${error.message}`);
        // Fallback to original query only
        return [await generateEmbedding(originalQuery)];
    }
}

/**
 * Creates query variations to improve semantic search recall
 * @param {string} query 
 * @returns {Array<string>}
 */
function generateQueryVariations(query) {
    const originalQuery = query.trim();
    const variations = new Set([originalQuery]);

    // CORRECTED: Smartly strip disambiguation tags for broader searches
    const coreEntity = originalQuery.replace(/\s*\((company|person)\)$/, '').trim();
    if (coreEntity !== originalQuery) {
        variations.add(coreEntity);
    }

    // Pattern for "Who founded X?"
    const हूंFounderMatch = coreEntity.toLowerCase().match(/^(?:who|what)\s+(?:is|was|founded|created)\s+(.+)/);
    if ( हूंFounderMatch) {
        let subject = हूंFounderMatch[1].replace(/\?/g, '').replace(/^(the|a|an)\s/,'').trim();
        variations.add(subject);
        variations.add(`${subject} founder`);
        variations.add(`founder of ${subject}`);
        variations.add(`${subject} history`);
    } else {
        // General question pattern
        const questionMatch = coreEntity.toLowerCase().match(/^(who|what|when|where|why|how)\s(is|are|was|were|did|does|do)\s(.+)/);
        if (questionMatch) {
            let subject = questionMatch[3].replace(/\?/g, '').trim();
            variations.add(subject);
            
            const simplified = subject.replace(/^(the|a|an)\s/,'').split(' of ');
            if (simplified.length > 1) {
                variations.add(`${simplified[1].trim()} ${simplified[0].trim()}`);
            }
        }
    }
    
    // Add generic variations for the core entity
    if (hasProperNouns(coreEntity)) {
        variations.add(`${coreEntity} background details`);
        variations.add(`Information about ${coreEntity}`);
    }
    
    // Return the top 4 most distinct variations
    return Array.from(variations).slice(0, 4);
}


/**
 * Simple check for proper nouns (capitalized words not at the start of a sentence)
 * @param {string} text 
 * @returns {boolean}
 */
function hasProperNouns(text) {
    // Looks for words starting with an uppercase letter
    return /\b[A-Z][a-z]+/.test(text);
}

/**
 * Batch embedding generation for efficiency
 * @param {Array<string>} texts 
 * @returns {Promise<Array<Array<number>>>}
 */
export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) {
        return [];
    }
    
    const embeddings = [];
    const extractor = await EmbeddingPipeline.getInstance();
    
    // Process in batches to avoid memory issues
    const BATCH_SIZE = 10;
    for (let i = 0; i < texts.length; i += BATCH_SIZE) {
        const batch = texts.slice(i, i + BATCH_SIZE);
        const batchPromises = batch.map(text => {
            const cacheKey = createCacheKey(text);
            if (embeddingCache.has(cacheKey)) {
                return Promise.resolve(embeddingCache.get(cacheKey));
            }
            return extractor(text, { pooling: 'mean', normalize: true })
                .then(output => {
                    const embedding = Array.from(output.data);
                    embeddingCache.set(cacheKey, embedding);
                    return embedding;
                });
        });
        
        const batchEmbeddings = await Promise.all(batchPromises);
        embeddings.push(...batchEmbeddings);
        
        console.log(`[Batch Embedding] Processed batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(texts.length/BATCH_SIZE)}`);
    }
    
    return embeddings;
}

/**
 * Calculate cosine similarity between two embeddings
 * @param {Array<number>} embedding1 
 * @param {Array<number>} embedding2 
 * @returns {Promise<number>} Similarity score between 0 and 1
 */
export async function calculateSimilarity(embedding1, embedding2) {
    if (embedding1.length !== embedding2.length) {
        throw new Error('Embeddings must have the same dimensions');
    }
    
    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;
    
    for (let i = 0; i < embedding1.length; i++) {
        dotProduct += embedding1[i] * embedding2[i];
        norm1 += embedding1[i] * embedding1[i];
        norm2 += embedding2[i] * embedding2[i];
    }
    
    if (norm1 === 0 || norm2 === 0) return 0;
    
    return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
}

/**
 * Get cache statistics for monitoring
 * @returns {Promise<Object>}
 */
export async function getCacheStats() {
    return {
        size: embeddingCache.size,
        maxSize: MAX_CACHE_SIZE,
        utilizationPercent: Math.round((embeddingCache.size / MAX_CACHE_SIZE) * 100)
    };
}
```

## 📄 src/embeddings/vectorSearch.js
*Lines: 77, Size: 2.49 KB*

```javascript
// packages/ai-services/src/embeddings/vectorSearch.js
'use server'
import { Pinecone } from '@pinecone-database/pinecone'
import { logger } from '@headlines/utils-server'
import { generateEmbedding } from './embeddings.js'
import { env } from '@headlines/config'

const { PINECONE_API_KEY, PINECONE_INDEX_NAME } = env

const SIMILARITY_THRESHOLD = 0.65
const MAX_CONTEXT_ARTICLES = 3
const MAX_RETRIES = 2 // Add retry configuration

let pineconeIndex
if (PINECONE_API_KEY) {
  const pc = new Pinecone({ apiKey: PINECONE_API_KEY })
  pineconeIndex = pc.index(PINECONE_INDEX_NAME)
} else {
  logger.warn(
    'Pinecone API Key not found. RAG/vector search functionality will be disabled.'
  )
}

export async function findSimilarArticles(queryText) {
  if (!pineconeIndex) return []
  logger.info('RAG: Searching for historical context in Pinecone...')
  if (!queryText || typeof queryText !== 'string' || queryText.trim().length === 0)
    return []

  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      const queryEmbedding = await generateEmbedding(queryText)

      const queryResponse = await pineconeIndex.query({
        topK: MAX_CONTEXT_ARTICLES,
        vector: queryEmbedding,
        includeMetadata: true,
      })

      const relevantMatches = queryResponse.matches.filter(
        (match) => match.score >= SIMILARITY_THRESHOLD
      )

      if (relevantMatches.length > 0) {
        const retrievedArticlesForLogging = relevantMatches
          .map(
            (match) =>
              `  - [Score: ${match.score.toFixed(3)}] "${match.metadata.headline}"`
          )
          .join('\n')
        logger.info(
          `RAG: Found ${relevantMatches.length} relevant historical articles:\n${retrievedArticlesForLogging}`
        )
        return relevantMatches.map((match) => ({
          headline: match.metadata.headline,
          newspaper: match.metadata.newspaper,
          assessment_article: match.metadata.summary,
        }))
      } else {
        logger.info('RAG: Found no relevant historical articles in Pinecone.')
        return []
      }
    } catch (error) {
      logger.error({ err: error }, `RAG: Pinecone query attempt ${attempt} failed.`)
      if (attempt === MAX_RETRIES) {
        logger.error(
          { err: error },
          'RAG: Pinecone query or embedding generation failed after all retries.'
        )
        return []
      }
      await new Promise((res) => setTimeout(res, 1000 * attempt)) // Exponential backoff
    }
  }
  return [] // Should be unreachable
}

```

## 📄 src/index.js
*Lines: 3, Size: 76 Bytes*

```javascript
// This is the default, Node.js-safe entry point.
export * from './core.js'

```

## 📄 src/lib/AIAgent.js
*Lines: 64, Size: 1.77 KB*

```javascript
import { callLanguageModel } from './langchain.js'
import { logger } from '@headlines/utils-server/node'

export class AIAgent {
  constructor({
    model,
    systemPrompt,
    isJson = true,
    fewShotInputs = [],
    fewShotOutputs = [],
    zodSchema,
  }) {
    if (!model || !systemPrompt) {
      throw new Error('AIAgent requires a model and systemPrompt.')
    }
    this.model = model
    this.systemPrompt = systemPrompt
    this.isJson = isJson
    this.fewShotInputs = fewShotInputs
    this.fewShotOutputs = fewShotOutputs
    this.zodSchema = zodSchema

    logger.trace(
      { agentConfig: { model, isJson, hasSchema: !!zodSchema } },
      'Initialized new AIAgent.'
    )
  }

  async execute(userContent) {
    let systemPromptContent = this.systemPrompt
    // If the provided prompt is a function (for dynamic settings), execute it.
    if (typeof systemPromptContent === 'function') {
      systemPromptContent = systemPromptContent()
    }

    const response = await callLanguageModel({
      modelName: this.model,
      systemPrompt: systemPromptContent,
      userContent,
      isJson: this.isJson,
      fewShotInputs: this.fewShotInputs,
      fewShotOutputs: this.fewShotOutputs,
    })

    if (this.isJson && this.zodSchema && !response.error) {
      const validationResult = this.zodSchema.safeParse(response)
      if (!validationResult.success) {
        logger.error(
          {
            details: validationResult.error.flatten(),
            model: this.model,
            rawResponse: response, // Log the raw response for debugging
          },
          `AI response failed Zod validation.`
        )
        return { error: 'Zod validation failed', details: validationResult.error }
      }
      return validationResult.data
    }

    return response
  }
}

```

## 📄 src/lib/langchain.js
*Lines: 92, Size: 2.97 KB*

```javascript
'use server';

import { ChatOpenAI } from '@langchain/openai';
import { env, settings } from '@headlines/config/node';
import { logger } from '@headlines/utils-server/node';
import { tokenTracker } from '@headlines/utils-server/node';
import { safeExecute } from '@headlines/utils-server/helpers';
import OpenAI from 'openai';

const modelConfig = { response_format: { type: 'json_object' } };

// Temperature parameters have been removed to rely on provider defaults, which is more robust.
export const getHeadlineModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_HEADLINE_ASSESSMENT }).bind(modelConfig);
export const getHighPowerModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_SYNTHESIS }).bind(modelConfig);
export const getUtilityModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_UTILITY }).bind(modelConfig);

const baseClient = new OpenAI({
  apiKey: env.OPENAI_API_KEY,
  timeout: 120 * 1000, // This is a TCP-level timeout, good to have.
  maxRetries: 3,
});

export async function callLanguageModel({
  modelName,
  prompt,
  systemPrompt,
  userContent,
  isJson = true,
  fewShotInputs = [],
  fewShotOutputs = [],
}) {
  const messages = [];
  if (systemPrompt) {
    const systemContent =
      typeof systemPrompt === 'object' ? JSON.stringify(systemPrompt) : systemPrompt;
    messages.push({ role: 'system', content: systemContent });
  }
  fewShotInputs.forEach((input, i) => {
    const shotContent = typeof input === 'string' ? input : JSON.stringify(input);
    if (shotContent) {
      messages.push({ role: 'user', content: shotContent });
      messages.push({ role: 'assistant', content: fewShotOutputs[i] });
    }
  });
  const finalUserContent = userContent || prompt;
  messages.push({ role: 'user', content: finalUserContent });
  logger.trace(
    { payload: { model: modelName, messages_count: messages.length } },
    'Sending payload to LLM.'
  );

  const apiPayload = {
    model: modelName,
    messages: messages,
  };
  if (isJson) {
    apiPayload.response_format = { type: 'json_object' };
  }

  // Use the robust safeExecute with its built-in timeout race.
  // We'll give it an 85-second timeout, slightly less than the safeExecute default.
  const result = await safeExecute(() => baseClient.chat.completions.create(apiPayload), {
    timeout: 85000,
  });

  if (!result) {
    // safeExecute will have already logged the error (timeout or otherwise).
    return { error: 'API call failed or timed out' };
  }

  if (result.usage) {
    tokenTracker.recordUsage(modelName, result.usage);
  }
  const responseContent = result.choices[0].message.content;
  logger.trace({ chars: responseContent.length }, 'Received LLM response.');

  if (isJson) {
    try {
      return JSON.parse(responseContent);
    } catch (parseError) {
      logger.error(
        { err: parseError, details: responseContent },
        `LLM response JSON Parse Error for model ${modelName}`
      );
      return { error: 'JSON Parsing Error' };
    }
  }
  return responseContent;
}
```

## 📄 src/lib/safeInvoke.js
*Lines: 94, Size: 2.96 KB*

```javascript
// File: packages/ai-services/src/lib/safeInvoke.js

'use server'
import { logger } from '@headlines/utils-server/logger'
import { getRedisClient } from '@headlines/utils-server/redisClient'
import { createHash } from 'crypto'
// REMOVED: StringOutputParser is no longer needed.

const MAX_RETRIES = 1
const CACHE_TTL_SECONDS = 60 * 60 * 24
const inMemoryCache = new Map()

function createCacheKey(agentName, input) {
  const hash = createHash('sha256') // FIX 1: Corrected hashing algorithm from 'sha269' to 'sha256'
  hash.update(JSON.stringify(input))
  return `ai_cache:${agentName}:${hash.digest('hex')}`
}

export async function safeInvoke(chain, input, agentName, zodSchema) {
  const redis = await getRedisClient()
  const cacheKey = createCacheKey(agentName, input)

  if (redis) {
    try {
      const cachedResult = await redis.get(cacheKey)
      if (typeof cachedResult === 'string' && cachedResult.length > 0) {
        logger.trace({ agent: agentName }, `[Redis Cache HIT] for ${agentName}.`)
        return JSON.parse(cachedResult)
      }
    } catch (err) {
      logger.error(
        { err, agent: agentName, key: cacheKey },
        `Redis GET or PARSE failed for ${agentName}.`
      )
    }
  } else if (inMemoryCache.has(cacheKey)) {
    logger.trace({ agent: agentName }, `[In-Memory Cache HIT] for ${agentName}.`)
    return inMemoryCache.get(cacheKey)
  }

  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
    try {
      // FIX 2: Invoke the chain directly. It already has a JsonOutputParser.
      // This eliminates the TypeError by not piping a JS object into a StringOutputParser.
      const result = await chain.invoke(input)

      const validation = zodSchema.safeParse(result)
      if (!validation.success) {
        logger.error(
          {
            details: validation.error.flatten(),
            agent: agentName,
            input,
            output: result,
          },
          `Zod validation failed for ${agentName}.`
        )
        throw new Error('Zod validation failed')
      }
      const dataToCache = validation.data

      if (redis) {
        try {
          await redis.set(cacheKey, JSON.stringify(dataToCache), {
            EX: CACHE_TTL_SECONDS,
          })
        } catch (err) {
          logger.error({ err, agent: agentName }, `Redis SET failed for ${agentName}.`)
        }
      } else {
        inMemoryCache.set(cacheKey, dataToCache)
      }
      return dataToCache
    } catch (error) {
      if (
        (error.message.includes('JSON') ||
          error.message.includes('Zod validation failed')) &&
        attempt < MAX_RETRIES
      ) {
        logger.warn(
          { agent: agentName, attempt: attempt + 1 },
          `LLM output validation failed for ${agentName}. Retrying...`
        )
        continue
      }
      logger.error(
        { err: error, agent: agentName },
        `LangChain invocation failed for ${agentName}.`
      )
      return { error: `Agent ${agentName} failed: ${error.message}` }
    }
  }
}

```

## 📄 src/next.js
*Lines: 65, Size: 1.46 KB*

```javascript
import 'server-only'

// Explicitly re-export all functions and services from the core module
// for the Next.js server runtime.
export {
  // Foundational exports
  performAiSanityCheck,
  processChatRequest,
  callLanguageModel,

  // Low-level chains
  articleChain,
  articlePreAssessmentChain,
  clusteringChain,
  contactFinderChain,
  contactResolverChain,
  disambiguationChain,
  emailIntroChain,
  emailSubjectChain,
  entityCanonicalizerChain,
  entityExtractorChain,
  executiveSummaryChain,
  headlineChain,
  batchHeadlineChain,
  judgeChain,
  opportunityChain,
  sectionClassifierChain,
  selectorRepairChain,
  synthesisChain,
  watchlistSuggestionChain,
  translateChain,
  countryCorrectionChain,

  // External search and data retrieval services
  findAlternativeSources,
  performGoogleSearch,
  findNewsApiArticlesForEvent,
  fetchWikipediaSummary,

  // Vector embedding and search services
  generateEmbedding,
  findSimilarArticles,

  // High-level agent functions
  assessArticleContent,
  preAssessArticle,
  batchAssessArticles,
  clusterArticlesIntoEvents,
  resolveVagueContact,
  findContactDetails,
  generateEmailSubjectLine,
  generatePersonalizedIntro,
  extractEntities,
  entityCanonicalizerAgent,
  assessHeadlinesInBatches,
  judgePipelineOutput,
  generateOpportunitiesFromEvent,
  sectionClassifierAgent,
  suggestNewSelector,
  synthesizeEvent,
  synthesizeFromHeadline,
  generateWatchlistSuggestions,
  generateExecutiveSummary,
} from './core.js'

```

## 📄 src/rag/generation.js
*Lines: 129, Size: 4.17 KB*

```javascript
// File: packages/ai-services/src/rag/generation.js (Unabridged, Final Fix)

'use server'

import { getSynthesizerPrompt } from './prompts.js'
import { checkGroundedness } from './validation.js'
import { callLanguageModel } from '../lib/langchain.js'
import { settings } from '@headlines/config'

const SYNTHESIZER_MODEL = settings.LLM_MODEL_SYNTHESIS

function assembleContext(ragResults, wikiResults, searchResults) {
  const dbContext =
    ragResults.length > 0
      ? ragResults
          .map(
            (match) =>
              `- [Similarity: ${match.score.toFixed(3)}] ${match.metadata.headline}: ${match.metadata.summary}`
          )
          .join('\n')
      : 'None'

  const wikiContext =
    wikiResults.length > 0
      ? wikiResults
          .map(
            (res) => `- [Quality: ${res.validation.quality}] ${res.title}: ${res.summary}`
          )
          .join('\n')
      : 'None'

  const searchContext =
    searchResults.length > 0
      ? searchResults
          .map((res) => `- [${res.title}](${res.link}): ${res.snippet}`)
          .join('\n')
      : 'None'

  return `---
Internal Database Context:
${dbContext}
---
Wikipedia Context:
${wikiContext}
---
Search Results Context:
${searchContext}
---`
}

function formatThoughts(plan, context, groundednessResult) {
  const thoughts = `
**THOUGHT PROCESS: THE PLAN**
${plan.plan.map((step) => `- ${step}`).join('\n')}

**REASONING:**
${plan.reasoning}

**RETRIEVED CONTEXT:**
- **Internal RAG Search:** ${context.ragResults.length} item(s) found.
${context.ragResults.map((r) => `  - [Score: ${r.score.toFixed(2)}] ${r.metadata.headline}`).join('\n')}

- **Wikipedia Search:** ${context.wikiResults.length} article(s) found.
${context.wikiResults.map((w) => `  - **Query:** "${w.query}"\n    - **Result:** ${w.title}: ${w.summary.substring(0, 100)}...`).join('\n')}

- **Web Search:** ${context.searchResults.length} result(s) found.
${context.searchResults.map((s) => `  - **Query:** "${plan.user_query}"\n    - **Result:** ${s.title}: ${s.snippet.substring(0, 100)}...`).join('\n')}

**FINAL CHECK:**
- **Groundedness Passed:** ${groundednessResult.is_grounded ? 'CONFIRMED' : 'FAILED'}
`
  return thoughts.trim().replace(/\n\n+/g, '\n\n')
}

export async function generateFinalResponse({ plan, context }) {
  const fullContextString = assembleContext(
    context.ragResults,
    context.wikiResults,
    context.searchResults
  )

  console.log(`[RAG Generation] Calling Synthesizer Agent with ${SYNTHESIZER_MODEL}...`)
  const synthesizerResponse = await callLanguageModel({
    modelName: SYNTHESIZER_MODEL,
    systemPrompt: getSynthesizerPrompt(),
    userContent: `CONTEXT:\n${fullContextString}\n\nPLAN:\n${JSON.stringify(
      plan.plan,
      null,
      2
    )}\n\nUSER'S QUESTION: "${plan.user_query}"`,
    isJson: false,
    // temperature: 0.1, // <-- THIS LINE IS NOW REMOVED
  })

  const rawResponse = synthesizerResponse
  if (typeof rawResponse !== 'string') {
    console.error(
      '[RAG Generation] Synthesizer Agent failed to return a valid string response.',
      rawResponse
    )
    return {
      answer: 'The AI synthesizer failed to generate a response.',
      thoughts: 'An error occurred during the final synthesis step.',
    }
  }

  const groundednessResult = await checkGroundedness(rawResponse, fullContextString)
  const thoughts = formatThoughts(plan, context, groundednessResult)

  let finalAnswer
  if (groundednessResult.is_grounded) {
    let responseWithSpans = rawResponse.replace(/<rag>/g, '<span class="rag-source">')
    responseWithSpans = responseWithSpans.replace(/<\/rag>/g, '</span>')
    responseWithSpans = responseWithSpans.replace(/<wiki>/g, '<span class="wiki-source">')
    responseWithSpans = responseWithSpans.replace(/<\/wiki>/g, '</span>')
    responseWithSpans = responseWithSpans.replace(
      /<search>/g,
      '<span class="llm-source">'
    )
    finalAnswer = responseWithSpans.replace(/<\/search>/g, '</span>')
  } else {
    console.warn('[RAG Pipeline] Groundedness check failed. Returning safe response.')
    finalAnswer =
      'I was unable to construct a reliable answer from the available sources. The context may be insufficient or conflicting.'
  }

  return { answer: finalAnswer, thoughts }
}

```

## 📄 src/rag/orchestrator.js
*Lines: 49, Size: 1.79 KB*

```javascript
'use server';

// src/lib/rag/orchestrator.js (version 4.2)
import { retrieveContextForQuery } from './retrieval.js'
import { assessContextQuality } from './validation.js'
import { generateFinalResponse } from './generation.js'
import { runPlannerAgent } from './planner.js'

/**
 * Main orchestrator for the Agentic RAG chat pipeline.
 * @param {Array<object>} messages - The chat messages from the client.
 * @returns {Promise<string>} The final, validated text response.
 */
export async function processChatRequest(messages) {
  console.log('--- [RAG Pipeline Start] ---')

  // 1. Planning Phase
  console.log('[RAG Pipeline] Step 1: Planning Phase Started...')
  const plan = await runPlannerAgent(messages)
  console.log('[RAG Pipeline] Step 1: Planning Phase Completed.')

  // 2. Retrieval & Validation Phase
  console.log('[RAG Pipeline] Step 2: Retrieval Phase Started...')
  const initialContext = await retrieveContextForQuery(plan, messages, 'ragOnly')
  const initialQuality = assessContextQuality(initialContext.ragResults, [], [])

  let finalContext = initialContext

  if (initialQuality.hasHighConfidenceRAG) {
    console.log('[RAG Pipeline] High confidence RAG hit. Short-circuiting retrieval.')
  } else {
    console.log('[RAG Pipeline] RAG context insufficient. Proceeding to full retrieval.')
    // Perform the remaining retrieval steps
    finalContext = await retrieveContextForQuery(plan, messages, 'full')
  }
  console.log('[RAG Pipeline] Step 2: Retrieval Phase Completed.')

  // 3. Synthesis Phase
  console.log('[RAG Pipeline] Step 3: Synthesis Phase Started...')
  const finalResponse = await generateFinalResponse({
    plan,
    context: finalContext,
  })
  console.log('[RAG Pipeline] Step 3: Synthesis Phase Completed.')

  console.log('--- [RAG Pipeline End] ---')
  return finalResponse
}

```

## 📄 src/rag/planner.js
*Lines: 40, Size: 1.1 KB*

```javascript
// File: packages/ai-services/src/rag/planner.js (Unabridged and Corrected)

'use server'

import { callLanguageModel } from '../lib/langchain.js'
import { PLANNER_PROMPT } from './prompts.js'
import { settings } from '@headlines/config'

const PLANNER_MODEL = settings.LLM_MODEL_UTILITY

export async function runPlannerAgent(messages) {
  const conversationText = messages
    .map((m) => `${m.role.toUpperCase()}: ${m.content}`)
    .join('\n\n')

  console.log(`[Planner Agent] Generating plan with ${PLANNER_MODEL}...`)

  const response = await callLanguageModel({
    modelName: PLANNER_MODEL,
    systemPrompt: PLANNER_PROMPT,
    userContent: conversationText,
    isJson: true,
  })

  if (response.error) {
    throw new Error(`Planner Agent failed: ${response.error}`)
  }

  const planObject = response

  console.groupCollapsed('[Planner Agent] Plan Generated')
  console.log('User Query:', planObject.user_query)
  console.log('Reasoning:', planObject.reasoning)
  console.log('Plan Steps:', planObject.plan)
  console.log('Search Queries:', planObject.search_queries)
  console.groupEnd()

  return planObject
}

```

## 📄 src/rag/prompts.js
*Lines: 103, Size: 6.08 KB*

```javascript
// File: packages/ai-services/src/rag/prompts.js (Unabridged and Corrected)

export const PLANNER_PROMPT = `You are an expert AI Planner. Your job is to analyze the user's query and conversation history to create a step-by-step plan for an AI Synthesizer Agent to follow. You also create a list of optimized search queries for a Retrieval Agent.

**Conversation History:**
{CONVERSATION_HISTORY}

**Latest User Query:**
"{USER_QUERY}"

**Your Task:**
1.  **Analyze the User's Intent:** Understand what the user is truly asking for.
2.  **Formulate a Plan:** Create a clear, step-by-step plan for the Synthesizer Agent.
3.  **Generate Search Queries:** Create an array of 1-3 optimized, self-contained search queries. **CRITICAL JSON RULE:** If a query within the 'search_queries' array requires double quotes, you MUST escape them with a backslash. For example: ["\\"Troels Holch Povlsen\\" sons", "Bestseller founder"].

**Example 1:**
User Query: "Which Danish Rich List person is involved in Technology?"
History: (empty)
Your JSON Output:
{
  "user_query": "Which Danish Rich List person is involved in Technology?",
  "reasoning": "The user wants a list of wealthy Danes involved in technology. I need to identify these individuals from the context and then filter them based on their tech involvement.",
  "plan": [
    "Scan all context to identify every unique individual mentioned who is on the Danish Rich List.",
    "For each person, look for evidence of direct involvement in the technology sector.",
    "Filter out individuals with no clear connection to technology.",
    "Synthesize the findings into a helpful list of names, citing their connection to technology.",
    "If no one is found, state that clearly."
  ],
  "search_queries": ["Danish Rich List technology involvement", "Wealthy Danish tech investors", "Danish tech company founders"]
}

**Example 2:**
User Query: "Does Troels Holch Povlsen have sons?"
History: (assistant previously mentioned Bestseller's founder)
Your JSON Output:
{
  "user_query": "Does Troels Holch Povlsen have sons?",
  "reasoning": "The user is asking a direct factual question about a specific person's family. The search queries must be precise.",
  "plan": [
      "Scan context for any mention of 'Troels Holch Povlsen' and his family, specifically children or sons.",
      "Extract the names of his sons if mentioned.",
      "Synthesize a complete and helpful answer, stating the names of the sons and any additional relevant context provided."
  ],
  "search_queries": ["\\"Troels Holch Povlsen\\" sons", "\\"Troels Holch Povlsen\\" children", "\\"Bestseller\\" founder family"]
}

Respond ONLY with a valid JSON object with the specified structure.
`

export const getSynthesizerPrompt =
  () => `You are an elite, fact-based intelligence analyst. Your SOLE task is to execute the provided "PLAN" using only the "CONTEXT" to answer the "USER'S QUESTION". You operate under a strict "ZERO HALLUCINATION" protocol. Your response must be confident, direct, and sound like a human expert.

**PRIMARY DIRECTIVE:**
Synthesize information from all sources in the "CONTEXT" into a single, cohesive, and well-written answer. Directly address the user's question and enrich it with relevant surrounding details found in the context.

**EXAMPLE TONE:**
-   **Bad:** "According to the context, Bestseller was founded by Troels Holch Povlsen."
-   **Good:** "Bestseller was founded in 1975 by Troels Holch Povlsen and his wife, Merete Bech Povlsen. The company is now run by their son, Anders Holch Povlsen."

**CRITICAL RULES OF ENGAGEMENT:**
1.  **NO OUTSIDE KNOWLEDGE:** You are forbidden from using any information not present in the provided "CONTEXT".
2.  **DIRECT ATTRIBUTION:** You MUST still cite your sources inline for the UI. Wrap facts from the Internal DB with <rag>tags</rag>, from Wikipedia with <wiki>tags</wiki>, and from Search Results with <search>tags</search>. The user will not see these tags, but they are essential for the system.
3.  **BE CONFIDENT AND DIRECT:** Present the synthesized facts as a definitive answer.
4.  **INSUFFICIENT DATA:** If the context is insufficient to answer the question at all, respond with EXACTLY: "I do not have sufficient information in my sources to answer that question."
5.  **DO NOT OFFER HELP (CRITICAL):** You MUST NOT end your response by offering to search for more information, provide more details, or ask follow-up questions. Your answer should be a complete, self-contained statement of facts.

**DO NOT:**
-   Use phrases like "According to the context provided...", "The sources state...", or "Based on the information...".
-   Apologize for not knowing or mention your limitations.
-   Talk about your process in the final answer.
-   Speculate or infer beyond what is explicitly stated in the context.

Answer the question directly and authoritatively, as if you are a world-class analyst presenting your verified findings.`

export const GROUNDEDNESS_CHECK_PROMPT = `You are a meticulous fact-checker AI. Your task is to determine if the "Proposed Response" is strictly grounded in the "Provided Context". A response is grounded if and only if ALL of its claims can be directly verified from the context.

**Provided Context:**
---
{CONTEXT}
---

**Proposed Response:**
---
{RESPONSE}
---

Analyze the "Proposed Response" sentence by sentence.

**Respond ONLY with a valid JSON object with the following structure:**
{
  "is_grounded": boolean, // true if ALL claims in the response are supported by the context, otherwise false.
  "unsupported_claims": [
    // List any specific claims from the response that are NOT supported by the context.
    "Claim 1 that is not supported.",
    "Claim 2 that is not supported."
  ]
}

If the response is fully supported, "unsupported_claims" should be an empty array. If the "Proposed Response" states that it cannot answer the question, consider it grounded.`

export const FAILED_GROUNDEDNESS_PROMPT = `I could not form a reliable answer based on the available information. The initial response I generated may have contained information not supported by the sources. For accuracy, please ask a more specific question or try rephrasing your request.`

```

## 📄 src/rag/retrieval.js
*Lines: 183, Size: 5.46 KB*

```javascript
// File: packages/ai-services/src/rag/retrieval.js (Unabridged and Corrected)

'use server'

import { OpenAI } from 'openai'
import { Pinecone } from '@pinecone-database/pinecone'
import { generateQueryEmbeddings } from '../embeddings/embeddings.js'
import {
  fetchBatchWikipediaSummaries,
  validateWikipediaContent,
} from '../search/wikipedia.js'
import { getGoogleSearchResults } from '../search/serpapi.js'
import { env } from '@headlines/config'

let openAIClient, pineconeIndex
function initializeClients() {
  if (!openAIClient) {
    openAIClient = new OpenAI({ apiKey: env.OPENAI_API_KEY })
    const pc = new Pinecone({ apiKey: env.PINECONE_API_KEY })
    pineconeIndex = pc.index(env.PINECONE_INDEX_NAME)
  }
}

const ENTITY_EXTRACTOR_MODEL = 'gpt-5-mini'
const SIMILARITY_THRESHOLD = 0.38
const ENTITY_EXTRACTOR_PROMPT_FOR_HISTORY = `You are an entity extractor. Your job is to identify all specific people and companies mentioned in a given text.
Respond ONLY with a valid JSON object with the following structure:
{ "entities": ["Entity Name 1", "Entity Name 2"] }
`

// This function is no longer called by the main orchestrator but is kept for potential future use.
async function extractEntitiesFromHistory(messages) {
  if (messages.length < 2) {
    return []
  }
  const historyText = messages
    .slice(-4)
    .map((m) => m.content)
    .join('\n')

  initializeClients()
  try {
    const entityResponse = await openAIClient.chat.completions.create({
      model: ENTITY_EXTRACTOR_MODEL,
      messages: [
        { role: 'system', content: ENTITY_EXTRACTOR_PROMPT_FOR_HISTORY },
        {
          role: 'user',
          content: `Extract all key people and companies from this text:\n"${historyText}"`,
        },
      ],
      response_format: { type: 'json_object' },
    })

    const { entities } = JSON.parse(entityResponse.choices[0].message.content)
    const cleanEntities = entities.map((e) =>
      e.replace(/\s*\((person|company)\)$/, '').trim()
    )
    if (cleanEntities.length > 0) {
      console.log(
        `[RAG Retrieval] Entities from history for exclusion: ${cleanEntities.join(', ')}`
      )
    }
    return cleanEntities
  } catch (error) {
    console.error('Could not extract entities from history:', error)
    return []
  }
}

async function fetchPineconeContext(queries, exclude_entities = []) {
  initializeClients()
  const queryEmbeddings = await Promise.all(
    queries.map((q) => generateQueryEmbeddings(q))
  )
  const allQueryEmbeddings = queryEmbeddings.flat()

  const filter =
    exclude_entities.length > 0
      ? { 'metadata.entities': { $nin: exclude_entities } }
      : undefined

  if (filter) {
    console.log('[RAG Retrieval] Applying Pinecone filter to exclude:', exclude_entities)
  }

  const pineconePromises = allQueryEmbeddings.map((embedding) =>
    pineconeIndex.query({
      topK: 5,
      vector: embedding,
      includeMetadata: true,
      filter: filter,
    })
  )
  const pineconeResponses = await Promise.all(pineconePromises)

  const uniqueMatches = new Map()
  pineconeResponses.forEach((response) => {
    response?.matches?.forEach((match) => {
      if (
        !uniqueMatches.has(match.id) ||
        match.score > uniqueMatches.get(match.id).score
      ) {
        uniqueMatches.set(match.id, match)
      }
    })
  })

  const results = Array.from(uniqueMatches.values())
    .filter((match) => match.score >= SIMILARITY_THRESHOLD)
    .sort((a, b) => b.score - a.score)
    .slice(0, 5)

  console.groupCollapsed(`[RAG Retrieval] Pinecone Results (${results.length})`)
  results.forEach((match) => {
    console.log(`- Score: ${match.score.toFixed(4)} | ID: ${match.id}`)
    console.log(`  Headline: ${match.metadata.headline}`)
  })
  console.groupEnd()

  return results
}

async function fetchValidatedWikipediaContext(entities) {
  const wikiResults = await fetchBatchWikipediaSummaries(entities)
  const validWikiResults = []
  for (const res of wikiResults.filter((r) => r.success)) {
    const validation = await validateWikipediaContent(res.summary)
    if (validation.valid) {
      validWikiResults.push({ ...res, validation })
    }
  }

  console.groupCollapsed(`[RAG Retrieval] Wikipedia Results (${validWikiResults.length})`)
  validWikiResults.forEach((res) => {
    console.log(`- Title: ${res.title}`)
    console.log(`  Summary: ${res.summary.substring(0, 200)}...`)
  })
  console.groupEnd()

  return validWikiResults
}

export async function retrieveContextForQuery(plan, messages, mode = 'full') {
  const { search_queries, user_query } = plan

  const entitiesToExclude = []
  console.log('[RAG Retrieval] History-based entity exclusion is temporarily disabled.')

  const pineconeResults = await fetchPineconeContext(search_queries, entitiesToExclude)

  if (mode === 'ragOnly') {
    return {
      ragResults: pineconeResults,
      wikiResults: [],
      searchResults: [],
    }
  }

  const [wikipediaResults, searchResultsObj] = await Promise.all([
    fetchValidatedWikipediaContext(search_queries),
    getGoogleSearchResults(user_query),
  ])

  const searchResults = searchResultsObj.success ? searchResultsObj.results : []

  console.groupCollapsed(
    `[RAG Retrieval] SerpAPI Google Search Results (${searchResults.length})`
  )
  searchResults.forEach((res) => {
    console.log(`- Title: ${res.title}`)
    console.log(`  Link: ${res.link}`)
    console.log(`  Snippet: ${res.snippet}`)
  })
  console.groupEnd()

  return {
    ragResults: pineconeResults,
    wikiResults: wikipediaResults,
    searchResults: searchResults,
  }
}

```

## 📄 src/rag/validation.js
*Lines: 142, Size: 4.79 KB*

```javascript
// packages/ai-services/src/rag/validation.js (Corrected)
import { settings } from '@headlines/config'
import { callLanguageModel } from '../lib/langchain.js'
import { GROUNDEDNESS_CHECK_PROMPT } from './prompts.js'

// --- Constants ---
const HIGH_CONFIDENCE_THRESHOLD = 0.75
const SIMILARITY_THRESHOLD = 0.38

// --- Internal Helper Functions ---
function simpleEntityExtractor(text, sourceIdentifier) {
  const match = text.match(/([A-Z][a-z]+(?:\s[A-Z][a-z]+)*)/)
  if (match) return [{ name: match[0], facts: [text] }]
  return [{ name: sourceIdentifier, facts: [text] }]
}

function extractEntitiesFromRAG(ragResults) {
  if (!ragResults) return []
  return ragResults.flatMap((r) =>
    simpleEntityExtractor(r.metadata?.summary || '', r.metadata?.headline || 'RAG Source')
  )
}

function extractEntitiesFromWiki(wikiResults) {
  if (!wikiResults) return []
  return wikiResults.flatMap((w) =>
    simpleEntityExtractor(w.summary || '', w.title || 'Wiki Source')
  )
}

function entitySimilarity(entityA, entityB) {
  const nameA = entityA.name.toLowerCase()
  const nameB = entityB.name.toLowerCase()
  if (nameA.includes(nameB) || nameB.includes(nameA)) return 0.9
  return 0
}

function factsConflict(factsA, factsB) {
  // Placeholder for a future NLI model implementation.
  return false
}

// --- Exported Validation Functions ---

export function assessContextQuality(ragResults, wikiResults, searchResults) {
  const ragScore = ragResults.length > 0 ? Math.max(...ragResults.map((r) => r.score)) : 0
  const highQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'high'
  ).length
  const mediumQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'medium'
  ).length
  const wikiScore = highQualityWiki > 0 ? 0.7 : mediumQualityWiki > 0 ? 0.5 : 0
  const searchScore = searchResults.length > 0 ? 0.6 : 0 // Assign a moderate score if search results exist

  const combinedScore = Math.max(ragScore, wikiScore, searchScore)

  return {
    hasHighConfidenceRAG: ragScore >= HIGH_CONFIDENCE_THRESHOLD,
    hasSufficientContext: combinedScore >= SIMILARITY_THRESHOLD,
    ragResultCount: ragResults.length,
    wikiResultCount: wikiResults.length,
    searchResultCount: searchResults.length,
    highQualityWikiCount: highQualityWiki,
    maxSimilarity: ragScore,
    combinedConfidence: combinedScore,
    hasMultipleSources:
      (ragResults.length > 0 ? 1 : 0) +
        (wikiResults.length > 0 ? 1 : 0) +
        (searchResults.length > 0 ? 1 : 0) >
      1,
    hasHighQualityContent: ragScore >= HIGH_CONFIDENCE_THRESHOLD || highQualityWiki > 0,
  }
}

export function crossValidateSources(ragResults, wikiResults) {
  // This function is becoming less critical with the strict generation prompt,
  // but we'll keep its structure.
  const validation = { conflicts: [], confirmations: [], reliability: 'unknown' }

  if (
    (!ragResults || ragResults.length === 0) &&
    (!wikiResults || wikiResults.length === 0)
  ) {
    validation.reliability = 'single_source' // or 'no_source'
    return validation
  }

  // Simplified logic for now
  if (ragResults.length > 0 && wikiResults.length > 0) {
    validation.reliability = 'confirmed' // Assume confirmation if both exist
  } else {
    validation.reliability = 'single_source'
  }

  return validation
}

export async function checkGroundedness(responseText, contextString) {
  console.log('[RAG Validation] Performing Groundedness Check...')
  if (
    responseText.trim() ===
    'I do not have sufficient information in my sources to answer that question.'
  ) {
    console.log('[RAG Validation] PASSED: Bot correctly stated insufficient info.')
    return { is_grounded: true, unsupported_claims: [] }
  }

  try {
    const prompt = GROUNDEDNESS_CHECK_PROMPT.replace('{CONTEXT}', contextString).replace(
      '{RESPONSE}',
      responseText
    )

    // CORRECTED: Use the project's standard AI call function and configured model
    const result = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: prompt,
      userContent: 'Perform the groundedness check based on the system prompt.',
      isJson: true,
    })

    if (result.error) {
      throw new Error(result.error)
    }

    if (result.is_grounded) {
      console.log('[RAG Validation] PASSED: Response is grounded in sources.')
    } else {
      console.warn('[RAG Validation] FAILED: Response contains unsupported claims.')
      console.groupCollapsed('Unsupported Claims Details')
      result.unsupported_claims.forEach((claim) => console.warn(`- ${claim}`))
      console.groupEnd()
    }
    return result
  } catch (error) {
    console.error('[RAG Validation] Error during verification:', error)
    // Fail safe: if the check fails, assume the response is not grounded.
    return { is_grounded: false, unsupported_claims: ['Fact-checking system failed.'] }
  }
}

```

## 📄 src/schemas/articleAssessmentSchema.js
*Lines: 24, Size: 753 Bytes*

```javascript
// packages/ai-services/src/schemas/articleAssessmentSchema.js (version 1.1)
import { z } from 'zod'

export const articleAssessmentSchema = z.object({
  reasoning: z.object({
    event_type: z.string(),
    is_liquidity_event: z.boolean(),
    beneficiary: z.string(),
  }),
  // NEW FIELD: Added classification to the schema.
  classification: z.enum(['New wealth', 'Wealth detection', 'Interview', 'IPO', 'Other']),
  relevance_article: z.number().min(0).max(100),
  assessment_article: z.string().min(1),
  amount: z.number().nullable().optional(),
  key_individuals: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      company: z.string().nullable(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/schemas/articlePreAssessmentSchema.js
*Lines: 7, Size: 223 Bytes*

```javascript
// packages/ai-services/src/schemas/articlePreAssessmentSchema.js (version 1.0)
import { z } from 'zod'

export const articlePreAssessmentSchema = z.object({
  classification: z.enum(['private', 'public', 'corporate']),
})

```

## 📄 src/schemas/batchArticleAssessmentSchema.js
*Lines: 8, Size: 285 Bytes*

```javascript
// packages/ai-services/src/schemas/batchArticleAssessmentSchema.js (version 1.0)
import { z } from 'zod'
import { articleAssessmentSchema } from './articleAssessmentSchema.js'

export const batchArticleAssessmentSchema = z.object({
  assessments: z.array(articleAssessmentSchema),
})

```

## 📄 src/schemas/batchHeadlineAssessmentSchema.js
*Lines: 9, Size: 371 Bytes*

```javascript
// packages/ai-services/src/schemas/batchHeadlineAssessmentSchema.js (version 1.0)
import { z } from 'zod'
import { headlineAssessmentSchema } from './headlineAssessmentSchema.js'

// The batch schema reuses the single assessment schema
export const batchHeadlineAssessmentSchema = z.object({
  assessments: z.array(headlineAssessmentSchema.shape.assessment.element),
})

```

## 📄 src/schemas/canonicalizerSchema.js
*Lines: 7, Size: 188 Bytes*

```javascript
// packages/ai-services/src/schemas/canonicalizerSchema.js (version 1.0)
import { z } from 'zod'

export const canonicalizerSchema = z.object({
  canonical_name: z.string().nullable(),
})

```

## 📄 src/schemas/clusterSchema.js
*Lines: 12, Size: 250 Bytes*

```javascript
// packages/ai-services/src/schemas/clusterSchema.js (version 1.0)
import { z } from 'zod'

export const clusterSchema = z.object({
  events: z.array(
    z.object({
      event_key: z.string(),
      article_ids: z.array(z.string()),
    })
  ),
})

```

## 📄 src/schemas/countryCorrectionSchema.js
*Lines: 7, Size: 269 Bytes*

```javascript
// packages/ai-services/src/schemas/countryCorrectionSchema.js
import { z } from 'zod';

export const countryCorrectionSchema = z.object({
  country: z.string().nullable().describe("The single, corrected, UN-recognized country name, or null if not determinable."),
});

```

## 📄 src/schemas/disambiguationSchema.js
*Lines: 7, Size: 186 Bytes*

```javascript
// packages/ai-services/src/schemas/disambiguationSchema.js (version 1.0)
import { z } from 'zod'

export const disambiguationSchema = z.object({
  best_title: z.string().nullable(),
})

```

## 📄 src/schemas/emailIntroSchema.js
*Lines: 10, Size: 240 Bytes*

```javascript
// packages/ai-services/src/schemas/emailIntroSchema.js (version 2.0)
import { z } from 'zod'

export const emailIntroSchema = z.object({
  greeting: z.string(),
  body: z.string(),
  bullets: z.array(z.string()),
  signoff: z.string(),
})

```

## 📄 src/schemas/emailSubjectSchema.js
*Lines: 7, Size: 184 Bytes*

```javascript
// packages/ai-services/src/schemas/emailSubjectSchema.js (version 1.0)
import { z } from 'zod'

export const emailSubjectSchema = z.object({
  subject_headline: z.string().min(1),
})

```

## 📄 src/schemas/enrichContactSchema.js
*Lines: 14, Size: 335 Bytes*

```javascript
// packages/ai-services/src/schemas/enrichContactSchema.js (version 1.0)
import { z } from 'zod'

export const enrichContactSchema = z.object({
  enriched_contacts: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      company: z.string(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/schemas/entitySchema.js
*Lines: 8, Size: 191 Bytes*

```javascript
// packages/ai-services/src/schemas/entitySchema.js (version 1.0)
import { z } from 'zod'

export const entitySchema = z.object({
  reasoning: z.string(),
  entities: z.array(z.string()),
})

```

## 📄 src/schemas/executiveSummarySchema.js
*Lines: 7, Size: 176 Bytes*

```javascript
// packages/ai-services/src/schemas/executiveSummarySchema.js (version 1.0)
import { z } from 'zod'

export const executiveSummarySchema = z.object({
  summary: z.string(),
})

```

## 📄 src/schemas/findContactSchema.js
*Lines: 7, Size: 183 Bytes*

```javascript
// packages/ai-services/src/schemas/findContactSchema.js (version 1.0)
import { z } from 'zod'

export const findContactSchema = z.object({
  email: z.string().email().nullable(),
})

```

## 📄 src/schemas/headlineAssessmentSchema.js
*Lines: 13, Size: 362 Bytes*

```javascript
// packages/ai-services/src/schemas/headlineAssessmentSchema.js (version 1.0)
import { z } from 'zod'

const singleAssessmentSchema = z.object({
  headline_en: z.string(),
  relevance_headline: z.number().min(0).max(100),
  assessment_headline: z.string(),
})

export const headlineAssessmentSchema = z.object({
  assessment: z.array(singleAssessmentSchema),
})

```

## 📄 src/schemas/index.js
*Lines: 25, Size: 1.02 KB*

```javascript
// packages/ai-services/src/schemas/index.js (version 2.1)
export * from './articleAssessmentSchema.js'
export * from './articlePreAssessmentSchema.js'
export * from './batchArticleAssessmentSchema.js'
export * from './canonicalizerSchema.js'
export * from './clusterSchema.js'
export * from './disambiguationSchema.js'
export * from './emailIntroSchema.js'
export * from './emailSubjectSchema.js'
export * from './enrichContactSchema.js'
export * from './entitySchema.js'
export * from './executiveSummarySchema.js'
export * from './findContactSchema.js'
export * from './headlineAssessmentSchema.js'
export * from './judgeSchema.js'
export * from './opportunitySchema.js'
export * from './selectorRepairSchema.js'
export * from './synthesisSchema.js'
export * from './watchlistSuggestionSchema.js'
export * from './sectionClassifierSchema.js'
export * from './batchHeadlineAssessmentSchema.js'
export * from './translateSchema.js'
// DEFINITIVE FIX: Add the missing export for the new schema.
export * from './countryCorrectionSchema.js'

```

## 📄 src/schemas/judgeSchema.js
*Lines: 14, Size: 404 Bytes*

```javascript
// packages/ai-services/src/schemas/judgeSchema.js (version 1.0)
import { z } from 'zod'

const verdictSchema = z.object({
  identifier: z.string(),
  quality: z.enum(['Excellent', 'Good', 'Acceptable', 'Marginal', 'Poor', 'Irrelevant']),
  commentary: z.string(),
})

export const judgeSchema = z.object({
  event_judgements: z.array(verdictSchema),
  opportunity_judgements: z.array(verdictSchema),
})

```

## 📄 src/schemas/opportunitySchema.js
*Lines: 22, Size: 929 Bytes*

```javascript
// packages/ai-services/src/schemas/opportunitySchema.js (version 2.1)
import { z } from 'zod'

export const opportunitySchema = z.object({
  opportunities: z.array(
    // DEFINITIVE FIX: Use .passthrough() to allow the AI to include extra fields
    // without causing a validation error. We will only use the fields we define.
    z.object({
      reachOutTo: z.string().describe("The full name of the individual or family to contact."),
      contactDetails: z.object({
        email: z.string().email().nullable(),
        role: z.string().nullable(),
        company: z.string().nullable(),
      }),
      basedIn: z.string().nullable(),
      whyContact: z.array(z.string()).describe("An array of concise, one-sentence reasons for contact."),
      likelyMMDollarWealth: z.number().nullable(),
      event_key: z.string().describe("The unique key of the source event for this opportunity."),
    }).passthrough()
  ),
})

```

## 📄 src/schemas/sectionClassifierSchema.js
*Lines: 14, Size: 411 Bytes*

```javascript
// packages/ai-services/src/schemas/sectionClassifierSchema.js (version 1.0)
import { z } from 'zod'

export const sectionClassifierSchema = z.object({
  classifications: z.array(
    z.object({
      classification: z.enum(['news_section', 'article_headline', 'navigation', 'other']),
      reasoning: z
        .string()
        .describe('A brief explanation for the classification choice.'),
    })
  ),
})

```

## 📄 src/schemas/selectorRepairSchema.js
*Lines: 13, Size: 415 Bytes*

```javascript
// packages/ai-services/src/schemas/selectorRepairSchema.js (version 1.0)
import { z } from 'zod'

export const selectorRepairSchema = z.object({
  reasoning: z.string(),
  suggested_selectors: z.object({
    headlineSelector: z.string().optional(),
    linkSelector: z.string().optional().nullable(),
    headlineTextSelector: z.string().optional().nullable(),
    articleSelector: z.string().optional(),
  }),
})

```

## 📄 src/schemas/synthesisSchema.js
*Lines: 27, Size: 819 Bytes*

```javascript
// packages/ai-services/src/schemas/synthesisSchema.js (version 1.1)
import { z } from 'zod'

export const synthesisSchema = z.object({
  events: z.array(
    z.object({
      headline: z.string().min(1),
      summary: z.string().min(1),
      advisor_summary: z
        .string()
        .min(1)
        .describe('The one-sentence actionable summary for wealth advisors.'),
      // DEFINITIVE FIX: Add the new classification field to the schema.
      eventClassification: z.string().min(1).describe("The event's classification type."),
      country: z.string().min(1),
      key_individuals: z.array(
        z.object({
          name: z.string(),
          role_in_event: z.string(),
          company: z.string().nullable(),
          email_suggestion: z.string().nullable(),
        })
      ),
    })
  ),
})

```

## 📄 src/schemas/translateSchema.js
*Lines: 7, Size: 179 Bytes*

```javascript
// packages/ai-services/src/schemas/translateSchema.js (version 1.0.0)
import { z } from 'zod'

export const translateSchema = z.object({
  translated_html: z.string().min(1),
})

```

## 📄 src/schemas/watchlistSuggestionSchema.js
*Lines: 16, Size: 496 Bytes*

```javascript
// packages/ai-services/src/schemas/watchlistSuggestionSchema.js (version 2.0.0 - With Search Terms)
import { z } from 'zod'

export const watchlistSuggestionSchema = z.object({
  suggestions: z.array(
    z.object({
      name: z.string(),
      type: z.enum(['person', 'family', 'company']),
      country: z.string(),
      rationale: z.string(),
      sourceEvent: z.string(),
      searchTerms: z.array(z.string()).describe("An array of 2-4 unique, lowercase search terms."),
    })
  ),
})

```

## 📄 src/search/search.js
*Lines: 86, Size: 3.1 KB*

```javascript
'use server'
import axios from 'axios'
import NewsAPI from 'newsapi'
import { env } from '@headlines/config'
import { logger, apiCallTracker } from '@headlines/utils-server'

const { SERPER_API_KEY, NEWSAPI_API_KEY } = env
const serperClient = SERPER_API_KEY
  ? axios.create({
      baseURL: 'https://google.serper.dev',
      headers: { 'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json' },
    })
  : null
const newsapi = NEWSAPI_API_KEY ? new NewsAPI(NEWSAPI_API_KEY) : null

if (!serperClient)
  logger.warn(
    'SERPER_API_KEY not found. Google Search dependent functions will be disabled.'
  )
if (!newsapi)
  logger.warn('NEWSAPI_API_KEY not found. NewsAPI dependent functions will be disabled.')

async function withRetry(apiCall, serviceName, maxRetries = 2) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await apiCall()
    } catch (error) {
      const isRetryable = error.response && error.response.status >= 500
      if (!isRetryable || attempt === maxRetries) {
        logger.error(
          { err: error?.response?.data || error },
          `${serviceName} search failed.`
        )
        return { success: false, error: error.message, results: [] }
      }
      const delay = 1000 * Math.pow(2, attempt - 1)
      logger.warn(`[${serviceName}] Attempt ${attempt} failed. Retrying in ${delay}ms...`)
      await new Promise((res) => setTimeout(res, delay))
    }
  }
}

export async function findAlternativeSources(headline) {
  if (!serperClient) return { success: false, results: [] }
  return withRetry(async () => {
    apiCallTracker.recordCall('serper_news')
    const response = await serperClient.post('/news', { q: headline })
    return { success: true, results: response.data.news || [] }
  }, 'Serper News')
}
export async function performGoogleSearch(query) {
  if (!serperClient) return { success: false, snippets: 'SERPER_API_KEY not configured.' }
  return withRetry(async () => {
    apiCallTracker.recordCall('serper_search')
    const response = await serperClient.post('/search', { q: query })
    const organicResults = response.data.organic || []
    if (organicResults.length > 0) {
      const snippets = organicResults
        .slice(0, 5)
        .map((res) => `- ${res.title}: ${res.snippet}`)
        .join('\n')
      return { success: true, snippets }
    }
    return { success: false, snippets: 'No search results found.' }
  }, 'Serper Search')
}
export async function findNewsApiArticlesForEvent(headline) {
  if (!newsapi) return { success: false, snippets: 'NewsAPI key not configured.' }
  return withRetry(async () => {
    apiCallTracker.recordCall('newsapi_search')
    const response = await newsapi.v2.everything({
      q: `"${headline}"`,
      pageSize: 5,
      sortBy: 'relevancy',
      language: 'en,da,sv,no',
    })
    if (response.articles && response.articles.length > 0) {
      const snippets = response.articles
        .map((a) => `- ${a.title} (${a.source.name}): ${a.description || ''}`)
        .join('\n')
      return { success: true, snippets }
    }
    return { success: false, snippets: 'No related articles found.' }
  }, 'NewsAPI')
}

```

## 📄 src/search/serpapi.js
*Lines: 63, Size: 1.78 KB*

```javascript
// File: packages/ai-services/src/search/serpapi.js (Unabridged)

'use server'

import { getJson } from 'serpapi'
import { env } from '@headlines/config'

const searchCache = new Map()
const CACHE_TTL = 1000 * 60 * 60 // 1 hour

export async function getGoogleSearchResults(query) {
  if (!env.SERPAPI_API_KEY) {
    console.warn('[SerpAPI] SERPAPI_API_KEY is not configured. Skipping web search.')
    return { success: true, results: [] }
  }

  if (!query) {
    return { success: false, error: 'Query is required.' }
  }

  const cacheKey = `serpapi_${query.toLowerCase().trim()}`
  if (searchCache.has(cacheKey)) {
    const cached = searchCache.get(cacheKey)
    if (Date.now() - cached.timestamp < CACHE_TTL) {
      console.log(`[SerpAPI Cache] Hit for query: "${query}"`)
      return cached.data
    }
  }

  console.log(`[SerpAPI] Performing live search for: "${query}"`)

  try {
    const response = await getJson({
      api_key: env.SERPAPI_API_KEY,
      engine: 'google',
      q: query,
      location: 'United States',
      gl: 'us',
      hl: 'en',
    })

    const organicResults = response.organic_results || []
    const answerBox = response.answer_box ? [response.answer_box] : []

    const formattedResults = [...answerBox, ...organicResults]
      .map((item) => ({
        title: item.title,
        link: item.link,
        snippet: item.snippet || item.answer || item.result,
        source: 'Google Search',
      }))
      .filter((item) => item.snippet)
      .slice(0, 5)

    const result = { success: true, results: formattedResults }
    searchCache.set(cacheKey, { data: result, timestamp: Date.now() })
    return result
  } catch (error) {
    console.error('[SerpAPI Error]', error)
    return { success: false, error: `Failed to fetch search results: ${error.message}` }
  }
}

```

## 📄 src/search/wikipedia.js
*Lines: 154, Size: 4.98 KB*

```javascript
// File: packages/ai-services/src/search/wikipedia.js (Unabridged and Corrected)

'use server'
import { logger, apiCallTracker } from '@headlines/utils-server'
import { settings } from '@headlines/config'
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '../schemas/index.js'
import { instructionDisambiguation } from '@headlines/prompts'

const WIKI_API_ENDPOINT = 'https://en.wikipedia.org/w/api.php'
const WIKI_SUMMARY_LENGTH = 750

// --- START: DEFINITIVE FIX FOR WIKIPEDIA CHAIN ---
const systemPrompt = [
  instructionDisambiguation.whoYouAre,
  instructionDisambiguation.whatYouDo,
  ...instructionDisambiguation.guidelines,
  instructionDisambiguation.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{inputText}'],
])

// Revert to the simpler, more direct chain. Our safeInvoke function will handle errors.
const disambiguationChain = RunnableSequence.from([
  prompt,
  getUtilityModel(),
  new JsonOutputParser(),
])
// --- END: DEFINITIVE FIX ---

async function fetchWithRetry(url, options, maxRetries = 2) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url, options)
      if (!response.ok) throw new Error(`API returned status ${response.status}`)
      return response
    } catch (error) {
      if (attempt === maxRetries) throw error
      const delay = 1000 * Math.pow(2, attempt - 1)
      logger.warn(
        `[Wikipedia Fetch] Attempt ${attempt} failed for ${url}. Retrying in ${delay}ms...`
      )
      await new Promise((res) => setTimeout(res, delay))
    }
  }
}

export async function fetchWikipediaSummary(query) {
  if (!query) return { success: false, error: 'Query cannot be empty.' }
  try {
    apiCallTracker.recordCall('wikipedia')
    const searchParams = new URLSearchParams({
      action: 'query',
      list: 'search',
      srsearch: query,
      srlimit: '5',
      format: 'json',
    })
    const searchResponse = await fetchWithRetry(
      `${WIKI_API_ENDPOINT}?${searchParams.toString()}`
    )
    const searchData = await searchResponse.json()
    const searchResults = searchData.query.search
    if (!searchResults || searchResults.length === 0)
      throw new Error(`No search results for "${query}".`)

    let best_title = null
    try {
      const userContent = `Original Query: "${query}"\n\nSearch Results:\n${JSON.stringify(searchResults.map((r) => ({ title: r.title, snippet: r.snippet })))}`

      const disambiguationResponse = await safeInvoke(
        disambiguationChain,
        { inputText: userContent },
        'disambiguationChain',
        disambiguationSchema
      )

      if (
        disambiguationResponse &&
        !disambiguationResponse.error &&
        disambiguationResponse.best_title
      ) {
        best_title = disambiguationResponse.best_title
      }
    } catch (e) {
      logger.warn({ err: e }, `Disambiguation chain failed for query "${query}".`)
    }

    if (!best_title) {
      best_title = searchResults[0].title
      logger.info(
        `[Wikipedia] AI disambiguation failed or returned null. Falling back to top search result: "${best_title}"`
      )
    }

    const summaryParams = new URLSearchParams({
      action: 'query',
      prop: 'extracts',
      exintro: 'true',
      explaintext: 'true',
      titles: best_title,
      format: 'json',
      redirects: '1',
    })
    const summaryResponse = await fetchWithRetry(
      `${WIKI_API_ENDPOINT}?${summaryParams.toString()}`
    )
    const summaryData = await summaryResponse.json()
    const pages = summaryData.query.pages
    const pageId = Object.keys(pages)[0]
    const summary = pages[pageId]?.extract
    if (!summary) throw new Error(`Could not extract summary for page "${best_title}".`)

    const conciseSummary =
      summary.length > WIKI_SUMMARY_LENGTH
        ? summary.substring(0, WIKI_SUMMARY_LENGTH) + '...'
        : summary
    return { success: true, summary: conciseSummary, title: best_title, query }
  } catch (error) {
    logger.warn(`Wikipedia lookup for "${query}" failed: ${error.message}`)
    return { success: false, error: error.message, query }
  }
}

export async function fetchBatchWikipediaSummaries(queries) {
  const promises = queries.map((q) => fetchWikipediaSummary(q))
  return Promise.all(promises)
}

export async function validateWikipediaContent(text) {
  const lowerText = text.toLowerCase()
  const isDisambiguation =
    lowerText.includes('may refer to:') || lowerText.includes('is a list of')
  if (isDisambiguation) {
    return {
      valid: false,
      quality: 'low',
      reason: 'Disambiguation page content detected.',
    }
  }
  return {
    valid: true,
    quality: 'high',
    reason: 'Content appears to be a valid summary.',
  }
}

```
