# ðŸ“ PROJECT DIRECTORY STRUCTURE

Total: 58 files, 10 directories

```
headlines-monorepo/
â”œâ”€â”€ ðŸ“ src/
â”‚   â”œâ”€â”€ ðŸ“ chains/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ articleChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ batchHeadlineChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ clusteringChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ contactFinderChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ contactResolverChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ countryCorrectionChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ disambiguationChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ dossierUpdateChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emailIntroChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ emailSubjectChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ entityCanonicalizerChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ entityExtractorChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ executiveSummaryChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ graphUpdaterChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ headlineChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ index.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ judgeChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ oppFactoryChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ opportunityChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ outreachDraftChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ sectionClassifierChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ selectorRepairChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ synthesisChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ translateChain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ watchlistSuggestionChain.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ wealthPredictorChain.js
â”‚   â”œâ”€â”€ ðŸ“ embeddings/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ embeddings.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ vectorSearch.js
â”‚   â”œâ”€â”€ ðŸ“ lib/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ AIAgent.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ langchain.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ promptBuilder.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ safeInvoke.js
â”‚   â”œâ”€â”€ ðŸ“ node/
â”‚   â”‚   â””â”€â”€ ðŸ“ agents/
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ articleAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ batchArticleAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ clusteringAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ headlineAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ judgeAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ sectionClassifierAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ selectorRepairAgent.js
â”‚   â”‚       â””â”€â”€ ðŸ“„ watchlistAgent.js
â”‚   â”œâ”€â”€ ðŸ“ rag/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ generation.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ orchestrator.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ planner.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ prompts.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ retrieval.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ validation.js
â”‚   â”œâ”€â”€ ðŸ“ search/
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ search.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ serpapi.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ wikipedia.js
â”‚   â”œâ”€â”€ ðŸ“ shared/
â”‚   â”‚   â””â”€â”€ ðŸ“ agents/
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ contactAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ emailAgents.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ entityAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ executiveSummaryAgent.js
â”‚   â”‚       â”œâ”€â”€ ðŸ“„ opportunityAgent.js
â”‚   â”‚       â””â”€â”€ ðŸ“„ synthesisAgent.js
â”‚   â”œâ”€â”€ ðŸ“„ index.js
â”‚   â””â”€â”€ ðŸ“„ next.js
â””â”€â”€ ðŸ“„ package.json
```

# ðŸ“‹ PROJECT METADATA

**Generated**: 2025-10-21T18:41:06.979Z
**Repository Path**: /home/mark/Repos/projects/headlines-monorepo/packages/ai-services
**Total Files**: 58
**Package**: @headlines/ai-services@1.0.0
**Description**: Centralized, LangChain-powered AI and external service logic.



---


## ðŸ“„ package.json
*Lines: 36, Size: 979 Bytes*

```json
{
  "name": "@headlines/ai-services",
  "version": "1.0.0",
  "description": "Centralized, LangChain-powered AI and external service logic.",
  "main": "src/index.js",
  "type": "module",
  "license": "ISC",
  "exports": {
    ".": "./src/index.js",
    "./node": "./src/index.js",
    "./next": "./src/next.js"
  },
  "dependencies": {
    "@headlines/config": "workspace:*",
    "@headlines/models": "workspace:*",
    "@headlines/prompts": "workspace:*",
    "@headlines/utils-server": "workspace:*",
    "@headlines/utils-shared": "workspace:*",
    "@langchain/community": "^0.3.57",
    "@langchain/core": "*",
    "@langchain/openai": "*",
    "@langchain/pinecone": "*",
    "@pinecone-database/pinecone": "^2.2.2",
    "@xenova/transformers": "^2.17.2",
    "axios": "^1.7.2",
    "langchain": "*",
    "mongoose": "^8.19.0",
    "newsapi": "^2.4.1",
    "openai": "^5.22.0",
    "p-limit": "^5.0.0",
    "serpapi": "^2.1.0",
    "sharp": "0.33.4",
    "zod": "*"
  }
}

```

## ðŸ“„ src/chains/articleChain.js
*Lines: 55, Size: 1.86 KB*

```javascript
// packages/ai-services/src/chains/articleChain.js
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { RunnableSequence } from '@langchain/core/runnables'
import { settings } from '@headlines/config'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { logger } from '@headlines/utils-shared'
import { articleAssessmentSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(getInstructionArticle)

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputArticle.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputArticle[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{article_text}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
const chain = RunnableSequence.from([prompt, getHighPowerModel()])

async function invoke(input) {
  const result = await safeInvoke(chain, input, 'articleChain', articleAssessmentSchema)
  if (result.error) return result
  if (result.key_individuals?.length > 0) {
    const articleTextLower = input.article_text.toLowerCase()
    result.key_individuals = result.key_individuals.filter((ind) => {
      if (!ind.name) return false
      const isPresent = ind.name
        .split(' ')
        .filter((p) => p.length > 2)
        .some((p) => articleTextLower.includes(p.toLowerCase()))
      if (!isPresent)
        logger.warn({ individual: ind.name }, 'Discarding hallucinated key individual.')
      return isPresent
    })
  }
  return result
}

export const articleChain = { invoke }

```

## ðŸ“„ src/chains/batchHeadlineChain.js
*Lines: 21, Size: 677 Bytes*

```javascript
// packages/ai-services/src/chains/batchHeadlineChain.js
import { AIAgent } from '../lib/AIAgent.js'
import { batchHeadlineAssessmentSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config'
import { instructionBatchHeadlineAssessment } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_HEADLINE_ASSESSMENT,
    systemPrompt: instructionBatchHeadlineAssessment,
    zodSchema: batchHeadlineAssessmentSchema,
  })

async function invoke(input) {
  const agent = getAgent()
  const result = await agent.execute(input.headlines_json_string)
  return result
}

export const batchHeadlineChain = { invoke }

```

## ðŸ“„ src/chains/clusteringChain.js
*Lines: 35, Size: 1.03 KB*

```javascript
// packages/ai-services/src/chains/clusteringChain.js
import { z } from 'zod'
import { AIAgent } from '../lib/AIAgent.js'
import { instructionCluster } from '@headlines/prompts'
import { settings } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

// This schema correctly matches the detailed prompt's output requirement.
const clusterSchema = z.object({
  events: z.array(
    z.object({
      event_key: z.string(),
      article_ids: z.array(z.string()),
    })
  ),
})

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS, // Clustering is a complex task
    systemPrompt: instructionCluster,
    zodSchema: clusterSchema,
  })

async function invoke(input) {
  const agent = getAgent()
  const result = await agent.execute(input.articles_json_string)

  // The AIAgent's execute method already handles retries, validation, and error logging.
  // The output will either be the successfully validated data or an object with an `error` key.
  return result
}

export const clusteringChain = { invoke }

```

## ðŸ“„ src/chains/contactFinderChain.js
*Lines: 22, Size: 816 Bytes*

```javascript
// packages/ai-services/src/chains/contactFinderChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionContacts } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { findContactSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionContacts)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{snippets}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const contactFinderChain = {
  invoke: (input) => safeInvoke(chain, input, 'contactFinderChain', findContactSchema),
}

```

## ðŸ“„ src/chains/contactResolverChain.js
*Lines: 23, Size: 839 Bytes*

```javascript
// packages/ai-services/src/chains/contactResolverChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEnrichContact } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { enrichContactSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionEnrichContact)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const contactResolverChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'contactResolverChain', enrichContactSchema),
}

```

## ðŸ“„ src/chains/countryCorrectionChain.js
*Lines: 40, Size: 1.99 KB*

```javascript
// packages/ai-services/src/chains/countryCorrectionChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { countryCorrectionSchema } from '@headlines/models/schemas'

const systemPrompt = `You are a data cleaning expert. Your sole task is to analyze a given text string that is supposed to represent a country and extract the single, correct, UN-recognized sovereign country name from it.

**CRITICAL INSTRUCTIONS:**
1.  Analyze the input string.
2.  Identify the most likely country. For example, "Denmark (Aarhus)" should be "Denmark". "London" should be "United Kingdom".
4.  Anything starting with "Central Europe" should be "Europe".
5.  "Denmark & Sweden" should be "Scandinavia"
6.  "International" should be "Global"
7. "Nordic Region" should be "Scandinavia" (also if followed by something between brackets)
8. "Pan-Europe" should be "Europe"
9. "Sweden & Norway" should be "Scandinavia"
10. "United States" should be "United States of America"
11. "UK" should be "United Kingdom"
12. anything starting with "Unknown" should simply be "Unknown"
13.  If a valid country name can be determined, return it.
14.  If the input is ambiguous or does not contain a clear country, you MUST return null.
15.  You MUST respond ONLY with a valid JSON object in this format: {{"country": "Correct Country Name"}} or {{"country": null}}`

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Location String: "{location_string}"'],
])

// --- DEFINITIVE FIX ---
// The chain now ends with the model. The JsonOutputParser has been removed.
// The new safeInvoke function will handle the parsing robustly.
const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const countryCorrectionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'countryCorrectionChain', countryCorrectionSchema),
}

```

## ðŸ“„ src/chains/disambiguationChain.js
*Lines: 25, Size: 1001 Bytes*

```javascript
// packages/ai-services/src/chains/disambiguationChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionDisambiguation } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

// The robust buildPrompt helper handles the construction of the system prompt,
// preventing errors if any properties on the instruction object are not iterable.
const systemPrompt = buildPrompt(instructionDisambiguation)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{inputText}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const disambiguationChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'disambiguationChain', disambiguationSchema),
}

```

## ðŸ“„ src/chains/dossierUpdateChain.js
*Lines: 45, Size: 1.49 KB*

```javascript
// packages/ai-services/src/chains/dossierUpdateChain.js
import { AIAgent } from '../lib/AIAgent.js'
import { instructionDossierUpdate } from '@headlines/prompts'
import { opportunitySchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

// --- START OF DEFINITIVE FIX ---
// The previous agent was too slow and complex, causing timeouts.
// This new version uses a faster model and a simplified task (re-generation instead of merging)
// to ensure reliability and speed.
const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS, // Use the faster, more reliable synthesis model
    systemPrompt: instructionDossierUpdate,
    zodSchema: opportunitySchema,
  })
// --- END OF DEFINITIVE FIX ---

async function invoke(input) {
  const agent = getAgent()
  const userContent = `Existing Dossier (JSON):\n\`\`\`${input.existing_dossier_json}\`\`\`\n\nNew Intelligence Brief (Text):\n\`\`\`${input.new_intelligence_text}\`\`\``
  const result = await agent.execute(userContent)

  if (result.error) {
    return result
  }

  if (
    Array.isArray(result.opportunities) &&
    result.opportunities.length > 0 &&
    Array.isArray(result.opportunities[0])
  ) {
    logger.warn(
      { agent: 'dossierUpdateChain' },
      'Detected nested array in opportunities output. Flattening to correct.'
    )
    result.opportunities = result.opportunities.flat()
  }

  return result
}

export const dossierUpdateChain = { invoke }

```

## ðŸ“„ src/chains/emailIntroChain.js
*Lines: 22, Size: 843 Bytes*

```javascript
// packages/ai-services/src/chains/emailIntroChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailIntro } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailIntroSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionEmailIntro)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Client and Event Data: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const emailIntroChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailIntroChain', emailIntroSchema),
}

```

## ðŸ“„ src/chains/emailSubjectChain.js
*Lines: 22, Size: 846 Bytes*

```javascript
// packages/ai-services/src/chains/emailSubjectChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailSubject } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailSubjectSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionEmailSubject)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const emailSubjectChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailSubjectChain', emailSubjectSchema),
}

```

## ðŸ“„ src/chains/entityCanonicalizerChain.js
*Lines: 23, Size: 851 Bytes*

```javascript
// packages/ai-services/src/chains/entityCanonicalizerChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCanonicalizer } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { canonicalizerSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionCanonicalizer)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{entity_name}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const entityCanonicalizerChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'entityCanonicalizerChain', canonicalizerSchema),
}

```

## ðŸ“„ src/chains/entityExtractorChain.js
*Lines: 22, Size: 808 Bytes*

```javascript
// packages/ai-services/src/chains/entityExtractorChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEntity } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { entitySchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionEntity)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{article_text}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const entityExtractorChain = {
  invoke: (input) => safeInvoke(chain, input, 'entityExtractorChain', entitySchema),
}

```

## ðŸ“„ src/chains/executiveSummaryChain.js
*Lines: 23, Size: 876 Bytes*

```javascript
// packages/ai-services/src/chains/executiveSummaryChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionExecutiveSummary } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { executiveSummarySchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionExecutiveSummary)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Run Data: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const executiveSummaryChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'executiveSummaryChain', executiveSummarySchema),
}

```

## ðŸ“„ src/chains/graphUpdaterChain.js
*Lines: 37, Size: 1.02 KB*

```javascript
// packages/ai-services/src/chains/graphUpdaterChain.js
import { z } from 'zod'
import { AIAgent } from '../lib/AIAgent.js'
import { instructionGraphUpdater } from '@headlines/prompts'
import { settings } from '@headlines/config'

const graphUpdaterSchema = z.object({
  entities: z
    .array(z.string())
    .describe('An array of all unique canonical entity names found.'),
  relationships: z
    .array(
      z.tuple([
        z.string(), // Subject
        z.string(), // Predicate (Relationship Type)
        z.string(), // Object
      ])
    )
    .describe('An array of Subject-Predicate-Object triples representing relationships.'),
})

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionGraphUpdater,
    zodSchema: graphUpdaterSchema,
  })

async function invoke(input) {
  const agent = getAgent()
  // The userContent is the synthesized summary of an event
  const result = await agent.execute(input.event_summary)
  return result
}

export const graphUpdaterChain = { invoke }

```

## ðŸ“„ src/chains/headlineChain.js
*Lines: 66, Size: 2.02 KB*

```javascript
// packages/ai-services/src/chains/headlineChain.js
import { AIAgent } from '../lib/AIAgent.js'
import { headlineAssessmentSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_HEADLINE_ASSESSMENT,
    systemPrompt: instructionHeadlines,
    fewShotInputs: shotsInputHeadlines,
    fewShotOutputs: shotsOutputHeadlines,
    zodSchema: headlineAssessmentSchema,
  })

function prepareInput({ article, hits }) {
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`
  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }
  return { headlineWithContext }
}

async function invoke({ article, hits }) {
  const agent = getAgent()
  const { headlineWithContext } = prepareInput({ article, hits })

  // The agent expects a single string of user content
  const response = await agent.execute(headlineWithContext)

  // Default response in case of total failure
  const fallbackAssessment = {
    relevance_headline: 0,
    assessment_headline: 'AI assessment failed.',
    headline_en: article.headline,
  }

  if (response.error || !response.assessment || response.assessment.length === 0) {
    return fallbackAssessment
  }

  // The schema ensures assessment is an array, but we only sent one headline
  const assessment = response.assessment[0]

  if (hits.length > 0) {
    const boost = settings.WATCHLIST_SCORE_BOOST
    if (boost > 0) {
      assessment.relevance_headline = Math.min(100, assessment.relevance_headline + boost)
      assessment.assessment_headline = `Watchlist boost (+${boost}). ${assessment.assessment_headline}`
    }
  }

  return assessment || fallbackAssessment
}

export const headlineChain = { invoke }

```

## ðŸ“„ src/chains/index.js
*Lines: 53, Size: 3.37 KB*

```javascript
// packages/ai-services/src/chains/index.js
import { articleChain as ac } from './articleChain.js'
import { clusteringChain as cc } from './clusteringChain.js'
import { contactFinderChain as cfc } from './contactFinderChain.js'
import { contactResolverChain as crc } from './contactResolverChain.js'
import { disambiguationChain as dc } from './disambiguationChain.js'
import { dossierUpdateChain as duc } from './dossierUpdateChain.js'
import { emailIntroChain as eic } from './emailIntroChain.js'
import { emailSubjectChain as esc } from './emailSubjectChain.js'
import { entityCanonicalizerChain as ecc } from './entityCanonicalizerChain.js'
import { entityExtractorChain as eec } from './entityExtractorChain.js'
import { executiveSummaryChain as exsc } from './executiveSummaryChain.js'
import { graphUpdaterChain as guc } from './graphUpdaterChain.js' // ADDED
import { headlineChain as hc } from './headlineChain.js'
import { judgeChain as jc } from './judgeChain.js'
import { opportunityChain as oc } from './opportunityChain.js'
import { oppFactoryChain as ofc } from './oppFactoryChain.js'
import { outreachDraftChain as odc } from './outreachDraftChain.js'
import { sectionClassifierChain as scc } from './sectionClassifierChain.js'
import { selectorRepairChain as src } from './selectorRepairChain.js'
import { synthesisChain as sc } from './synthesisChain.js'
import { watchlistSuggestionChain as wsc } from './watchlistSuggestionChain.js'
import { wealthPredictorChain as wpc } from './wealthPredictorChain.js'
import { batchHeadlineChain as bhc } from './batchHeadlineChain.js'
import { translateChain as tc } from './translateChain.js'
import { countryCorrectionChain as ccc } from './countryCorrectionChain.js'

export const articleChain = async (input) => ac.invoke(input)
export const clusteringChain = async (input) => cc.invoke(input)
export const contactFinderChain = async (input) => cfc.invoke(input)
export const contactResolverChain = async (input) => crc.invoke(input)
export const disambiguationChain = async (input) => dc.invoke(input)
export const dossierUpdateChain = async (input) => duc.invoke(input)
export const emailIntroChain = async (input) => eic.invoke(input)
export const emailSubjectChain = async (input) => esc.invoke(input)
export const entityCanonicalizerChain = async (input) => ecc.invoke(input)
export const entityExtractorChain = async (input) => eec.invoke(input)
export const executiveSummaryChain = async (input) => exsc.invoke(input)
export const graphUpdaterChain = async (input) => guc.invoke(input) // ADDED
export const headlineChain = async (input) => hc.invoke(input)
export const judgeChain = async (input) => jc.invoke(input)
export const oppFactoryChain = async (input) => ofc.invoke(input)
export const opportunityChain = async (input) => oc.invoke(input)
export const outreachDraftChain = async (input) => odc.invoke(input)
export const sectionClassifierChain = async (input) => scc.invoke(input)
export const selectorRepairChain = async (input) => src.invoke(input)
export const synthesisChain = async (input) => sc.invoke(input)
export const watchlistSuggestionChain = async (input) => wsc.invoke(input)
export const wealthPredictorChain = async (input) => wpc.invoke(input)
export const batchHeadlineChain = async (input) => bhc.invoke(input)
export const translateChain = async (input) => tc.invoke(input)
export const countryCorrectionChain = async (input) => ccc.invoke(input)

```

## ðŸ“„ src/chains/judgeChain.js
*Lines: 22, Size: 802 Bytes*

```javascript
// packages/ai-services/src/chains/judgeChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionJudge } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { judgeSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionJudge)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Data for review: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const judgeChain = {
  invoke: (input) => safeInvoke(chain, input, 'judgeChain', judgeSchema),
}

```

## ðŸ“„ src/chains/oppFactoryChain.js
*Lines: 41, Size: 1.33 KB*

```javascript
// packages/ai-services/src/chains/oppFactoryChain.js
import { AIAgent } from '../lib/AIAgent.js'
import { instructionOppFactory } from '@headlines/prompts'
import { settings } from '@headlines/config'
import { opportunitySchema } from '@headlines/models/schemas' // This is the rich, unified schema

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_PRO,
    systemPrompt: instructionOppFactory,
    zodSchema: opportunitySchema,
  })

async function invoke(input) {
  const agent = getAgent()
  const userContent = `Target Name: ${input.name}\n\n--- Scraped Articles ---\n${input.articles_text}`
  const result = await agent.execute(userContent)

  // --- START OF HARDENING ---
  // If the AI successfully generates a dossier but fails to provide a 'whyContact' reason,
  // inject a generic fallback to prevent Zod validation failure.
  if (
    result &&
    !result.error &&
    result.opportunities &&
    result.opportunities.length > 0
  ) {
    const opp = result.opportunities[0]
    if (!opp.whyContact || opp.whyContact.length === 0) {
      opp.whyContact = [
        `Identified as a high-value individual (${input.name}) based on recent intelligence signals warranting further research and potential outreach.`,
      ]
    }
  }
  // --- END OF HARDENING ---

  return result
}

export const oppFactoryChain = { invoke }

```

## ðŸ“„ src/chains/opportunityChain.js
*Lines: 77, Size: 2.73 KB*

```javascript
// packages/ai-services/src/chains/opportunityChain.js
import { z } from 'zod'
import { AIAgent } from '../lib/AIAgent.js'
import { settings } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

// --- START OF MODIFICATION ---
// A new, leaner schema and prompt specifically for the fast, in-pipeline chain.
const simpleOpportunitySchema = z.object({
  opportunities: z.array(
    z
      .object({
        reachOutTo: z.string(),
        contactDetails: z
          .union([
            z.string(),
            z.object({
              role: z.string().nullable(),
              company: z.string().nullable(),
            }),
          ])
          .transform((val) => {
            if (typeof val === 'string') {
              return { role: val, company: null }
            }
            return val
          })
          .default({}),
        lastKnownEventLiquidityMM: z.number().nullable(),
        whyContact: z
          .union([z.string(), z.array(z.string())])
          .transform((val) => (Array.isArray(val) ? val : [val])),
        event_key: z.string().optional(),
      })
      .passthrough()
  ),
})

const simpleInstruction = {
  whoYouAre: `You are a high-speed intelligence extraction engine. Your task is to extract critical, Tier-1 data points about individuals from a text.`,
  whatYouDo: `From the provided text, extract ONLY the following for each relevant individual: their full name, their role/company, the liquidity from this event, and a reason to contact them. Be fast and precise. Ignore biographical details.`,
  outputFormatDescription: `Respond ONLY with a valid JSON object with a single key "opportunities", which is an array of objects containing ONLY 'reachOutTo', 'contactDetails', 'lastKnownEventLiquidityMM', and 'whyContact'. The 'whyContact' field MUST be an array of strings.`,
}

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS, // Use a powerful but fast model
    systemPrompt: simpleInstruction,
    zodSchema: simpleOpportunitySchema,
  })

async function invoke(input) {
  const agent = getAgent()
  // We no longer need to summarize; we just send the context to the simpler agent.
  const userContent = `New Intelligence Brief (Text):\n\`\`\`${input.context_text}\`\`\``
  const result = await agent.execute(userContent)

  if (result.error) {
    return result
  }

  // Post-processing to ensure the event_key is attached, as the AI no longer handles it.
  if (result.opportunities) {
    const eventKeyMatch = input.context_text.match(/Event Key: ([\w-]+)/)
    if (eventKeyMatch) {
      result.opportunities.forEach((opp) => {
        opp.event_key = eventKeyMatch[1]
      })
    }
  }

  return result
}
// --- END OF MODIFICATION ---

export const opportunityChain = { invoke }

```

## ðŸ“„ src/chains/outreachDraftChain.js
*Lines: 28, Size: 1.01 KB*

```javascript
// packages/ai-services/src/chains/outreachDraftChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionOutreachDraft } from '@headlines/prompts'
import { getProModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { z } from 'zod'
import { buildPrompt } from '../lib/promptBuilder.js'

// Define the Zod schema for validation
const outreachDraftSchema = z.object({
  subject: z.string().min(1, 'Subject line cannot be empty.'),
  body: z.string().min(1, 'Email body cannot be empty.'),
})

const systemPrompt = buildPrompt(instructionOutreachDraft)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Intelligence Dossier (JSON):\n```{opportunity_json_string}```'],
])

const chain = RunnableSequence.from([prompt, getProModel()])

export const outreachDraftChain = {
  invoke: (input) => safeInvoke(chain, input, 'outreachDraftChain', outreachDraftSchema),
}

```

## ðŸ“„ src/chains/sectionClassifierChain.js
*Lines: 23, Size: 867 Bytes*

```javascript
// packages/ai-services/src/chains/sectionClassifierChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { sectionClassifierSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'
import { instructionSectionClassifier } from '@headlines/prompts'

const systemPrompt = buildPrompt(instructionSectionClassifier)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{links_json_string}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const sectionClassifierChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'sectionClassifierChain', sectionClassifierSchema),
}

```

## ðŸ“„ src/chains/selectorRepairChain.js
*Lines: 23, Size: 848 Bytes*

```javascript
// packages/ai-services/src/chains/selectorRepairChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSelectorRepair } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { selectorRepairSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionSelectorRepair)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const selectorRepairChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'selectorRepairChain', selectorRepairSchema),
}

```

## ðŸ“„ src/chains/synthesisChain.js
*Lines: 22, Size: 803 Bytes*

```javascript
// packages/ai-services/src/chains/synthesisChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSynthesize } from '@headlines/prompts'
import { getProModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { synthesisSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionSynthesize)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_json_string}'],
])

const chain = RunnableSequence.from([prompt, getProModel()])

export const synthesisChain = {
  invoke: (input) => safeInvoke(chain, input, 'synthesisChain', synthesisSchema),
}

```

## ðŸ“„ src/chains/translateChain.js
*Lines: 22, Size: 854 Bytes*

```javascript
// packages/ai-services/src/chains/translateChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionTranslate } from '@headlines/prompts'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { translateSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionTranslate)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Target Language: {language}\n\nHTML Content:\n```{html_content}```'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel()])

export const translateChain = {
  invoke: (input) => safeInvoke(chain, input, 'translateChain', translateSchema),
}

```

## ðŸ“„ src/chains/watchlistSuggestionChain.js
*Lines: 23, Size: 899 Bytes*

```javascript
// packages/ai-services/src/chains/watchlistSuggestionChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionWatchlistSuggestion } from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { watchlistSuggestionSchema } from '@headlines/models/schemas'
import { buildPrompt } from '../lib/promptBuilder.js'

const systemPrompt = buildPrompt(instructionWatchlistSuggestion)

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel()])

export const watchlistSuggestionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'watchlistSuggestionChain', watchlistSuggestionSchema),
}

```

## ðŸ“„ src/chains/wealthPredictorChain.js
*Lines: 22, Size: 793 Bytes*

```javascript
// packages/ai-services/src/chains/wealthPredictorChain.js
import { AIAgent } from '../lib/AIAgent.js'
import { instructionWealthPredictor } from '@headlines/prompts'
import { settings } from '@headlines/config'
import { wealthPredictorSchema } from '@headlines/models/schemas' // CORRECTED IMPORT

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY, // Fast and cheap for classification
    systemPrompt: instructionWealthPredictor,
    zodSchema: wealthPredictorSchema,
  })

async function invoke(input) {
  const agent = getAgent()
  const userContent = `Person's Name: "${input.name}"\n\nContext from article or entity graph: "${input.context}"`
  const result = await agent.execute(userContent)
  return result
}

export const wealthPredictorChain = { invoke }

```

## ðŸ“„ src/embeddings/embeddings.js
*Lines: 249, Size: 7.44 KB*

```javascript
// packages/ai-services/src/embeddings/embeddings.js
// src/lib/embeddings.js (Enhanced version with query expansion and caching)

// In-memory cache for embeddings (consider Redis for production)
const embeddingCache = new Map()
const MAX_CACHE_SIZE = 1000

// Singleton pattern to ensure we only load the model once per server instance
class EmbeddingPipeline {
  static task = 'feature-extraction'
  static model = 'Xenova/all-MiniLM-L6-v2'
  static instance = null

  static async getInstance() {
    if (this.instance === null) {
      const { pipeline } = await import('@xenova/transformers')
      this.instance = await pipeline(this.task, this.model)
    }
    return this.instance
  }
}

/**
 * Creates a cache key from text
 * @param {string} text
 * @returns {string}
 */
function createCacheKey(text) {
  return `embed_${text.toLowerCase().trim().replace(/\s+/g, '_')}`
}

/**
 * Manages cache size to prevent memory bloat
 */
function manageCacheSize() {
  if (embeddingCache.size >= MAX_CACHE_SIZE) {
    // Remove oldest 20% of entries (FIFO-ish)
    const keysToRemove = Array.from(embeddingCache.keys()).slice(
      0,
      Math.floor(MAX_CACHE_SIZE * 0.2)
    )
    keysToRemove.forEach((key) => embeddingCache.delete(key))
    console.log(`[Embedding Cache] Cleaned ${keysToRemove.length} entries`)
  }
}

/**
 * Generates an embedding for a given text with caching
 * @param {string} text The text to embed
 * @returns {Promise<Array<number>>} A promise that resolves to the embedding vector
 */
export async function generateEmbedding(text) {
  if (!text || text.trim().length === 0) {
    throw new Error('Text cannot be empty for embedding generation')
  }

  const cleanText = text.trim()
  const cacheKey = createCacheKey(cleanText)

  // Check cache first
  if (embeddingCache.has(cacheKey)) {
    console.log(`[Embedding Cache] Hit for text: "${cleanText.substring(0, 50)}..."`)
    return embeddingCache.get(cacheKey)
  }

  try {
    const extractor = await EmbeddingPipeline.getInstance()
    const output = await extractor(cleanText, { pooling: 'mean', normalize: true })
    const embedding = Array.from(output.data)

    // Cache the result
    manageCacheSize()
    embeddingCache.set(cacheKey, embedding)

    console.log(
      `[Embedding] Generated embedding for text: "${cleanText.substring(0, 50)}..." (${embedding.length} dimensions)`
    )
    return embedding
  } catch (error) {
    console.error(`[Embedding Error] Failed to generate embedding: ${error.message}`)
    throw new Error(`Failed to generate embedding: ${error.message}`)
  }
}

/**
 * Generates multiple query variations to improve RAG recall
 * @param {string} originalQuery
 * @returns {Promise<Array<Array<number>>>} Array of embeddings for different query variations
 */
export async function generateQueryEmbeddings(originalQuery) {
  const variations = generateQueryVariations(originalQuery)
  const embeddingPromises = variations.map((query) => generateEmbedding(query))

  try {
    const embeddings = await Promise.all(embeddingPromises)
    console.log(
      `[Query Expansion] Generated ${embeddings.length} query variations for: "${originalQuery}" ->`,
      variations
    )
    return embeddings
  } catch (error) {
    console.error(`[Query Expansion Error] ${error.message}`)
    // Fallback to original query only
    return [await generateEmbedding(originalQuery)]
  }
}

/**
 * Creates query variations to improve semantic search recall
 * @param {string} query
 * @returns {Array<string>}
 */
function generateQueryVariations(query) {
  const originalQuery = query.trim()
  const variations = new Set([originalQuery])

  // CORRECTED: Smartly strip disambiguation tags for broader searches
  const coreEntity = originalQuery.replace(/\s*\((company|person)\)$/, '').trim()
  if (coreEntity !== originalQuery) {
    variations.add(coreEntity)
  }

  // Pattern for "Who founded X?"
  const à¤¹à¥‚à¤‚FounderMatch = coreEntity
    .toLowerCase()
    .match(/^(?:who|what)\s+(?:is|was|founded|created)\s+(.+)/)
  if (à¤¹à¥‚à¤‚FounderMatch) {
    let subject = à¤¹à¥‚à¤‚FounderMatch[1]
      .replace(/\?/g, '')
      .replace(/^(the|a|an)\s/, '')
      .trim()
    variations.add(subject)
    variations.add(`${subject} founder`)
    variations.add(`founder of ${subject}`)
    variations.add(`${subject} history`)
  } else {
    // General question pattern
    const questionMatch = coreEntity
      .toLowerCase()
      .match(/^(who|what|when|where|why|how)\s(is|are|was|were|did|does|do)\s(.+)/)
    if (questionMatch) {
      let subject = questionMatch[3].replace(/\?/g, '').trim()
      variations.add(subject)

      const simplified = subject.replace(/^(the|a|an)\s/, '').split(' of ')
      if (simplified.length > 1) {
        variations.add(`${simplified[1].trim()} ${simplified[0].trim()}`)
      }
    }
  }

  // Add generic variations for the core entity
  if (hasProperNouns(coreEntity)) {
    variations.add(`${coreEntity} background details`)
    variations.add(`Information about ${coreEntity}`)
  }

  // Return the top 4 most distinct variations
  return Array.from(variations).slice(0, 4)
}

/**
 * Simple check for proper nouns (capitalized words not at the start of a sentence)
 * @param {string} text
 * @returns {boolean}
 */
function hasProperNouns(text) {
  // Looks for words starting with an uppercase letter
  return /\b[A-Z][a-z]+/.test(text)
}

/**
 * Batch embedding generation for efficiency
 * @param {Array<string>} texts
 * @returns {Promise<Array<Array<number>>>}
 */
export async function generateBatchEmbeddings(texts) {
  if (!texts || texts.length === 0) {
    return []
  }

  const embeddings = []
  const extractor = await EmbeddingPipeline.getInstance()

  // Process in batches to avoid memory issues
  const BATCH_SIZE = 10
  for (let i = 0; i < texts.length; i += BATCH_SIZE) {
    const batch = texts.slice(i, i + BATCH_SIZE)
    const batchPromises = batch.map((text) => {
      const cacheKey = createCacheKey(text)
      if (embeddingCache.has(cacheKey)) {
        return Promise.resolve(embeddingCache.get(cacheKey))
      }
      return extractor(text, { pooling: 'mean', normalize: true }).then((output) => {
        const embedding = Array.from(output.data)
        embeddingCache.set(cacheKey, embedding)
        return embedding
      })
    })

    const batchEmbeddings = await Promise.all(batchPromises)
    embeddings.push(...batchEmbeddings)

    console.log(
      `[Batch Embedding] Processed batch ${Math.floor(i / BATCH_SIZE) + 1}/${Math.ceil(texts.length / BATCH_SIZE)}`
    )
  }

  return embeddings
}

/**
 * Calculate cosine similarity between two embeddings
 * @param {Array<number>} embedding1
 * @param {Array<number>} embedding2
 * @returns {Promise<number>} Similarity score between 0 and 1
 */
export async function calculateSimilarity(embedding1, embedding2) {
  if (embedding1.length !== embedding2.length) {
    throw new Error('Embeddings must have the same dimensions')
  }

  let dotProduct = 0
  let norm1 = 0
  let norm2 = 0

  for (let i = 0; i < embedding1.length; i++) {
    dotProduct += embedding1[i] * embedding2[i]
    norm1 += embedding1[i] * embedding1[i]
    norm2 += embedding2[i] * embedding2[i]
  }

  if (norm1 === 0 || norm2 === 0) return 0

  return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2))
}

/**
 * Get cache statistics for monitoring
 * @returns {Promise<Object>}
 */
export async function getEmbeddingCacheStats() {
  return {
    size: embeddingCache.size,
    maxSize: MAX_CACHE_SIZE,
    utilizationPercent: Math.round((embeddingCache.size / MAX_CACHE_SIZE) * 100),
  }
}

```

## ðŸ“„ src/embeddings/vectorSearch.js
*Lines: 76, Size: 2.48 KB*

```javascript
// packages/ai-services/src/embeddings/vectorSearch.js
import { Pinecone } from '@pinecone-database/pinecone'
import { logger } from '@headlines/utils-shared'
import { generateEmbedding } from './embeddings.js'
import { env } from '@headlines/config'

const { PINECONE_API_KEY, PINECONE_INDEX_NAME } = env

const SIMILARITY_THRESHOLD = 0.65
const MAX_CONTEXT_ARTICLES = 3
const MAX_RETRIES = 2 // Add retry configuration

let pineconeIndex
if (PINECONE_API_KEY) {
  const pc = new Pinecone({ apiKey: PINECONE_API_KEY })
  pineconeIndex = pc.index(PINECONE_INDEX_NAME)
} else {
  logger.warn(
    'Pinecone API Key not found. RAG/vector search functionality will be disabled.'
  )
}

export async function findSimilarArticles(queryText) {
  if (!pineconeIndex) return []
  logger.info('RAG: Searching for historical context in Pinecone...')
  if (!queryText || typeof queryText !== 'string' || queryText.trim().length === 0)
    return []

  for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
    try {
      const queryEmbedding = await generateEmbedding(queryText)

      const queryResponse = await pineconeIndex.query({
        topK: MAX_CONTEXT_ARTICLES,
        vector: queryEmbedding,
        includeMetadata: true,
      })

      const relevantMatches = queryResponse.matches.filter(
        (match) => match.score >= SIMILARITY_THRESHOLD
      )

      if (relevantMatches.length > 0) {
        const retrievedArticlesForLogging = relevantMatches
          .map(
            (match) =>
              `  - [Score: ${match.score.toFixed(3)}] "${match.metadata.headline}"`
          )
          .join('\n')
        logger.info(
          `RAG: Found ${relevantMatches.length} relevant historical articles:\n${retrievedArticlesForLogging}`
        )
        return relevantMatches.map((match) => ({
          headline: match.metadata.headline,
          newspaper: match.metadata.newspaper,
          assessment_article: match.metadata.summary,
        }))
      } else {
        logger.info('RAG: Found no relevant historical articles in Pinecone.')
        return []
      }
    } catch (error) {
      logger.error({ err: error }, `RAG: Pinecone query attempt ${attempt} failed.`)
      if (attempt === MAX_RETRIES) {
        logger.error(
          { err: error },
          'RAG: Pinecone query or embedding generation failed after all retries.'
        )
        return []
      }
      await new Promise((res) => setTimeout(res, 1000 * attempt)) // Exponential backoff
    }
  }
  return [] // Should be unreachable
}

```

## ðŸ“„ src/index.js
*Lines: 195, Size: 6.65 KB*

```javascript
// packages/ai-services/src/index.js
import { AIAgent } from './lib/AIAgent.js'
export { AIAgent }
export * from './lib/langchain.js'
export * from './chains/index.js'
export * from './search/search.js'
export * from './search/wikipedia.js'
export * from './embeddings/embeddings.js'
export * from './embeddings/vectorSearch.js'
export * from './rag/orchestrator.js'
export * from './shared/agents/synthesisAgent.js'
export * from './shared/agents/opportunityAgent.js'
export * from './shared/agents/contactAgent.js'
export * from './shared/agents/entityAgent.js'
export * from './shared/agents/emailAgents.js'
export * from './shared/agents/executiveSummaryAgent.js'
export { dossierUpdateChain } from './chains/index.js'

export * from './node/agents/articleAgent.js'
export * from './node/agents/clusteringAgent.js'
export * from './node/agents/headlineAgent.js'
export * from './node/agents/judgeAgent.js'
export * from './node/agents/sectionClassifierAgent.js'
export * from './node/agents/selectorRepairAgent.js'
export * from './node/agents/watchlistAgent.js'

import { logger } from '@headlines/utils-shared'
import { settings } from '@headlines/config/node'
import { callLanguageModel } from './lib/langchain.js'
import { SynthesizedEvent, Opportunity, Article } from '@headlines/models/node'
import { synthesizeEvent } from './shared/agents/synthesisAgent.js'
import { generateOpportunitiesFromEvent } from './shared/agents/opportunityAgent.js'
import { instructionSourceDiscovery } from '@headlines/prompts'
import { generateEmbedding } from './embeddings/embeddings.js'
import mongoose from 'mongoose'

const TITLE_GENERATOR_PROMPT = `You are a title generation AI. Your task is to read a conversation and create a concise, 5-word-or-less title that accurately summarizes the main topic. Example Title: "Anders Holch Povlsen's Bestseller"`

export async function generateChatTitle(messages) {
  if (!messages || messages.length < 2) {
    return { success: false, error: 'Not enough messages to generate a title.' }
  }
  try {
    const conversationText = messages.map((m) => `${m.role}: ${m.content}`).join('\n')
    const title = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: TITLE_GENERATOR_PROMPT,
      userContent: conversationText,
      isJson: false,
    })
    const cleanedTitle = title.trim().replace(/"/g, '')
    return { success: true, title: cleanedTitle }
  } catch (error) {
    return { success: false, error: 'Failed to generate title.' }
  }
}

export async function processUploadedArticle(item, userId) {
  if (!userId) {
    return { success: false, error: 'Authentication required' }
  }
  try {
    const enrichedArticle = {
      ...item,
      relevance_article: 100,
      assessment_article: item.article,
      articleContent: { contents: [item.article] },
      newspaper: 'Manual Upload',
      country: ['Denmark'],
      key_individuals: [],
    }

    const synthesizedResult = await synthesizeEvent([enrichedArticle], [], '', '')
    if (
      !synthesizedResult ||
      !synthesizedResult.events ||
      synthesizedResult.events.length === 0
    ) {
      throw new Error('AI failed to synthesize an event from the provided text.')
    }
    const eventData = synthesizedResult.events[0]

    const eventToSave = new SynthesizedEvent({
      ...eventData,
      event_key: `manual-${new Date().toISOString()}`,
      highest_relevance_score: 100,
      source_articles: [
        { headline: item.headline, link: '#manual', newspaper: 'Manual Upload' },
      ],
    })

    const opportunitiesToSave = await generateOpportunitiesFromEvent(eventToSave, [
      enrichedArticle,
    ])

    await eventToSave.save()
    if (opportunitiesToSave.length > 0) {
      await Opportunity.insertMany(
        opportunitiesToSave.map((opp) => ({ ...opp, events: [eventToSave._id] }))
      )
    }

    return { success: true, event: eventToSave.synthesized_headline }
  } catch (e) {
    console.error('[Upload Action Error]:', e)
    return { success: false, error: e.message }
  }
}

export async function addKnowledge(data) {
  const { headline, business_summary, source, country, link } = data
  if (!headline || !business_summary || !source || !country || !link) {
    return { success: false, message: 'All fields are required.' }
  }
  try {
    const textToEmbed = `${headline}\n${business_summary}`
    const embedding = await generateEmbedding(textToEmbed)
    const newArticle = new Article({
      _id: new mongoose.Types.ObjectId(),
      headline,
      link,
      newspaper: source,
      source: 'Manual Upload',
      country: [country],
      relevance_headline: 100,
      assessment_headline: 'Manually uploaded by user.',
      relevance_article: 100,
      assessment_article: business_summary,
      embedding: embedding,
      key_individuals: [],
    })
    await newArticle.save()
    // Pinecone logic would go here
    return { success: true, message: 'Knowledge successfully added and embedded.' }
  } catch (error) {
    console.error('[Add Knowledge Error]', error)
    return { success: false, message: 'Failed to add knowledge.' }
  }
}

export async function suggestSections(url) {
  const scrapeResult = { success: true, content: '<div>Mock Content</div>' }
  try {
    const data = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: instructionSourceDiscovery,
      userContent: `Analyze the HTML from ${url}:\n\n${scrapeResult.content}`,
      isJson: true,
    })
    return { success: true, data: data.suggestions }
  } catch (e) {
    return { success: false, error: 'AI agent failed to suggest sections.' }
  }
}

export async function performAiSanityCheck() {
  try {
    logger.info('ðŸ”¬ Performing AI service sanity check (OpenAI)...')
    const answer = await callLanguageModel({
      modelName: 'gpt-5-nano',
      userContent: 'In one word, what is the capital of France?',
      isJson: false,
    })

    if (answer && answer.error) {
      logger.fatal(
        { details: answer.error },
        'OpenAI sanity check failed. The API call failed or timed out. This is often due to an incorrect API key, network issues, or service outage.'
      )
      return false
    }

    if (
      !answer ||
      typeof answer !== 'string' ||
      !answer.trim().toLowerCase().includes('paris')
    ) {
      logger.fatal(
        { details: { expected: 'paris', received: answer } },
        `OpenAI sanity check failed. The model did not return the expected response.`
      )
      return false
    }

    logger.info('âœ… AI service sanity check passed.')
    return true
  } catch (error) {
    logger.fatal(
      { err: error },
      'OpenAI sanity check failed with an unexpected exception.'
    )
    return false
  }
}

```

## ðŸ“„ src/lib/AIAgent.js
*Lines: 111, Size: 3.98 KB*

```javascript
// packages/ai-services/src/lib/AIAgent.js
import { callLanguageModel } from './langchain.js'
import { logger } from '@headlines/utils-shared'
import { buildPrompt } from './promptBuilder.js'

const MAX_CORRECTION_ATTEMPTS = 1

export class AIAgent {
  constructor({
    model,
    systemPrompt,
    isJson = true,
    fewShotInputs = [],
    fewShotOutputs = [],
    zodSchema,
    responseWrapperKey = null,
  }) {
    if (!model || !systemPrompt) {
      throw new Error('AIAgent requires a model and systemPrompt.')
    }
    this.model = model
    this.isJson = isJson
    this.fewShotInputs = fewShotInputs
    this.fewShotOutputs = fewShotOutputs
    this.zodSchema = zodSchema
    this.systemPrompt = buildPrompt(systemPrompt)
    this.responseWrapperKey = responseWrapperKey
  }

  async execute(userContent) {
    let currentContent = userContent
    try {
      for (let attempt = 0; attempt <= MAX_CORRECTION_ATTEMPTS; attempt++) {
        const response = await callLanguageModel({
          modelName: this.model,
          systemPrompt: this.systemPrompt,
          userContent: currentContent,
          isJson: this.isJson,
          fewShotInputs: this.fewShotInputs,
          fewShotOutputs: this.fewShotOutputs,
        })

        // --- START OF DEFINITIVE FIX ---
        // If the underlying API call returns an error object (e.g., from a timeout),
        // return it gracefully instead of throwing a fatal exception.
        if (response.error) {
          logger.error(
            { agent: this.constructor.name, model: this.model, error: response.error },
            `AIAgent's call to language model failed.`
          )
          return { error: response.error }
        }
        // --- END OF DEFINITIVE FIX ---

        let dataToValidate = response
        if (this.responseWrapperKey && response[this.responseWrapperKey]) {
          dataToValidate = response[this.responseWrapperKey]
        }

        if (!this.zodSchema) {
          return dataToValidate
        }

        const validationResult = this.zodSchema.safeParse(dataToValidate)

        if (validationResult.success) {
          if (attempt > 0) {
            logger.warn(
              { model: this.model, agent: this.constructor.name },
              `AI self-correction succeeded on attempt ${attempt + 1}.`
            )
          }
          return this.responseWrapperKey
            ? { [this.responseWrapperKey]: validationResult.data }
            : validationResult.data
        }

        if (attempt === MAX_CORRECTION_ATTEMPTS) {
          logger.error(
            {
              agentName: this.constructor.name,
              model: this.model,
              validationErrors: validationResult.error.flatten(),
              rawResponseFromAI: response,
            },
            `AI response failed Zod validation after all correction attempts.`
          )
          throw new Error('Zod validation failed permanently')
        }

        logger.warn(
          {
            model: this.model,
            agent: this.constructor.name,
            errors: validationResult.error.flatten(),
          },
          `Zod validation failed on attempt ${attempt + 1}. Initiating self-correction...`
        )

        currentContent = `The previous response you provided was invalid. You MUST correct it. Here was the original request (condensed):\n---\n${userContent.substring(0, 2000)}...\n---\nHere was your invalid JSON response:\n---\n${JSON.stringify(response, null, 2)}\n---\nHere are the specific validation errors you MUST fix:\n---\n${JSON.stringify(validationResult.error.flatten(), null, 2)}\n---\nNow, generate a new, corrected JSON response that strictly adheres to the schema and fixes all the listed errors. Respond ONLY with the corrected JSON object.`
      }
    } catch (error) {
      logger.error(
        { err: error, agent: this.constructor.name, model: this.model },
        `AIAgent execution failed catastrophically.`
      )
      return { error: error.message || 'A critical error occurred in the AI agent.' }
    }
  }
}

```

## ðŸ“„ src/lib/langchain.js
*Lines: 107, Size: 3.84 KB*

```javascript
// packages/ai-services/src/lib/langchain.js
import { ChatOpenAI } from '@langchain/openai'
import { env, settings } from '@headlines/config/node'
import { logger } from '@headlines/utils-shared'
import { tokenTracker } from '@headlines/utils-server/node'
import { safeExecute } from '@headlines/utils-server/helpers'
import OpenAI from 'openai'

// This function is kept for the pre-flight check
export function validatePromptBraces(promptText, agentName) {
  const singleBraceRegex = /(?<!\{)\{(?!\{)|(?<!\})\}(?!\})/g
  const match = singleBraceRegex.exec(promptText)
  if (match) {
    const char = match[0]
    const index = match.index
    const contextSnippet = promptText.substring(
      Math.max(0, index - 30),
      Math.min(promptText.length, index + 30)
    )
    const errorMessage = `\n[PROMPT VALIDATION ERROR] for agent/model '${agentName}'.\nFound a single unpaired curly brace '${char}' at position ${index}.\nAll curly braces in instruction prompts must be doubled (e.g., '{{' and '}}') to be treated as literal text and avoid template errors.\n\nContext:\n..."${contextSnippet}"...\n         ^\n`
    throw new Error(errorMessage)
  }
}

const modelConfig = { response_format: { type: 'json_object' } }

// LangChain model exports are kept for potential future use with different features (e.g., streaming)
export const getHeadlineModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_HEADLINE_ASSESSMENT }).bind(modelConfig)
export const getHighPowerModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_SYNTHESIS }).bind(modelConfig)
export const getUtilityModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_UTILITY }).bind(modelConfig)
export const getProModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_PRO }).bind(modelConfig)

// Use the official OpenAI client for core, reliable API calls.
const baseClient = new OpenAI({
  apiKey: env.OPENAI_API_KEY,
  timeout: 120 * 1000,
  maxRetries: 3,
})

export async function callLanguageModel({
  modelName,
  systemPrompt,
  userContent,
  isJson = true,
  fewShotInputs = [],
  fewShotOutputs = [],
}) {
  const messages = []
  if (systemPrompt) {
    messages.push({ role: 'system', content: systemPrompt })
  }
  fewShotInputs.forEach((input, i) => {
    const shotContent = typeof input === 'string' ? input : JSON.stringify(input)
    if (shotContent) {
      messages.push({ role: 'user', content: shotContent })
      messages.push({ role: 'assistant', content: fewShotOutputs[i] })
    }
  })
  messages.push({ role: 'user', content: userContent })

  const apiPayload = { model: modelName, messages: messages }
  if (isJson) {
    apiPayload.response_format = { type: 'json_object' }
  }

  // --- START OF DEFINITIVE FIX ---
  // The timeout wrapper is now placed around the direct API call,
  // not the entire agent's logic.
  const result = await safeExecute(() => baseClient.chat.completions.create(apiPayload), {
    timeout: 85000,
  })

  if (!result) return { error: 'API call failed or timed out' }
  // --- END OF DEFINITIVE FIX ---

  if (result.usage) tokenTracker.recordUsage(modelName, result.usage)

  const responseContent = result.choices[0]?.message?.content

  if (typeof responseContent !== 'string') {
    logger.error(
      { response: result },
      `LLM response for model ${modelName} was empty or in an unexpected format.`
    )
    return { error: 'LLM response was empty or invalid.' }
  }

  if (isJson) {
    try {
      const jsonMatch = responseContent.match(/\{[\s\S]*\}/)
      if (!jsonMatch)
        throw new Error("No valid JSON object found in the LLM's string response.")
      return JSON.parse(jsonMatch[0])
    } catch (parseError) {
      logger.error(
        { err: parseError, rawContent: responseContent },
        `LLM response JSON Parse Error for model ${modelName}`
      )
      return { error: 'JSON Parsing Error' }
    }
  }
  return responseContent
}
```

## ðŸ“„ src/lib/promptBuilder.js
*Lines: 35, Size: 1.2 KB*

```javascript
// packages/ai-services/src/lib/promptBuilder.js
import { settings } from '@headlines/config'

/**
 * A robust, centralized function to build a string prompt from an instruction object.
 * It handles instruction objects, functions that return objects, and plain strings.
 * @param {object|Function|string} instruction - The prompt instruction object or function.
 * @returns {string} The fully constructed prompt string.
 */
export function buildPrompt(instruction) {
  let promptSource = instruction

  // If the prompt is a function, execute it to get the object.
  if (typeof promptSource === 'function') {
    promptSource = promptSource(settings)
  }

  // If the result is an object, automatically build the string.
  if (typeof promptSource === 'object' && promptSource !== null) {
    return Object.values(promptSource)
      .flat() // Seamlessly handles both 'string' and ['array', 'of', 'strings']
      .filter((value) => typeof value === 'string')
      .join('\n\n')
  }

  // If it's already a string, use it directly.
  if (typeof promptSource === 'string') {
    return promptSource
  }

  throw new Error(
    'buildPrompt received an invalid instruction type. Must be an object, function, or string.'
  )
}

```

## ðŸ“„ src/lib/safeInvoke.js
*Lines: 104, Size: 3.45 KB*

```javascript
// packages/ai-services/src/lib/safeInvoke.js
import { logger } from '@headlines/utils-shared'
import { getRedisClient } from '@headlines/utils-server/node'
import { createHash } from 'crypto'

const MAX_RETRIES = 1
const CACHE_TTL_SECONDS = 60 * 60 * 24
const inMemoryCache = new Map()

function createCacheKey(agentName, input) {
  const hash = createHash('sha256')
  hash.update(JSON.stringify(input))
  return `ai_cache:${agentName}:${hash.digest('hex')}`
}

export async function safeInvoke(chain, input, agentName, zodSchema) {
  const redis = await getRedisClient()
  const cacheKey = createCacheKey(agentName, input)

  if (redis) {
    try {
      const cachedResult = await redis.get(cacheKey)
      if (typeof cachedResult === 'string' && cachedResult.length > 0) {
        logger.trace({ agent: agentName }, `[Redis Cache HIT] for ${agentName}.`)
        return JSON.parse(cachedResult)
      }
    } catch (err) {
      logger.error({ err, agent: agentName, key: cacheKey }, `Redis GET or PARSE failed.`)
    }
  } else if (inMemoryCache.has(cacheKey)) {
    logger.trace({ agent: agentName }, `[In-Memory Cache HIT] for ${agentName}.`)
    return inMemoryCache.get(cacheKey)
  }

  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
    try {
      let result = await chain.invoke(input)

      // --- START OF DEFINITIVE, FINAL FIX ---
      // The old logic relied on LangChain's strict JsonOutputParser. This new logic is more robust.
      // 1. Unwrap the AIMessage object to get the raw string content.
      if (result && typeof result.content === 'string' && result.id) {
        result = result.content
      }

      if (typeof result !== 'string') {
        throw new Error('AI response was not a string after unwrapping.')
      }

      // 2. Use a regex to find the JSON block, ignoring any conversational text from the AI.
      const jsonMatch = result.match(/\{[\s\S]*\}/)
      if (!jsonMatch) {
        throw new Error("No valid JSON object found in the LLM's string response.")
      }

      // 3. Parse only the extracted JSON block.
      const parsedResult = JSON.parse(jsonMatch[0])
      // --- END OF DEFINITIVE, FINAL FIX ---

      const validation = zodSchema.safeParse(parsedResult)
      if (!validation.success) {
        logger.error(
          {
            agent: agentName,
            zodErrorSummary: validation.error.flatten(),
            rawAIDataThatFailedValidation: parsedResult,
          },
          `Zod validation failed for ${agentName}.`
        )
        throw new Error('Zod validation failed')
      }

      const dataToCache = validation.data

      if (redis) {
        try {
          await redis.set(cacheKey, JSON.stringify(dataToCache), {
            EX: CACHE_TTL_SECONDS,
          })
        } catch (err) {
          logger.error({ err, agent: agentName }, `Redis SET failed for ${agentName}.`)
        }
      } else {
        inMemoryCache.set(cacheKey, dataToCache)
      }
      return dataToCache
    } catch (error) {
      if (attempt < MAX_RETRIES) {
        logger.warn(
          { agent: agentName, attempt: attempt + 1, error: error.message },
          `Invocation failed for ${agentName}. Retrying...`
        )
        await new Promise((res) => setTimeout(res, 1500 * (attempt + 1)))
        continue
      }
      logger.error(
        { err: error, agent: agentName },
        `Chain invocation failed for ${agentName} after all retries.`
      )
      return { error: `Agent ${agentName} failed: ${error.message}` }
    }
  }
}

```

## ðŸ“„ src/next.js
*Lines: 54, Size: 1.6 KB*

```javascript
// packages/ai-services/src/next.js
import 'server-only'

export * from './chains/index.js'
export * from './search/search.js'
export * from './search/wikipedia.js'
export * from './embeddings/embeddings.js'
export * from './embeddings/vectorSearch.js'
export * from './rag/orchestrator.js'
export * from './shared/agents/synthesisAgent.js'
export * from './shared/agents/opportunityAgent.js'
export * from './shared/agents/contactAgent.js'
export * from './shared/agents/entityAgent.js'
export * from './shared/agents/emailAgents.js'
export * from './shared/agents/executiveSummaryAgent.js'

import {
  generateChatTitle as coreGenTitle,
  processUploadedArticle as coreUpload,
  addKnowledge as coreAddKnowledge,
  suggestSections as coreSuggestSections,
} from './index.js'
import dbConnect from '@headlines/data-access/dbConnect/next'
import { revalidatePath } from 'next/cache'

// Wrap core functions with dbConnect for the Next.js environment
export const generateChatTitle = async (...args) => {
  await dbConnect()
  return coreGenTitle(...args)
}

export const processUploadedArticle = async (...args) => {
  await dbConnect()
  const result = await coreUpload(...args)
  if (result.success) {
    revalidatePath('/events')
    revalidatePath('/opportunities')
  }
  return result
}

export const addKnowledge = async (...args) => {
  await dbConnect()
  return coreAddKnowledge(...args)
}

export const suggestSections = async (...args) => {
  await dbConnect()
  return coreSuggestSections(...args)
}

import { performAiSanityCheck as coreSanityCheck } from './index.js'
export const performAiSanityCheck = coreSanityCheck

```

## ðŸ“„ src/node/agents/articleAgent.js
*Lines: 108, Size: 3.69 KB*

```javascript
// packages/ai-services/src/node/agents/articleAgent.js
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { articleAssessmentSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionArticle,
    fewShotInputs: shotsInputArticle,
    fewShotOutputs: shotsOutputArticle,
    zodSchema: articleAssessmentSchema,
  })

export async function assessArticleContent(
  article,
  hits = [],
  isSalvaged = false,
  externalContext = ''
) {
  const articleAssessmentAgent = getAgent()
  const fullContent = (article.articleContent?.contents || []).join('\n')
  const truncatedContent = truncateString(fullContent, settings.LLM_CONTEXT_MAX_CHARS)

  if (fullContent.length > settings.LLM_CONTEXT_MAX_CHARS) {
    logger.warn(
      {
        originalLength: fullContent.length,
        truncatedLength: truncatedContent.length,
        limit: settings.LLM_CONTEXT_MAX_CHARS,
      },
      `Article content for LLM was truncated.`
    )
  }

  let articleText = `HEADLINE: ${article.headline}\n\nBODY:\n${article.headline}\n\n${truncatedContent}`

  if (hits.length > 0) {
    const hitStrings = hits.map(
      (hit) =>
        `[WATCHLIST HIT: ${hit.entity.name} | CONTEXT: ${hit.entity.context || 'N/A'}]`
    )
    const hitPrefix = hitStrings.join(' ')
    articleText = `${hitPrefix} ${articleText}`
    logger.info(
      { hits: hits.map((h) => h.entity.name) },
      'Watchlist entities found in article.'
    )
  }

  if (isSalvaged) {
    articleText = `[SALVAGE CONTEXT: The original source for this headline failed to scrape. This content is from an alternative source. Please assess based on this new context.]\n\n${articleText}`
  }

  if (externalContext) {
    articleText = `[EXTERNAL CONTEXT FROM WEB SEARCH]:\n${externalContext}\n\n[ORIGINAL ARTICLE DATA]:\n${articleText}`
  }

  const response = await articleAssessmentAgent.execute(articleText)

  if (response.error) {
    logger.error(
      { article: { link: article.link }, details: response },
      `Article assessment failed for ${article.link}.`
    )
    return { ...article, error: `AI Error: ${response.error}` }
  }

  // --- START OF DEFINITIVE FIX ---
  // The AI can sometimes hallucinate individuals not present in the text. This filter ensures that only
  // individuals who are actually mentioned in the article body are included in the final output.
  if (response.key_individuals && response.key_individuals.length > 0) {
    const articleTextLower = articleText.toLowerCase()
    response.key_individuals = response.key_individuals.filter((ind) => {
      if (!ind || !ind.name) return false // Filter out null/undefined entries
      const isPresent = ind.name
        .split(' ')
        .filter((p) => p.length > 2)
        .some((p) => articleTextLower.includes(p.toLowerCase()))
      if (!isPresent) {
        logger.warn(
          { individual: ind.name, article: article.headline },
          'Discarding hallucinated key individual not present in article text.'
        )
      }
      return isPresent
    })
  }
  // --- END OF DEFINITIVE FIX ---

  if (
    response.amount > 0 &&
    response.amount < settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  ) {
    response.relevance_article = 10
    response.assessment_article = `Dropped: Amount ($${response.amount}M) is below the financial threshold of $${settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS}M.`
  }

  return { ...article, ...response, error: null }
}

```

## ðŸ“„ src/node/agents/batchArticleAgent.js
*Lines: 64, Size: 2.01 KB*

```javascript
// packages/ai-services/src/agents/batchArticleAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { batchArticleAssessmentSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings, AI_BATCH_SIZE } from '@headlines/config/node'
import { getInstructionBatchArticleAssessment } from '@headlines/prompts'
import { assessArticleContent } from './articleAgent.js'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionBatchArticleAssessment,
    zodSchema: batchArticleAssessmentSchema,
  })

export async function batchAssessArticles(articles) {
  if (!articles || articles.length === 0) return []

  const batchAgent = getAgent()
  const articleBatches = []
  for (let i = 0; i < articles.length; i += AI_BATCH_SIZE) {
    articleBatches.push(articles.slice(i, i + AI_BATCH_SIZE))
  }

  const allResults = []

  for (const batch of articleBatches) {
    const payload = batch.map((article) => ({
      headline: article.headline,
      content: (article.articleContent?.contents || []).join('\n'),
    }))

    const response = await batchAgent.execute(JSON.stringify(payload))

    if (
      response.error ||
      !response.assessments ||
      response.assessments.length !== batch.length
    ) {
      logger.error(
        {
          details: response,
          expectedCount: batch.length,
          receivedCount: response.assessments?.length,
        },
        'Batch assessment failed or returned mismatched count. Falling back to single-article processing for this batch.'
      )

      const fallbackPromises = batch.map((article) => assessArticleContent(article))
      const fallbackResults = await Promise.all(fallbackPromises)
      allResults.push(...fallbackResults)
      continue
    }

    const mergedResults = batch.map((originalArticle, index) => ({
      ...originalArticle,
      ...response.assessments[index],
    }))
    allResults.push(...mergedResults)
  }

  return allResults
}

```

## ðŸ“„ src/node/agents/clusteringAgent.js
*Lines: 75, Size: 2.3 KB*

```javascript
// packages/ai-services/src/node/agents/clusteringAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { clusterSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionCluster } from '@headlines/prompts'

const CLUSTER_BATCH_SIZE = 25

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionCluster,
    zodSchema: clusterSchema,
  })

export async function clusterArticlesIntoEvents(articles) {
  const articleClusterAgent = getAgent()
  logger.info(`Clustering ${articles.length} articles into unique events...`)

  if (!articles || articles.length === 0) {
    return []
  }

  const batches = []
  for (let i = 0; i < articles.length; i += CLUSTER_BATCH_SIZE) {
    batches.push(articles.slice(i, i + CLUSTER_BATCH_SIZE))
  }
  logger.info(`Processing clusters in ${batches.length} batches.`)

  const allClusters = []
  for (const [index, batch] of batches.entries()) {
    logger.info(`Clustering batch ${index + 1} of ${batches.length}...`)
    const articlePayload = batch.map((a) => ({
      id: a._id.toString(),
      headline: a.headline,
      source: a.newspaper,
      summary: (a.assessment_article || a.assessment_headline || '').substring(0, 400),
    }))
    const userContent = JSON.stringify(articlePayload)
    const response = await articleClusterAgent.execute(userContent)

    if (response.error || !response.events) {
      logger.error(`Failed to cluster articles in batch ${index + 1}.`, {
        response,
      })
      continue
    }
    allClusters.push(...response.events)
  }

  if (allClusters.length === 0) {
    logger.warn('Failed to cluster any articles across all batches.')
    return []
  }

  const finalEventMap = new Map()
  allClusters.forEach((event) => {
    if (finalEventMap.has(event.event_key)) {
      const existing = finalEventMap.get(event.event_key)
      event.article_ids.forEach((id) => existing.article_ids.add(id))
    } else {
      finalEventMap.set(event.event_key, {
        event_key: event.event_key,
        article_ids: new Set(event.article_ids),
      })
    }
  })

  return Array.from(finalEventMap.values()).map((event) => ({
    event_key: event.event_key,
    article_ids: Array.from(event.article_ids),
  }))
}

```

## ðŸ“„ src/node/agents/headlineAgent.js
*Lines: 66, Size: 2.08 KB*

```javascript
// packages/ai-services/src/agents/headlineAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { headlineAssessmentSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings } from '@headlines/config/node'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_HEADLINE_ASSESSMENT,
    systemPrompt: instructionHeadlines,
    fewShotInputs: shotsInputHeadlines,
    fewShotOutputs: shotsOutputHeadlines,
    zodSchema: headlineAssessmentSchema,
  })

async function assessSingleHeadline(article, hits = []) {
  const headlineAssessmentAgent = getAgent()
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`

  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }

  const response = await headlineAssessmentAgent.execute(headlineWithContext)

  let assessment = {
    relevance_headline: 0,
    assessment_headline: 'AI assessment failed.',
    headline_en: article.headline,
  }

  if (response && response.assessment && response.assessment.length > 0) {
    assessment = response.assessment[0]
    let score = assessment.relevance_headline
    const boost = settings.WATCHLIST_SCORE_BOOST

    if (hits.length > 0 && boost > 0) {
      score = Math.min(100, score + boost)
      assessment.assessment_headline = `Watchlist boost (+${boost}). ${assessment.assessment_headline}`
    }
    assessment.relevance_headline = score
  }

  return { ...article, ...assessment }
}

export async function assessHeadlinesInBatches(articles, articlesHits) {
  const assessmentPromises = articles.map((article, index) => {
    const hitsForArticle = articlesHits[index] || []
    return assessSingleHeadline(article, hitsForArticle)
  })

  const results = await Promise.all(assessmentPromises)
  return results
}

```

## ðŸ“„ src/node/agents/judgeAgent.js
*Lines: 62, Size: 1.76 KB*

```javascript
// packages/ai-services/src/node/agents/judgeAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { judgeSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionJudge } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionJudge,
    zodSchema: judgeSchema,
  })

export async function judgePipelineOutput(events, opportunities) {
  const judgeAgent = getAgent() // <-- FIX APPLIED HERE
  if (
    (!events || events.length === 0) &&
    (!opportunities || opportunities.length === 0)
  ) {
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }
  logger.info('âš–ï¸ [Judge Agent] Reviewing final pipeline output for quality control...')

  const lightweightEvents = (events || []).map((e) => ({
    identifier: `Event: ${e.synthesized_headline}`,
    summary: e.synthesized_summary,
    assessment: e.ai_assessment_reason,
    score: e.highest_relevance_score,
  }))

  const lightweightOpportunities = (opportunities || []).map((o) => ({
    identifier: `Opportunity: ${o.reachOutTo}`,
    reason: o.whyContact,
    wealth_estimate_mm: o.likelyMMDollarWealth,
  }))

  const inputText = JSON.stringify({
    events: lightweightEvents,
    opportunities: lightweightOpportunities,
  })

  const response = await judgeAgent.execute(inputText)

  if (response.error) {
    logger.error({ details: response }, 'Judge Agent failed to produce a verdict.')
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }

  logger.info(
    { details: response },
    '[Judge Agent] Successfully produced quality control verdicts.'
  )
  return response
}

```

## ðŸ“„ src/node/agents/sectionClassifierAgent.js
*Lines: 42, Size: 1.29 KB*

```javascript
// packages/ai-services/src/node/agents/sectionClassifierAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { sectionClassifierSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionSectionClassifier } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY, // Using the cheap and fast model
    systemPrompt: [
      instructionSectionClassifier.whoYouAre,
      instructionSectionClassifier.whatYouDo,
      ...instructionSectionClassifier.guidelines,
      instructionSectionClassifier.outputFormatDescription,
    ].join('\n\n'),
    zodSchema: sectionClassifierSchema,
  })

export async function classifyLinks(links) {
  if (!links || links.length === 0) {
    return []
  }

  const agent = getAgent() // <-- FIX APPLIED HERE
  const response = await agent.execute(JSON.stringify(links))

  if (
    response.error ||
    !response.classifications ||
    response.classifications.length !== links.length
  ) {
    logger.error(
      { response, expectedCount: links.length },
      'Section classifier agent failed or returned mismatched count.'
    )
    return null // Return null to indicate failure
  }

  return response.classifications
}

```

## ðŸ“„ src/node/agents/selectorRepairAgent.js
*Lines: 55, Size: 1.64 KB*

```javascript
// packages/ai-services/src/node/agents/selectorRepairAgent.js
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { selectorRepairSchema } from '@headlines/models/schemas'
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { instructionSelectorRepair } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: [
      instructionSelectorRepair.whoYouAre,
      instructionSelectorRepair.whatYouDo,
      ...instructionSelectorRepair.guidelines,
      instructionSelectorRepair.outputFormatDescription,
      instructionSelectorRepair.reiteration,
    ].join('\n\n'),
    zodSchema: selectorRepairSchema,
  })

export async function suggestNewSelector(
  url,
  failedSelector,
  htmlContent,
  heuristicSuggestions = []
) {
  const selectorRepairAgent = getAgent()
  try {
    const payload = {
      url,
      failed_selector: failedSelector,
      heuristic_suggestions: heuristicSuggestions.map((s) => ({
        selector: s.selector,
        samples: s.samples.slice(0, 3),
      })),
      html_content: truncateString(htmlContent, LLM_CONTEXT_MAX_CHARS),
    }

    const response = await selectorRepairAgent.execute(JSON.stringify(payload))

    if (response.error || !response.suggested_selectors) {
      logger.error('Selector repair agent failed to produce a valid suggestion.', {
        response,
      })
      return null
    }

    return response
  } catch (error) {
    logger.error({ err: error }, 'Error in suggestNewSelector')
    return null
  }
}

```

## ðŸ“„ src/node/agents/watchlistAgent.js
*Lines: 48, Size: 1.76 KB*

```javascript
// packages/ai-services/src/node/agents/watchlistAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { watchlistSuggestionSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionWatchlistSuggestion } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: [
      instructionWatchlistSuggestion.whoYouAre,
      instructionWatchlistSuggestion.whatYouDo,
      ...instructionWatchlistSuggestion.guidelines,
      instructionWatchlistSuggestion.outputFormatDescription,
      instructionWatchlistSuggestion.reiteration,
    ].join('\n\n'),
    zodSchema: watchlistSuggestionSchema,
  })

/**
 * Analyzes events to generate new watchlist suggestions.
 * @param {Array<object>} events - High-quality synthesized events.
 * @param {Set<string>} existingWatchlistNames - A set of lowercase names already on the watchlist.
 * @returns {Promise<Array<object>>} An array of new WatchlistSuggestion documents.
 */
export async function generateWatchlistSuggestions(events, existingWatchlistNames) {
  const watchlistSuggestionAgent = getAgent() // <-- FIX APPLIED HERE
  try {
    const payload = { events }
    const response = await watchlistSuggestionAgent.execute(JSON.stringify(payload))

    if (response.error || !Array.isArray(response.suggestions)) {
      logger.warn('AI failed to generate watchlist suggestions.', response)
      return []
    }

    const newSuggestions = response.suggestions.filter(
      (s) => !existingWatchlistNames.has(s.name.toLowerCase())
    )

    return newSuggestions
  } catch (error) {
    logger.error({ err: error }, 'Error in generateWatchlistSuggestions')
    return []
  }
}

```

## ðŸ“„ src/rag/generation.js
*Lines: 137, Size: 4.31 KB*

```javascript
// packages/ai-services/src/rag/generation.js
import { getSynthesizerPrompt } from './prompts.js'
import { checkGroundedness } from './validation.js'
import { callLanguageModel } from '../lib/langchain.js'
import { settings } from '@headlines/config'
import { ragResponseSchema } from '@headlines/models/schemas'
import { logger } from '@headlines/utils-shared'

const SYNTHESIZER_MODEL = settings.LLM_MODEL_SYNTHESIS

function assembleContext(ragResults, wikiResults, searchResults) {
  const dbContext =
    ragResults.length > 0
      ? ragResults
          .map(
            (match) =>
              `- [Similarity: ${match.score.toFixed(3)}] ${match.metadata.headline}: ${match.metadata.summary}`
          )
          .join('\n')
      : 'None'

  const wikiContext =
    wikiResults.length > 0
      ? wikiResults
          .map(
            (res) => `- [Quality: ${res.validation.quality}] ${res.title}: ${res.summary}`
          )
          .join('\n')
      : 'None'

  const searchContext =
    searchResults.length > 0
      ? searchResults
          .map((res) => `- [${res.title}](${res.link}): ${res.snippet}`)
          .join('\n')
      : 'None'

  return `---
Internal Database Context:
${dbContext}
---
Wikipedia Context:
${wikiContext}
---
Search Results Context:
${searchContext}
---`
}

function formatThoughts(plan, context, groundednessResult) {
  const thoughts = `
**THOUGHT PROCESS: THE PLAN**
${plan.plan.map((step) => `- ${step}`).join('\n')}

**REASONING:**
${plan.reasoning}

**RETRIEVED CONTEXT:**
- **Internal RAG Search:** ${context.ragResults.length} item(s) found.
${context.ragResults.map((r) => `  - [Score: ${r.score.toFixed(2)}] ${r.metadata.headline}`).join('\n')}

- **Wikipedia Search:** ${context.wikiResults.length} article(s) found.
${context.wikiResults.map((w) => `  - **Query:** "${w.query}"\n    - **Result:** ${w.title}: ${w.summary.substring(0, 100)}...`).join('\n')}

- **Web Search:** ${context.searchResults.length} result(s) found.
${context.searchResults.map((s) => `  - **Query:** "${plan.user_query}"\n    - **Result:** ${s.title}: ${s.snippet.substring(0, 100)}...`).join('\n')}

**FINAL CHECK:**
- **Groundedness Passed:** ${groundednessResult.is_grounded ? 'CONFIRMED' : 'FAILED'}
`
  return thoughts.trim().replace(/\n\n+/g, '\n\n')
}

function buildHtmlFromAnswerParts(answerParts) {
  if (!answerParts || answerParts.length === 0) return ''
  return answerParts
    .map((part) => {
      const sourceClass = {
        rag: 'rag-source',
        wiki: 'wiki-source',
        search: 'llm-source',
        llm: '',
      }[part.source]
      return sourceClass ? `<span class="${sourceClass}">${part.text}</span>` : part.text
    })
    .join('')
}

export async function generateFinalResponse({ plan, context }) {
  const fullContextString = assembleContext(
    context.ragResults,
    context.wikiResults,
    context.searchResults
  )

  logger.info(`[RAG Generation] Calling Synthesizer Agent with ${SYNTHESIZER_MODEL}...`)
  const synthesizerResponse = await callLanguageModel({
    modelName: SYNTHESIZER_MODEL,
    systemPrompt: getSynthesizerPrompt(),
    userContent: `CONTEXT:\n${fullContextString}\n\nPLAN:\n${JSON.stringify(
      plan.plan,
      null,
      2
    )}\n\nUSER'S QUESTION: "${plan.user_query}"`,
    isJson: true,
  })

  const validation = ragResponseSchema.safeParse(synthesizerResponse)
  if (!validation.success) {
    logger.error(
      { err: validation.error },
      '[RAG Generation] Synthesizer Agent failed to return valid structured JSON.'
    )
    return {
      answer: 'The AI synthesizer failed to generate a structured response.',
      thoughts: 'An error occurred during the final synthesis step.',
    }
  }

  const answerParts = validation.data.answer_parts
  const rawResponseText = answerParts.map((p) => p.text).join(' ')

  const groundednessResult = await checkGroundedness(rawResponseText, fullContextString)
  const thoughts = formatThoughts(plan, context, groundednessResult)

  let finalAnswer
  if (groundednessResult.is_grounded) {
    finalAnswer = buildHtmlFromAnswerParts(answerParts)
  } else {
    logger.warn('[RAG Pipeline] Groundedness check failed. Returning safe response.')
    finalAnswer =
      'I was unable to construct a reliable answer from the available sources. The context may be insufficient or conflicting.'
  }

  return { answer: finalAnswer, thoughts }
}

```

## ðŸ“„ src/rag/orchestrator.js
*Lines: 41, Size: 1.52 KB*

```javascript
// packages/ai-services/src/rag/orchestrator.js
import { retrieveContextForQuery } from './retrieval.js'
import { assessContextQuality } from './validation.js'
import { generateFinalResponse } from './generation.js'
import { runPlannerAgent } from './planner.js'
import { logger } from '@headlines/utils-shared'

export async function processChatRequest(messages) {
  logger.info('--- [RAG Pipeline Start] ---')

  logger.info('[RAG Pipeline] Step 1: Planning Phase Started...')
  const plan = await runPlannerAgent(messages)
  logger.info('[RAG Pipeline] Step 1: Planning Phase Completed.')

  logger.info('[RAG Pipeline] Step 2: Retrieval Phase Started...')
  const initialContext = await retrieveContextForQuery(plan, messages, 'ragOnly')
  const initialQuality = assessContextQuality(initialContext.ragResults, [], [])

  let finalContext = initialContext

  if (initialQuality.hasHighConfidenceRAG) {
    logger.info(
      '[RAG Pipeline] High confidence RAG hit found. Short-circuiting retrieval.'
    )
  } else {
    logger.info('[RAG Pipeline] RAG context insufficient. Proceeding to full retrieval.')
    finalContext = await retrieveContextForQuery(plan, messages, 'full')
  }
  logger.info('[RAG Pipeline] Step 2: Retrieval Phase Completed.')

  logger.info('[RAG Pipeline] Step 3: Synthesis Phase Started...')
  const finalResponse = await generateFinalResponse({
    plan,
    context: finalContext,
  })
  logger.info('[RAG Pipeline] Step 3: Synthesis Phase Completed.')

  logger.info('--- [RAG Pipeline End] ---')
  return finalResponse
}

```

## ðŸ“„ src/rag/planner.js
*Lines: 46, Size: 1.26 KB*

```javascript
// packages/ai-services/src/rag/planner.js
import { callLanguageModel } from '../lib/langchain.js'
import { PLANNER_PROMPT } from './prompts.js'
import { settings } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

const PLANNER_MODEL = settings.LLM_MODEL_UTILITY

export async function runPlannerAgent(messages) {
  const conversationText = messages
    .map((m) => `${m.role.toUpperCase()}: ${m.content}`)
    .join('\n\n')

  logger.info(`[Planner Agent] Generating plan with ${PLANNER_MODEL}...`)

  const response = await callLanguageModel({
    modelName: PLANNER_MODEL,
    systemPrompt: PLANNER_PROMPT,
    userContent: conversationText,
    isJson: true,
  })

  if (response.error) {
    throw new Error(`Planner Agent failed: ${response.error}`)
  }

  // --- START OF THE FIX ---
  // Replaced browser-specific logger.groupCollapsed with a single structured log object.
  // This is safe to run on the server.
  logger.trace(
    {
      agent: 'Planner Agent',
      planDetails: {
        userQuery: response.user_query,
        reasoning: response.reasoning,
        planSteps: response.plan,
        searchQueries: response.search_queries,
      },
    },
    '[Planner Agent] Plan Generated'
  )
  // --- END OF THE FIX ---

  return response
}

```

## ðŸ“„ src/rag/prompts.js
*Lines: 103, Size: 6.08 KB*

```javascript
// File: packages/ai-services/src/rag/prompts.js (Unabridged and Corrected)

export const PLANNER_PROMPT = `You are an expert AI Planner. Your job is to analyze the user's query and conversation history to create a step-by-step plan for an AI Synthesizer Agent to follow. You also create a list of optimized search queries for a Retrieval Agent.

**Conversation History:**
{CONVERSATION_HISTORY}

**Latest User Query:**
"{USER_QUERY}"

**Your Task:**
1.  **Analyze the User's Intent:** Understand what the user is truly asking for.
2.  **Formulate a Plan:** Create a clear, step-by-step plan for the Synthesizer Agent.
3.  **Generate Search Queries:** Create an array of 1-3 optimized, self-contained search queries. **CRITICAL JSON RULE:** If a query within the 'search_queries' array requires double quotes, you MUST escape them with a backslash. For example: ["\\"Troels Holch Povlsen\\" sons", "Bestseller founder"].

**Example 1:**
User Query: "Which Danish Rich List person is involved in Technology?"
History: (empty)
Your JSON Output:
{
  "user_query": "Which Danish Rich List person is involved in Technology?",
  "reasoning": "The user wants a list of wealthy Danes involved in technology. I need to identify these individuals from the context and then filter them based on their tech involvement.",
  "plan": [
    "Scan all context to identify every unique individual mentioned who is on the Danish Rich List.",
    "For each person, look for evidence of direct involvement in the technology sector.",
    "Filter out individuals with no clear connection to technology.",
    "Synthesize the findings into a helpful list of names, citing their connection to technology.",
    "If no one is found, state that clearly."
  ],
  "search_queries": ["Danish Rich List technology involvement", "Wealthy Danish tech investors", "Danish tech company founders"]
}

**Example 2:**
User Query: "Does Troels Holch Povlsen have sons?"
History: (assistant previously mentioned Bestseller's founder)
Your JSON Output:
{
  "user_query": "Does Troels Holch Povlsen have sons?",
  "reasoning": "The user is asking a direct factual question about a specific person's family. The search queries must be precise.",
  "plan": [
      "Scan context for any mention of 'Troels Holch Povlsen' and his family, specifically children or sons.",
      "Extract the names of his sons if mentioned.",
      "Synthesize a complete and helpful answer, stating the names of the sons and any additional relevant context provided."
  ],
  "search_queries": ["\\"Troels Holch Povlsen\\" sons", "\\"Troels Holch Povlsen\\" children", "\\"Bestseller\\" founder family"]
}

Respond ONLY with a valid JSON object with the specified structure.
`

export const getSynthesizerPrompt =
  () => `You are an elite, fact-based intelligence analyst. Your SOLE task is to execute the provided "PLAN" using only the "CONTEXT" to answer the "USER'S QUESTION". You operate under a strict "ZERO HALLUCINATION" protocol. Your response must be confident, direct, and sound like a human expert.

**PRIMARY DIRECTIVE:**
Synthesize information from all sources in the "CONTEXT" into a single, cohesive, and well-written answer. Directly address the user's question and enrich it with relevant surrounding details found in the context.

**EXAMPLE TONE:**
-   **Bad:** "According to the context, Bestseller was founded by Troels Holch Povlsen."
-   **Good:** "Bestseller was founded in 1975 by Troels Holch Povlsen and his wife, Merete Bech Povlsen. The company is now run by their son, Anders Holch Povlsen."

**CRITICAL RULES OF ENGAGEMENT:**
1.  **NO OUTSIDE KNOWLEDGE:** You are forbidden from using any information not present in the provided "CONTEXT".
2.  **DIRECT ATTRIBUTION:** You MUST still cite your sources inline for the UI. Wrap facts from the Internal DB with <rag>tags</rag>, from Wikipedia with <wiki>tags</wiki>, and from Search Results with <search>tags</search>. The user will not see these tags, but they are essential for the system.
3.  **BE CONFIDENT AND DIRECT:** Present the synthesized facts as a definitive answer.
4.  **INSUFFICIENT DATA:** If the context is insufficient to answer the question at all, respond with EXACTLY: "I do not have sufficient information in my sources to answer that question."
5.  **DO NOT OFFER HELP (CRITICAL):** You MUST NOT end your response by offering to search for more information, provide more details, or ask follow-up questions. Your answer should be a complete, self-contained statement of facts.

**DO NOT:**
-   Use phrases like "According to the context provided...", "The sources state...", or "Based on the information...".
-   Apologize for not knowing or mention your limitations.
-   Talk about your process in the final answer.
-   Speculate or infer beyond what is explicitly stated in the context.

Answer the question directly and authoritatively, as if you are a world-class analyst presenting your verified findings.`

export const GROUNDEDNESS_CHECK_PROMPT = `You are a meticulous fact-checker AI. Your task is to determine if the "Proposed Response" is strictly grounded in the "Provided Context". A response is grounded if and only if ALL of its claims can be directly verified from the context.

**Provided Context:**
---
{CONTEXT}
---

**Proposed Response:**
---
{RESPONSE}
---

Analyze the "Proposed Response" sentence by sentence.

**Respond ONLY with a valid JSON object with the following structure:**
{
  "is_grounded": boolean, // true if ALL claims in the response are supported by the context, otherwise false.
  "unsupported_claims": [
    // List any specific claims from the response that are NOT supported by the context.
    "Claim 1 that is not supported.",
    "Claim 2 that is not supported."
  ]
}

If the response is fully supported, "unsupported_claims" should be an empty array. If the "Proposed Response" states that it cannot answer the question, consider it grounded.`

export const FAILED_GROUNDEDNESS_PROMPT = `I could not form a reliable answer based on the available information. The initial response I generated may have contained information not supported by the sources. For accuracy, please ask a more specific question or try rephrasing your request.`

```

## ðŸ“„ src/rag/retrieval.js
*Lines: 134, Size: 3.8 KB*

```javascript
// packages/ai-services/src/rag/retrieval.js
import { OpenAI } from 'openai'
import { Pinecone } from '@pinecone-database/pinecone'
import { generateQueryEmbeddings } from '../embeddings/embeddings.js'
import {
  fetchBatchWikipediaSummaries,
  validateWikipediaContent,
} from '../search/wikipedia.js'
import { getGoogleSearchResults } from '../search/serpapi.js'
import { env } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

let openAIClient, pineconeIndex
function initializeClients() {
  if (!openAIClient) {
    if (env.OPENAI_API_KEY) {
      openAIClient = new OpenAI({ apiKey: env.OPENAI_API_KEY })
    }
    if (env.PINECONE_API_KEY) {
      const pc = new Pinecone({ apiKey: env.PINECONE_API_KEY })
      pineconeIndex = pc.index(env.PINECONE_INDEX_NAME)
    }
  }
}

const SIMILARITY_THRESHOLD = 0.38

async function fetchPineconeContext(queries, exclude_entities = []) {
  initializeClients()
  if (!pineconeIndex) {
    logger.warn(
      '[RAG Retrieval] Pinecone is not configured. Skipping internal DB search.'
    )
    return []
  }

  const queryEmbeddings = await Promise.all(
    queries.map((q) => generateQueryEmbeddings(q))
  )
  const allQueryEmbeddings = queryEmbeddings.flat()

  const pineconePromises = allQueryEmbeddings.map((embedding) =>
    pineconeIndex.query({
      topK: 5,
      vector: embedding,
      includeMetadata: true,
    })
  )
  const pineconeResponses = await Promise.all(pineconePromises)

  const uniqueMatches = new Map()
  pineconeResponses.forEach((response) => {
    response?.matches?.forEach((match) => {
      if (
        !uniqueMatches.has(match.id) ||
        match.score > uniqueMatches.get(match.id).score
      ) {
        uniqueMatches.set(match.id, match)
      }
    })
  })

  const results = Array.from(uniqueMatches.values())
    .filter((match) => match.score >= SIMILARITY_THRESHOLD)
    .sort((a, b) => b.score - a.score)
    .slice(0, 5)

  logger.groupCollapsed(`[RAG Retrieval] Pinecone Results (${results.length})`)
  results.forEach((match) => {
    logger.trace(`- Score: ${match.score.toFixed(4)} | ID: ${match.id}`)
    logger.trace(`  Headline: ${match.metadata.headline}`)
  })
  logger.groupEnd()

  return results
}

async function fetchValidatedWikipediaContext(entities) {
  const wikiResults = await fetchBatchWikipediaSummaries(entities)
  const validWikiResults = []
  for (const res of wikiResults.filter((r) => r.success)) {
    const validation = await validateWikipediaContent(res.summary)
    if (validation.valid) {
      validWikiResults.push({ ...res, validation })
    }
  }

  logger.groupCollapsed(`[RAG Retrieval] Wikipedia Results (${validWikiResults.length})`)
  validWikiResults.forEach((res) => {
    logger.trace(`- Title: ${res.title}`)
    logger.trace(`  Summary: ${res.summary.substring(0, 200)}...`)
  })
  logger.groupEnd()

  return validWikiResults
}

export async function retrieveContextForQuery(plan, messages, mode = 'full') {
  const { search_queries, user_query } = plan

  const pineconeResults = await fetchPineconeContext(search_queries)

  if (mode === 'ragOnly') {
    return {
      ragResults: pineconeResults,
      wikiResults: [],
      searchResults: [],
    }
  }

  const [wikipediaResults, searchResultsObj] = await Promise.all([
    fetchValidatedWikipediaContext(search_queries),
    getGoogleSearchResults(user_query),
  ])

  const searchResults = searchResultsObj.success ? searchResultsObj.results : []

  logger.groupCollapsed(
    `[RAG Retrieval] SerpAPI Google Search Results (${searchResults.length})`
  )
  searchResults.forEach((res) => {
    logger.trace(`- Title: ${res.title}`)
    logger.trace(`  Link: ${res.link}`)
    logger.trace(`  Snippet: ${res.snippet}`)
  })
  logger.groupEnd()

  return {
    ragResults: pineconeResults,
    wikiResults: wikipediaResults,
    searchResults: searchResults,
  }
}

```

## ðŸ“„ src/rag/validation.js
*Lines: 82, Size: 2.9 KB*

```javascript
// packages/ai-services/src/rag/validation.js
import { settings } from '@headlines/config'
import { callLanguageModel } from '../lib/langchain.js'
import { GROUNDEDNESS_CHECK_PROMPT } from './prompts.js'
import { logger } from '@headlines/utils-shared'

const HIGH_CONFIDENCE_THRESHOLD = 0.75
const SIMILARITY_THRESHOLD = 0.38

export function assessContextQuality(ragResults, wikiResults, searchResults) {
  const ragScore = ragResults.length > 0 ? Math.max(...ragResults.map((r) => r.score)) : 0
  const highQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'high'
  ).length
  const mediumQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'medium'
  ).length
  const wikiScore = highQualityWiki > 0 ? 0.7 : mediumQualityWiki > 0 ? 0.5 : 0
  const searchScore = searchResults.length > 0 ? 0.6 : 0

  const combinedScore = Math.max(ragScore, wikiScore, searchScore)

  return {
    hasHighConfidenceRAG: ragScore >= HIGH_CONFIDENCE_THRESHOLD,
    hasSufficientContext: combinedScore >= SIMILARITY_THRESHOLD,
    ragResultCount: ragResults.length,
    wikiResultCount: wikiResults.length,
    searchResultCount: searchResults.length,
    highQualityWikiCount: highQualityWiki,
    maxSimilarity: ragScore,
    combinedConfidence: combinedScore,
    hasMultipleSources:
      (ragResults.length > 0 ? 1 : 0) +
        (wikiResults.length > 0 ? 1 : 0) +
        (searchResults.length > 0 ? 1 : 0) >
      1,
    hasHighQualityContent: ragScore >= HIGH_CONFIDENCE_THRESHOLD || highQualityWiki > 0,
  }
}

export async function checkGroundedness(responseText, contextString) {
  logger.info('[RAG Validation] Performing Groundedness Check...')
  if (
    responseText.trim() ===
    'I do not have sufficient information in my sources to answer that question.'
  ) {
    logger.info('[RAG Validation] PASSED: Bot correctly stated insufficient info.')
    return { is_grounded: true, unsupported_claims: [] }
  }

  try {
    const prompt = GROUNDEDNESS_CHECK_PROMPT.replace('{CONTEXT}', contextString).replace(
      '{RESPONSE}',
      responseText
    )

    const result = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      systemPrompt: prompt,
      userContent: 'Perform the groundedness check based on the system prompt.',
      isJson: true,
    })

    if (result.error) {
      throw new Error(result.error)
    }

    if (result.is_grounded) {
      logger.info('[RAG Validation] PASSED: Response is grounded in sources.')
    } else {
      logger.warn('[RAG Validation] FAILED: Response contains unsupported claims.')
      logger.groupCollapsed('Unsupported Claims Details')
      result.unsupported_claims.forEach((claim) => logger.warn(`- ${claim}`))
      logger.groupEnd()
    }
    return result
  } catch (error) {
    logger.error({ err: error }, '[RAG Validation] Error during verification:')
    return { is_grounded: false, unsupported_claims: ['Fact-checking system failed.'] }
  }
}

```

## ðŸ“„ src/search/search.js
*Lines: 489, Size: 11.94 KB*

```javascript
// packages/ai-services/src/search/search.js
import axios from 'axios'
import NewsAPI from 'newsapi'
import { env } from '@headlines/config'
import { logger, apiCallTracker } from '@headlines/utils-shared'

const { SERPER_API_KEY, NEWSAPI_API_KEY } = env

/**
 * API configuration constants
 */
const API_CONFIG = {
  SERPER: {
    BASE_URL: 'https://google.serper.dev',
    TIMEOUT: 30000,
    MAX_RETRIES: 2,
    DEFAULT_RESULTS: 5,
  },
  NEWSAPI: {
    TIMEOUT: 30000,
    MAX_RETRIES: 2,
    DEFAULT_PAGE_SIZE: 5,
    DEFAULT_LANGUAGES: 'en,da,sv,no',
  },
}

/**
 * Initialize Serper API client with proper configuration
 */
const serperClient = SERPER_API_KEY
  ? axios.create({
      baseURL: API_CONFIG.SERPER.BASE_URL,
      timeout: API_CONFIG.SERPER.TIMEOUT,
      headers: {
        'X-API-KEY': SERPER_API_KEY,
        'Content-Type': 'application/json',
      },
      validateStatus: (status) => status < 500, // Don't throw on 4xx errors
    })
  : null

/**
 * Initialize NewsAPI client
 */
const newsapi = NEWSAPI_API_KEY ? new NewsAPI(NEWSAPI_API_KEY) : null

// Log API availability on initialization
if (!serperClient) {
  logger.warn('SERPER_API_KEY not configured. Google Search features will be disabled.')
}

if (!newsapi) {
  logger.warn('NEWSAPI_API_KEY not configured. NewsAPI features will be disabled.')
}

/**
 * Determines if an error is retryable
 * @param {Error} error - Error object
 * @returns {boolean} True if the error should trigger a retry
 */
function isRetryableError(error) {
  // Network errors
  if (
    error.code === 'ECONNRESET' ||
    error.code === 'ETIMEDOUT' ||
    error.code === 'ENOTFOUND'
  ) {
    return true
  }

  // 5xx server errors
  if (error.response && error.response.status >= 500) {
    return true
  }

  // Rate limiting (429) - worth retrying after backoff
  if (error.response && error.response.status === 429) {
    return true
  }

  return false
}

/**
 * Executes an API call with automatic retry logic and exponential backoff
 * @param {Function} apiCall - Async function that performs the API call
 * @param {string} serviceName - Name of the service for logging
 * @param {number} maxRetries - Maximum number of retry attempts
 * @returns {Promise<Object>} API response or error object
 */
async function withRetry(
  apiCall,
  serviceName,
  maxRetries = API_CONFIG.SERPER.MAX_RETRIES
) {
  const startTime = Date.now()

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const result = await apiCall()

      if (attempt > 1) {
        const duration = Date.now() - startTime
        logger.info(
          {
            service: serviceName,
            attempt,
            duration: `${duration}ms`,
          },
          'API call succeeded after retry'
        )
      }

      return result
    } catch (error) {
      const isLastAttempt = attempt === maxRetries
      const shouldRetry = isRetryableError(error)

      // Log the error with context
      const errorContext = {
        service: serviceName,
        attempt,
        maxRetries,
        status: error.response?.status,
        code: error.code,
        message: error.message,
      }

      if (!shouldRetry || isLastAttempt) {
        const duration = Date.now() - startTime

        logger.error(
          {
            ...errorContext,
            duration: `${duration}ms`,
            err: error.response?.data || error,
          },
          `${serviceName} API call failed${isLastAttempt && shouldRetry ? ' after retries' : ''}`
        )

        return {
          success: false,
          error: error.message,
          errorCode: error.response?.status || error.code,
          results: [],
        }
      }

      // Calculate exponential backoff delay
      const delay = 1000 * Math.pow(2, attempt - 1)

      logger.warn(
        {
          ...errorContext,
          delay: `${delay}ms`,
        },
        `${serviceName} attempt ${attempt} failed. Retrying...`
      )

      await new Promise((resolve) => setTimeout(resolve, delay))
    }
  }
}

/**
 * Validates and sanitizes search query
 * @param {string} query - Raw search query
 * @returns {string|null} Sanitized query or null if invalid
 */
function sanitizeQuery(query) {
  if (!query || typeof query !== 'string') {
    return null
  }

  return query
    .trim()
    .replace(/\s+/g, ' ') // Normalize whitespace
    .substring(0, 500) // Limit length to prevent issues
}

/**
 * Finds alternative news sources for a headline using Serper News API
 * @param {string} headline - Article headline to search for
 * @param {Object} options - Search options
 * @returns {Promise<Object>} Search results with success status
 */
export async function findAlternativeSources(headline, options = {}) {
  if (!serperClient) {
    return {
      success: false,
      results: [],
      error: 'Serper API not configured',
    }
  }

  const sanitizedQuery = sanitizeQuery(headline)

  if (!sanitizedQuery) {
    logger.warn({ headline }, 'Invalid headline provided for Serper News search')
    return {
      success: false,
      results: [],
      error: 'Invalid search query',
    }
  }

  const startTime = Date.now()

  return withRetry(async () => {
    apiCallTracker.recordCall('serper_news')

    const requestParams = {
      q: sanitizedQuery,
      num: options.numResults || API_CONFIG.SERPER.DEFAULT_RESULTS,
      ...options.additionalParams,
    }

    const response = await serperClient.post('/news', requestParams)

    const duration = Date.now() - startTime
    const results = response.data.news || []

    logger.debug(
      {
        query: sanitizedQuery,
        results: results.length,
        duration: `${duration}ms`,
      },
      'Serper News search completed'
    )

    return {
      success: true,
      results,
      metadata: {
        query: sanitizedQuery,
        resultCount: results.length,
        duration,
      },
    }
  }, 'Serper News')
}

/**
 * Performs a Google search using Serper API
 * @param {string} query - Search query
 * @param {Object} options - Search options
 * @returns {Promise<Object>} Search results with snippets
 */
export async function performGoogleSearch(query, options = {}) {
  if (!serperClient) {
    return {
      success: false,
      snippets: 'SERPER_API_KEY not configured.',
      error: 'API not configured',
    }
  }

  const sanitizedQuery = sanitizeQuery(query)

  if (!sanitizedQuery) {
    logger.warn({ query }, 'Invalid query provided for Google search')
    return {
      success: false,
      snippets: 'Invalid search query.',
      error: 'Invalid query',
    }
  }

  const startTime = Date.now()

  return withRetry(async () => {
    apiCallTracker.recordCall('serper_search')

    const requestParams = {
      q: sanitizedQuery,
      num: options.numResults || API_CONFIG.SERPER.DEFAULT_RESULTS,
      ...options.additionalParams,
    }

    const response = await serperClient.post('/search', requestParams)

    const organicResults = response.data.organic || []
    const duration = Date.now() - startTime

    if (organicResults.length === 0) {
      logger.debug(
        { query: sanitizedQuery, duration: `${duration}ms` },
        'Google search returned no results'
      )

      return {
        success: false,
        snippets: 'No search results found.',
        results: [],
        metadata: {
          query: sanitizedQuery,
          resultCount: 0,
          duration,
        },
      }
    }

    // Format results into snippets
    const maxResults = options.maxSnippets || API_CONFIG.SERPER.DEFAULT_RESULTS
    const snippets = organicResults
      .slice(0, maxResults)
      .map((res, index) => {
        const title = res.title || 'Untitled'
        const snippet = res.snippet || 'No description available'
        return `${index + 1}. ${title}: ${snippet}`
      })
      .join('\n')

    logger.debug(
      {
        query: sanitizedQuery,
        results: organicResults.length,
        duration: `${duration}ms`,
      },
      'Google search completed'
    )

    return {
      success: true,
      snippets,
      results: organicResults.slice(0, maxResults),
      metadata: {
        query: sanitizedQuery,
        resultCount: organicResults.length,
        duration,
      },
    }
  }, 'Serper Search')
}

/**
 * Finds related articles for an event using NewsAPI
 * @param {string} headline - Event headline to search for
 * @param {Object} options - Search options
 * @returns {Promise<Object>} Related articles with snippets
 */
export async function findNewsApiArticlesForEvent(headline, options = {}) {
  if (!newsapi) {
    return {
      success: false,
      snippets: 'NewsAPI key not configured.',
      error: 'API not configured',
    }
  }

  const sanitizedQuery = sanitizeQuery(headline)

  if (!sanitizedQuery) {
    logger.warn({ headline }, 'Invalid headline provided for NewsAPI search')
    return {
      success: false,
      snippets: 'Invalid search query.',
      error: 'Invalid query',
    }
  }

  const startTime = Date.now()

  return withRetry(
    async () => {
      apiCallTracker.recordCall('newsapi_search')

      const queryParams = {
        q: `"${sanitizedQuery}"`,
        pageSize: options.pageSize || API_CONFIG.NEWSAPI.DEFAULT_PAGE_SIZE,
        sortBy: options.sortBy || 'relevancy',
        language: options.languages || API_CONFIG.NEWSAPI.DEFAULT_LANGUAGES,
        ...options.additionalParams,
      }

      const response = await Promise.race([
        newsapi.v2.everything(queryParams),
        new Promise((_, reject) =>
          setTimeout(
            () => reject(new Error('Request timeout')),
            API_CONFIG.NEWSAPI.TIMEOUT
          )
        ),
      ])

      const duration = Date.now() - startTime
      const articles = response.articles || []

      if (articles.length === 0) {
        logger.debug(
          { query: sanitizedQuery, duration: `${duration}ms` },
          'NewsAPI search returned no results'
        )

        return {
          success: false,
          snippets: 'No related articles found.',
          results: [],
          metadata: {
            query: sanitizedQuery,
            resultCount: 0,
            duration,
          },
        }
      }

      // Format articles into snippets
      const snippets = articles
        .map((article, index) => {
          const title = article.title || 'Untitled'
          const source = article.source?.name || 'Unknown'
          const description = article.description || 'No description'
          return `${index + 1}. ${title} (${source}): ${description}`
        })
        .join('\n')

      logger.debug(
        {
          query: sanitizedQuery,
          results: articles.length,
          duration: `${duration}ms`,
        },
        'NewsAPI search completed'
      )

      return {
        success: true,
        snippets,
        results: articles,
        metadata: {
          query: sanitizedQuery,
          resultCount: articles.length,
          duration,
        },
      }
    },
    'NewsAPI',
    API_CONFIG.NEWSAPI.MAX_RETRIES
  )
}

/**
 * Health check for configured search APIs
 * @returns {Promise<Object>} Status of each API
 */
export async function checkExternalSearchApiHealth() {
  const health = {
    serper: {
      configured: !!serperClient,
      operational: false,
    },
    newsapi: {
      configured: !!newsapi,
      operational: false,
    },
  }

  // Test Serper if configured
  if (serperClient) {
    try {
      const result = await performGoogleSearch('test', { numResults: 1 })
      health.serper.operational = result.success
    } catch (error) {
      logger.debug({ err: error }, 'Serper health check failed')
    }
  }

  // Test NewsAPI if configured
  if (newsapi) {
    try {
      const result = await findNewsApiArticlesForEvent('test', { pageSize: 1 })
      health.newsapi.operational = result.success
    } catch (error) {
      logger.debug({ err: error }, 'NewsAPI health check failed')
    }
  }

  return health
}

/**
 * Get API usage statistics
 * @returns {Object} Usage statistics from apiCallTracker
 */
export function getApiUsageStats() {
  return apiCallTracker.getStats()
}

```

## ðŸ“„ src/search/serpapi.js
*Lines: 432, Size: 9.78 KB*

```javascript
// packages/ai-services/src/search/serpapi.js (version 2.0.0)
import { getJson } from 'serpapi'
import { env } from '@headlines/config'
import { logger } from '@headlines/utils-shared'

/**
 * Configuration for SerpAPI and caching
 */
const SERPAPI_CONFIG = {
  CACHE_TTL: 1000 * 60 * 60, // 1 hour
  CACHE_MAX_SIZE: 1000, // Maximum cache entries
  DEFAULT_LOCATION: 'United States',
  DEFAULT_COUNTRY: 'us',
  DEFAULT_LANGUAGE: 'en',
  MAX_RESULTS: 5,
  REQUEST_TIMEOUT: 30000, // 30 seconds
  MAX_RETRIES: 2,
}

/**
 * In-memory cache with LRU-style eviction
 */
class SearchCache {
  constructor(maxSize = SERPAPI_CONFIG.CACHE_MAX_SIZE, ttl = SERPAPI_CONFIG.CACHE_TTL) {
    this.cache = new Map()
    this.maxSize = maxSize
    this.ttl = ttl
    this.hits = 0
    this.misses = 0
  }

  /**
   * Generates a cache key from query and options
   */
  generateKey(query, options = {}) {
    const normalizedQuery = query.toLowerCase().trim()
    const optionsKey = JSON.stringify({
      location: options.location,
      gl: options.gl,
      hl: options.hl,
    })
    return `serpapi_${normalizedQuery}_${optionsKey}`
  }

  /**
   * Gets a value from cache if valid
   */
  get(key) {
    if (!this.cache.has(key)) {
      this.misses++
      return null
    }

    const cached = this.cache.get(key)

    // Check if expired
    if (Date.now() - cached.timestamp > this.ttl) {
      this.cache.delete(key)
      this.misses++
      return null
    }

    // Move to end (LRU behavior)
    this.cache.delete(key)
    this.cache.set(key, cached)
    this.hits++

    return cached.data
  }

  /**
   * Sets a value in cache with LRU eviction
   */
  set(key, data) {
    // If at capacity, remove oldest entry
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value
      this.cache.delete(firstKey)

      logger.debug(
        { evicted: firstKey, size: this.cache.size },
        'Cache eviction occurred'
      )
    }

    this.cache.set(key, {
      data,
      timestamp: Date.now(),
    })
  }

  /**
   * Clears expired entries from cache
   */
  prune() {
    const now = Date.now()
    let pruned = 0

    for (const [key, value] of this.cache.entries()) {
      if (now - value.timestamp > this.ttl) {
        this.cache.delete(key)
        pruned++
      }
    }

    if (pruned > 0) {
      logger.debug({ pruned, remaining: this.cache.size }, 'Cache pruned expired entries')
    }

    return pruned
  }

  /**
   * Gets cache statistics
   */
  getStats() {
    const total = this.hits + this.misses
    const hitRate = total > 0 ? ((this.hits / total) * 100).toFixed(2) : 0

    return {
      size: this.cache.size,
      maxSize: this.maxSize,
      hits: this.hits,
      misses: this.misses,
      hitRate: `${hitRate}%`,
      ttl: this.ttl,
    }
  }

  /**
   * Clears the entire cache
   */
  clear() {
    const size = this.cache.size
    this.cache.clear()
    this.hits = 0
    this.misses = 0

    logger.info({ cleared: size }, 'Cache cleared')
  }
}

// Initialize cache
const searchCache = new SearchCache()

// Periodic cache pruning (every 15 minutes)
setInterval(
  () => {
    searchCache.prune()
  },
  15 * 60 * 1000
)

/**
 * Validates and sanitizes search query
 * @param {string} query - Raw search query
 * @returns {string|null} Sanitized query or null if invalid
 */
function sanitizeQuery(query) {
  if (!query || typeof query !== 'string') {
    return null
  }

  const sanitized = query
    .trim()
    .replace(/\s+/g, ' ') // Normalize whitespace
    .substring(0, 500) // Limit length

  return sanitized.length > 0 ? sanitized : null
}

/**
 * Normalizes a search result item
 * @param {Object} item - Raw result item from SerpAPI
 * @returns {Object|null} Normalized result or null if invalid
 */
function normalizeResult(item) {
  if (!item) return null

  const title = item.title || item.question || 'Untitled'
  const link = item.link || item.source_link || null
  const snippet = item.snippet || item.answer || item.result || item.description || null

  // Must have at least a snippet and title
  if (!snippet || !title) {
    return null
  }

  return {
    title: title.trim(),
    link,
    snippet: snippet.trim(),
    source: 'Google Search',
    position: item.position || null,
    date: item.date || null,
  }
}

/**
 * Executes SerpAPI search with timeout and retry logic
 * @param {Object} params - Search parameters
 * @param {number} retryCount - Current retry attempt
 * @returns {Promise<Object>} Search response
 */
async function executeSerpApiSearch(params, retryCount = 0) {
  try {
    // Wrap in timeout promise
    const response = await Promise.race([
      getJson(params),
      new Promise((_, reject) =>
        setTimeout(
          () => reject(new Error('Request timeout')),
          SERPAPI_CONFIG.REQUEST_TIMEOUT
        )
      ),
    ])

    return response
  } catch (error) {
    const isRetryable =
      error.code === 'ETIMEDOUT' ||
      error.code === 'ECONNRESET' ||
      error.message?.includes('timeout') ||
      error.message?.includes('network')

    if (isRetryable && retryCount < SERPAPI_CONFIG.MAX_RETRIES) {
      const delay = 1000 * Math.pow(2, retryCount)

      logger.warn(
        {
          attempt: retryCount + 1,
          maxRetries: SERPAPI_CONFIG.MAX_RETRIES,
          delay: `${delay}ms`,
          err: error.message,
        },
        'SerpAPI request failed, retrying...'
      )

      await new Promise((resolve) => setTimeout(resolve, delay))
      return executeSerpApiSearch(params, retryCount + 1)
    }

    throw error
  }
}

/**
 * Performs a Google search using SerpAPI
 * @param {string} query - Search query
 * @param {Object} options - Search options
 * @returns {Promise<Object>} Search results with success status
 */
export async function getGoogleSearchResults(query, options = {}) {
  const startTime = Date.now()

  // Check API key
  if (!env.SERPAPI_API_KEY) {
    logger.warn('SERPAPI_API_KEY is not configured. Skipping web search.')
    return {
      success: true,
      results: [],
      cached: false,
      error: 'API key not configured',
    }
  }

  // Validate query
  const sanitizedQuery = sanitizeQuery(query)

  if (!sanitizedQuery) {
    logger.warn({ query }, 'Invalid query provided for SerpAPI search')
    return {
      success: false,
      error: 'Query is required and must be a non-empty string.',
      results: [],
    }
  }

  // Check cache
  const cacheKey = searchCache.generateKey(sanitizedQuery, options)
  const cachedResult = searchCache.get(cacheKey)

  if (cachedResult) {
    const duration = Date.now() - startTime

    logger.info(
      {
        query: sanitizedQuery,
        duration: `${duration}ms`,
        stats: searchCache.getStats(),
      },
      'SerpAPI cache hit'
    )

    return {
      ...cachedResult,
      cached: true,
    }
  }

  // Perform live search
  logger.info({ query: sanitizedQuery }, 'SerpAPI performing live search')

  try {
    const searchParams = {
      api_key: env.SERPAPI_API_KEY,
      engine: 'google',
      q: sanitizedQuery,
      location: options.location || SERPAPI_CONFIG.DEFAULT_LOCATION,
      gl: options.country || SERPAPI_CONFIG.DEFAULT_COUNTRY,
      hl: options.language || SERPAPI_CONFIG.DEFAULT_LANGUAGE,
      num: options.numResults || SERPAPI_CONFIG.MAX_RESULTS,
      ...options.additionalParams,
    }

    const response = await executeSerpApiSearch(searchParams)

    // Extract and normalize results
    const organicResults = response.organic_results || []
    const answerBox = response.answer_box ? [response.answer_box] : []
    const knowledgeGraph = response.knowledge_graph ? [response.knowledge_graph] : []

    // Combine all result types
    const allResults = [...answerBox, ...knowledgeGraph, ...organicResults]

    const formattedResults = allResults
      .map(normalizeResult)
      .filter(Boolean)
      .slice(0, options.maxResults || SERPAPI_CONFIG.MAX_RESULTS)

    const duration = Date.now() - startTime

    const result = {
      success: true,
      results: formattedResults,
      cached: false,
      metadata: {
        query: sanitizedQuery,
        resultCount: formattedResults.length,
        duration,
        searchInfo: response.search_information || null,
      },
    }

    // Cache successful result
    searchCache.set(cacheKey, result)

    logger.info(
      {
        query: sanitizedQuery,
        results: formattedResults.length,
        duration: `${duration}ms`,
      },
      'SerpAPI search completed successfully'
    )

    return result
  } catch (error) {
    const duration = Date.now() - startTime

    logger.error(
      {
        query: sanitizedQuery,
        err: error.message || error,
        duration: `${duration}ms`,
      },
      'SerpAPI search failed'
    )

    return {
      success: false,
      error: `Failed to fetch search results: ${error.message}`,
      results: [],
      cached: false,
    }
  }
}

/**
 * Gets cache statistics for monitoring
 * @returns {Object} Cache statistics
 */
export function getCacheStats() {
  return searchCache.getStats()
}

/**
 * Clears the search cache
 * @returns {void}
 */
export function clearCache() {
  searchCache.clear()
}

/**
 * Manually prunes expired cache entries
 * @returns {number} Number of entries pruned
 */
export function pruneCache() {
  return searchCache.prune()
}

/**
 * Health check for SerpAPI
 * @returns {Promise<Object>} API health status
 */
export async function checkApiHealth() {
  if (!env.SERPAPI_API_KEY) {
    return {
      configured: false,
      operational: false,
      error: 'API key not configured',
    }
  }

  try {
    const result = await getGoogleSearchResults('test', { numResults: 1 })

    return {
      configured: true,
      operational: result.success,
      error: result.error || null,
    }
  } catch (error) {
    return {
      configured: true,
      operational: false,
      error: error.message,
    }
  }
}

```

## ðŸ“„ src/search/wikipedia.js
*Lines: 643, Size: 15.1 KB*

```javascript
// packages/ai-services/src/search/wikipedia.js
import { logger, apiCallTracker } from '@headlines/utils-shared'
import { settings } from '@headlines/config'
import { disambiguationChain } from '../chains/index.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '@headlines/models/schemas'

/**
 * Wikipedia API configuration
 */
const WIKIPEDIA_CONFIG = {
  API_ENDPOINT: 'https://en.wikipedia.org/w/api.php',
  SUMMARY_LENGTH: 750,
  MAX_SEARCH_RESULTS: 5,
  REQUEST_TIMEOUT: 30000, // 30 seconds
  MAX_RETRIES: 2,
  USER_AGENT: 'HeadlinesBot/1.0 (Educational; https://headlines.app)',
  MIN_SUMMARY_LENGTH: 100, // Minimum viable summary length
  CACHE_TTL: 1000 * 60 * 60 * 24, // 24 hours
  MAX_CACHE_SIZE: 500,
}

/**
 * Simple in-memory cache for Wikipedia results
 */
class WikipediaCache {
  constructor(
    maxSize = WIKIPEDIA_CONFIG.MAX_CACHE_SIZE,
    ttl = WIKIPEDIA_CONFIG.CACHE_TTL
  ) {
    this.cache = new Map()
    this.maxSize = maxSize
    this.ttl = ttl
  }

  generateKey(query) {
    return `wiki_${query.toLowerCase().trim()}`
  }

  get(query) {
    const key = this.generateKey(query)

    if (!this.cache.has(key)) {
      return null
    }

    const cached = this.cache.get(key)

    // Check expiration
    if (Date.now() - cached.timestamp > this.ttl) {
      this.cache.delete(key)
      return null
    }

    return cached.data
  }

  set(query, data) {
    // LRU eviction if at capacity
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value
      this.cache.delete(firstKey)
    }

    const key = this.generateKey(query)
    this.cache.set(key, {
      data,
      timestamp: Date.now(),
    })
  }

  clear() {
    this.cache.clear()
  }

  getStats() {
    return {
      size: this.cache.size,
      maxSize: this.maxSize,
      ttl: this.ttl,
    }
  }
}

const wikipediaCache = new WikipediaCache()

/**
 * Validates and sanitizes Wikipedia query
 * @param {string} query - Raw query
 * @returns {string|null} Sanitized query or null if invalid
 */
function sanitizeQuery(query) {
  if (!query || typeof query !== 'string') {
    return null
  }

  const sanitized = query.trim().replace(/\s+/g, ' ').substring(0, 300) // Limit length

  return sanitized.length > 0 ? sanitized : null
}

/**
 * Fetches a URL with retry logic and exponential backoff
 * @param {string} url - URL to fetch
 * @param {Object} options - Fetch options
 * @param {number} maxRetries - Maximum retry attempts
 * @returns {Promise<Response>} Fetch response
 */
async function fetchWithRetry(
  url,
  options = {},
  maxRetries = WIKIPEDIA_CONFIG.MAX_RETRIES
) {
  const fetchOptions = {
    ...options,
    headers: {
      'User-Agent': WIKIPEDIA_CONFIG.USER_AGENT,
      ...options.headers,
    },
  }

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await Promise.race([
        fetch(url, fetchOptions),
        new Promise((_, reject) =>
          setTimeout(
            () => reject(new Error('Request timeout')),
            WIKIPEDIA_CONFIG.REQUEST_TIMEOUT
          )
        ),
      ])

      if (!response.ok) {
        throw new Error(`Wikipedia API returned status ${response.status}`)
      }

      return response
    } catch (error) {
      const isLastAttempt = attempt === maxRetries
      const isRetryable =
        error.message?.includes('timeout') ||
        error.message?.includes('network') ||
        error.code === 'ETIMEDOUT' ||
        error.code === 'ECONNRESET'

      if (!isRetryable || isLastAttempt) {
        logger.error(
          {
            url,
            attempt,
            err: error.message,
          },
          'Wikipedia fetch failed'
        )
        throw error
      }

      const delay = 1000 * Math.pow(2, attempt - 1)

      logger.warn(
        {
          url,
          attempt,
          maxRetries,
          delay: `${delay}ms`,
          err: error.message,
        },
        'Wikipedia fetch failed, retrying...'
      )

      await new Promise((resolve) => setTimeout(resolve, delay))
    }
  }
}

/**
 * Searches Wikipedia for matching articles
 * @param {string} query - Search query
 * @param {number} limit - Maximum results
 * @returns {Promise<Array>} Search results
 */
async function searchWikipedia(query, limit = WIKIPEDIA_CONFIG.MAX_SEARCH_RESULTS) {
  const searchParams = new URLSearchParams({
    action: 'query',
    list: 'search',
    srsearch: query,
    srlimit: String(limit),
    format: 'json',
    origin: '*',
  })

  const url = `${WIKIPEDIA_CONFIG.API_ENDPOINT}?${searchParams.toString()}`

  const response = await fetchWithRetry(url)
  const data = await response.json()

  const results = data.query?.search || []

  if (results.length === 0) {
    throw new Error(`No Wikipedia search results found for "${query}"`)
  }

  return results
}

/**
 * Fetches article summary from Wikipedia
 * @param {string} title - Article title
 * @returns {Promise<Object>} Article summary and metadata
 */
async function fetchArticleSummary(title) {
  const summaryParams = new URLSearchParams({
    action: 'query',
    prop: 'extracts|info|pageimages',
    exintro: 'true',
    explaintext: 'true',
    titles: title,
    format: 'json',
    redirects: '1',
    inprop: 'url',
    piprop: 'thumbnail',
    pithumbsize: '300',
    origin: '*',
  })

  const url = `${WIKIPEDIA_CONFIG.API_ENDPOINT}?${summaryParams.toString()}`

  const response = await fetchWithRetry(url)
  const data = await response.json()

  const pages = data.query?.pages

  if (!pages) {
    throw new Error(`No page data returned for "${title}"`)
  }

  const pageId = Object.keys(pages)[0]
  const page = pages[pageId]

  // Check for missing page
  if (pageId === '-1' || page.missing) {
    throw new Error(`Wikipedia page not found for "${title}"`)
  }

  const summary = page.extract

  if (!summary) {
    throw new Error(`Could not extract summary for page "${title}"`)
  }

  return {
    title: page.title,
    summary,
    url: page.fullurl || `https://en.wikipedia.org/wiki/${encodeURIComponent(title)}`,
    thumbnail: page.thumbnail?.source || null,
    pageId: page.pageid,
  }
}

/**
 * Uses AI to disambiguate between multiple Wikipedia search results
 * @param {string} query - Original query
 * @param {Array} searchResults - Search results to disambiguate
 * @returns {Promise<string|null>} Best matching title or null
 */
async function disambiguateResults(query, searchResults) {
  if (searchResults.length === 1) {
    return searchResults[0].title
  }

  try {
    const userContent = `Original Query: "${query}"\n\nSearch Results:\n${JSON.stringify(
      searchResults.map((r) => ({
        title: r.title,
        snippet: r.snippet?.replace(/<[^>]*>/g, '') || '', // Strip HTML tags
      }))
    )}`

    logger.debug(
      {
        query,
        candidates: searchResults.length,
      },
      'Attempting AI disambiguation'
    )

    const disambiguationResponse = await disambiguationChain({ inputText: userContent })

    if (
      disambiguationResponse &&
      !disambiguationResponse.error &&
      disambiguationResponse.best_title
    ) {
      logger.info(
        {
          query,
          selected: disambiguationResponse.best_title,
          confidence: disambiguationResponse.confidence || 'unknown',
        },
        'AI disambiguation successful'
      )

      return disambiguationResponse.best_title
    }

    logger.debug({ query }, 'AI disambiguation returned no result')
    return null
  } catch (error) {
    logger.warn(
      {
        err: error.message,
        query,
      },
      'AI disambiguation failed'
    )
    return null
  }
}

/**
 * Fetches Wikipedia summary with intelligent disambiguation
 * @param {string} query - Search query
 * @param {Object} options - Fetch options
 * @returns {Promise<Object>} Summary result with metadata
 */
export async function fetchWikipediaSummary(query, options = {}) {
  const startTime = Date.now()

  // Validate query
  const sanitizedQuery = sanitizeQuery(query)

  if (!sanitizedQuery) {
    logger.warn({ query }, 'Invalid Wikipedia query provided')
    return {
      success: false,
      error: 'Query cannot be empty or invalid.',
      query,
    }
  }

  // Check cache
  const cached = wikipediaCache.get(sanitizedQuery)

  if (cached && !options.skipCache) {
    const duration = Date.now() - startTime

    logger.debug(
      {
        query: sanitizedQuery,
        duration: `${duration}ms`,
      },
      'Wikipedia cache hit'
    )

    return {
      ...cached,
      cached: true,
    }
  }

  try {
    apiCallTracker.recordCall('wikipedia')

    // 1. Search for articles
    const searchResults = await searchWikipedia(
      sanitizedQuery,
      options.searchLimit || WIKIPEDIA_CONFIG.MAX_SEARCH_RESULTS
    )

    logger.debug(
      {
        query: sanitizedQuery,
        results: searchResults.length,
      },
      'Wikipedia search completed'
    )

    // 2. Disambiguate (if needed)
    let bestTitle = null

    if (options.useDisambiguation !== false) {
      bestTitle = await disambiguateResults(sanitizedQuery, searchResults)
    }

    // 3. Fallback to first result if disambiguation fails
    if (!bestTitle) {
      bestTitle = searchResults[0].title

      logger.info(
        {
          query: sanitizedQuery,
          fallback: bestTitle,
        },
        'Using top search result (disambiguation unavailable or failed)'
      )
    }

    // 4. Fetch article summary
    const article = await fetchArticleSummary(bestTitle)

    // 5. Validate content quality
    const validation = await validateWikipediaContent(article.summary)

    if (!validation.valid) {
      logger.warn(
        {
          query: sanitizedQuery,
          title: bestTitle,
          reason: validation.reason,
        },
        'Wikipedia content validation failed'
      )

      // Try second result if available
      if (searchResults.length > 1) {
        logger.info('Attempting second search result...')
        const secondTitle = searchResults[1].title
        const secondArticle = await fetchArticleSummary(secondTitle)
        const secondValidation = await validateWikipediaContent(secondArticle.summary)

        if (secondValidation.valid) {
          Object.assign(article, secondArticle)
        }
      }
    }

    // 6. Truncate summary if needed
    const summaryLength = options.summaryLength || WIKIPEDIA_CONFIG.SUMMARY_LENGTH
    const conciseSummary =
      article.summary.length > summaryLength
        ? article.summary.substring(0, summaryLength) + '...'
        : article.summary

    const duration = Date.now() - startTime

    const result = {
      success: true,
      summary: conciseSummary,
      fullSummary: article.summary,
      title: article.title,
      url: article.url,
      thumbnail: article.thumbnail,
      query: sanitizedQuery,
      quality: validation.quality,
      cached: false,
      metadata: {
        duration,
        pageId: article.pageId,
        searchResults: searchResults.length,
        disambiguated: !!options.useDisambiguation,
      },
    }

    // Cache successful result
    wikipediaCache.set(sanitizedQuery, result)

    logger.info(
      {
        query: sanitizedQuery,
        title: article.title,
        summaryLength: conciseSummary.length,
        duration: `${duration}ms`,
      },
      'Wikipedia summary fetched successfully'
    )

    return result
  } catch (error) {
    const duration = Date.now() - startTime

    logger.warn(
      {
        query: sanitizedQuery,
        err: error.message,
        duration: `${duration}ms`,
      },
      'Wikipedia lookup failed'
    )

    return {
      success: false,
      error: error.message,
      query: sanitizedQuery,
      cached: false,
    }
  }
}

/**
 * Fetches Wikipedia summaries for multiple queries in parallel
 * @param {string[]} queries - Array of search queries
 * @param {Object} options - Fetch options
 * @returns {Promise<Array>} Array of summary results
 */
export async function fetchBatchWikipediaSummaries(queries, options = {}) {
  if (!Array.isArray(queries) || queries.length === 0) {
    logger.warn('Invalid or empty queries array provided for batch fetch')
    return []
  }

  const startTime = Date.now()

  logger.info({ count: queries.length }, 'Starting batch Wikipedia fetch')

  const promises = queries.map((query) =>
    fetchWikipediaSummary(query, options).catch((error) => ({
      success: false,
      error: error.message,
      query,
    }))
  )

  const results = await Promise.all(promises)

  const duration = Date.now() - startTime
  const successful = results.filter((r) => r.success).length
  const failed = results.length - successful

  logger.info(
    {
      total: results.length,
      successful,
      failed,
      duration: `${duration}ms`,
    },
    'Batch Wikipedia fetch completed'
  )

  return results
}

/**
 * Validates Wikipedia content quality
 * @param {string} text - Wikipedia article text
 * @returns {Promise<Object>} Validation result
 */
export async function validateWikipediaContent(text) {
  if (!text || typeof text !== 'string') {
    return {
      valid: false,
      quality: 'low',
      reason: 'Content is empty or invalid',
    }
  }

  const lowerText = text.toLowerCase()

  // Check for disambiguation pages
  const isDisambiguation =
    lowerText.includes('may refer to:') ||
    lowerText.includes('is a list of') ||
    lowerText.includes('disambiguation)')

  if (isDisambiguation) {
    return {
      valid: false,
      quality: 'low',
      reason: 'Disambiguation page content detected',
    }
  }

  // Check for list pages (often low quality for summaries)
  const isListPage =
    lowerText.startsWith('this is a list') || lowerText.startsWith('list of')

  if (isListPage) {
    return {
      valid: false,
      quality: 'low',
      reason: 'List page content detected',
    }
  }

  // Check minimum length
  if (text.length < WIKIPEDIA_CONFIG.MIN_SUMMARY_LENGTH) {
    return {
      valid: false,
      quality: 'low',
      reason: `Summary too short (${text.length} chars, minimum ${WIKIPEDIA_CONFIG.MIN_SUMMARY_LENGTH})`,
    }
  }

  // Check for stub articles
  const isStub = lowerText.includes('stub') && text.length < 500

  if (isStub) {
    return {
      valid: true,
      quality: 'medium',
      reason: 'Stub article detected',
    }
  }

  return {
    valid: true,
    quality: 'high',
    reason: 'Content appears to be a valid, substantive summary',
  }
}

/**
 * Gets cache statistics
 * @returns {Object} Cache statistics
 */
export function getWikipediaCacheStats() {
  return wikipediaCache.getStats()
}

/**
 * Clears the Wikipedia cache
 * @returns {void}
 */
export function clearCache() {
  wikipediaCache.clear()
  logger.info('Wikipedia cache cleared')
}

/**
 * Health check for Wikipedia API
 * @returns {Promise<Object>} API health status
 */
export async function checkWikipediaApiHealth() {
  try {
    const result = await fetchWikipediaSummary('Wikipedia', {
      skipCache: true,
      useDisambiguation: false,
    })

    return {
      operational: result.success,
      error: result.error || null,
      responseTime: result.metadata?.duration || null,
    }
  } catch (error) {
    return {
      operational: false,
      error: error.message,
      responseTime: null,
    }
  }
}

```

## ðŸ“„ src/shared/agents/contactAgent.js
*Lines: 54, Size: 1.71 KB*

```javascript
// packages/ai-services/src/shared/agents/contactAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { findContactSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionContacts } from '@headlines/prompts'
import { performGoogleSearch } from '../../search/search.js'

const getFinderAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionContacts,
    zodSchema: findContactSchema,
  })

export async function findContactDetails(person) {
  const contactFinderAgent = getFinderAgent()
  logger.info(`[Contact Research Agent] Initiated for: ${person.reachOutTo}`)

  const company = person.contactDetails?.company || ''
  const queries = [
    `"${person.reachOutTo}" ${company} email address`,
    `"${person.reachOutTo}" contact information`,
  ]

  let combinedSnippets = ''
  for (const query of queries) {
    const searchResult = await performGoogleSearch(query)
    if (searchResult.success && searchResult.snippets) {
      combinedSnippets += `\n--- Results for query: "${query}" ---\n${searchResult.snippets}`
    }
  }

  if (!combinedSnippets) {
    logger.warn(`[Contact Research Agent] No search results for "${person.reachOutTo}".`)
    return { email: null }
  }

  const response = await contactFinderAgent.execute(combinedSnippets)

  if (response.error || !response.email) {
    logger.warn(
      `[Contact Research Agent] LLM failed to extract details for "${person.reachOutTo}".`
    )
    return { email: null }
  }

  logger.info(
    { details: response },
    `[Contact Research Agent] Found details for "${person.reachOutTo}".`
  )
  return response
}

```

## ðŸ“„ src/shared/agents/emailAgents.js
*Lines: 77, Size: 2.68 KB*

```javascript
// packages/ai-services/src/shared/agents/emailAgents.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { emailSubjectSchema, emailIntroSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionEmailSubject, instructionEmailIntro } from '@headlines/prompts'

const getAgent = (systemPrompt, zodSchema) =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt,
    zodSchema,
  })

export async function generateEmailSubjectLine(events) {
  const subjectLineAgent = getAgent(instructionEmailSubject, emailSubjectSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const response = await subjectLineAgent.execute(JSON.stringify(eventPayload))
    if (response.error || !response.subject_headline) {
      logger.warn('AI failed to generate a custom email subject line.', response)
      return 'Key Developments' // Fallback
    }
    return response.subject_headline
  } catch (error) {
    logger.error({ err: error }, 'Error in generateEmailSubjectLine')
    return 'Key Developments' // Fallback
  }
}

export async function generatePersonalizedIntro(user, events) {
  const introAgent = getAgent(instructionEmailIntro, emailIntroSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const payload = {
      firstName: user.firstName,
      events: eventPayload,
    }
    const response = await introAgent.execute(JSON.stringify(payload))

    if (response.error || !response.greeting) {
      logger.warn('AI failed to generate a personalized intro.', response)
      return {
        greeting: `Dear ${user.firstName},`,
        body: 'Here are the latest relevant wealth events we have identified for your review.',
        bullets: events
          .slice(0, 2)
          .map(
            (e) =>
              `A key development regarding ${e.synthesized_headline.substring(0, 40)}...`
          ),
        signoff: ['We wish you a fruitful day!', 'The team at Wealth Watch'],
      }
    }
    return response
  } catch (error) {
    logger.error({ err: error }, 'Error in generatePersonalizedIntro')
    return {
      greeting: `Dear ${user.firstName},`,
      body: 'Here are the latest relevant wealth events we have identified for your review.',
      bullets: events
        .slice(0, 2)
        .map(
          (e) =>
            `A key development regarding ${e.synthesized_headline.substring(0, 40)}...`
        ),
      signoff: ['We wish you a fruitful day!', 'The team at Wealth Watch'],
    }
  }
}

```

## ðŸ“„ src/shared/agents/entityAgent.js
*Lines: 67, Size: 2.36 KB*

```javascript
// packages/ai-services/src/shared/agents/entityAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { entitySchema, canonicalizerSchema } from '@headlines/models/schemas'
import { settings } from '@headlines/config/node'
import { instructionEntity, instructionCanonicalizer } from '@headlines/prompts'

const getEntityExtractorAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionEntity,
    zodSchema: entitySchema,
  })

const getEntityCanonicalizerAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_UTILITY,
    systemPrompt: instructionCanonicalizer,
    zodSchema: canonicalizerSchema,
  })

// DEFINITIVE FIX:
// The agent is no longer instantiated at the module level.
// We now export the function that creates it, preventing the side-effect during import.
export const entityCanonicalizerAgent = getEntityCanonicalizerAgent

export async function extractEntities(text) {
  const entityExtractorAgent = getEntityExtractorAgent()
  // Now we call the function to get the agent instance when we need it.
  const canonicalizer = getEntityCanonicalizerAgent()

  if (!text) return []

  try {
    const response = await entityExtractorAgent.execute(`Article Text:\n${text}`)

    if (response.error) {
      throw new Error(response.error)
    }

    const { reasoning, entities } = response
    logger.info(`[Query Planner Agent] Reasoning: ${reasoning}`)
    if (!entities || !Array.isArray(entities)) return []

    const canonicalizationPromises = entities
      .map((entity) => entity.replace(/\s*\(.*\)\s*/g, '').trim())
      .filter(Boolean)
      .map(async (entity) => {
        const canonResponse = await canonicalizer.execute(entity)
        if (canonResponse && !canonResponse.error && canonResponse.canonical_name) {
          logger.trace(`Canonicalized "${entity}" -> "${canonResponse.canonical_name}"`)
          return canonResponse.canonical_name
        }
        return null
      })

    const canonicalEntities = await Promise.all(canonicalizationPromises)
    const uniqueEntities = [...new Set(canonicalEntities.filter(Boolean))]

    logger.info({ entities: uniqueEntities }, `Final list of canonical entities for RAG.`)
    return uniqueEntities
  } catch (error) {
    logger.warn({ err: error }, 'Wikipedia query planning (entity extraction) failed.')
    return []
  }
}

```

## ðŸ“„ src/shared/agents/executiveSummaryAgent.js
*Lines: 36, Size: 1.25 KB*

```javascript
// packages/ai-services/src/agents/executiveSummaryAgent.js
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { executiveSummarySchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings } from '@headlines/config/node'
import { instructionExecutiveSummary } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionExecutiveSummary,
    zodSchema: executiveSummarySchema,
  })

export async function generateExecutiveSummary(judgeVerdict, runStats) {
  const agent = getAgent()
  try {
    const payload = {
      freshHeadlinesFound: runStats.freshHeadlinesFound,
      judgeVerdict: judgeVerdict || { event_judgements: [], opportunity_judgements: [] },
    }

    const response = await agent.execute(JSON.stringify(payload))

    if (response.error || !response.summary) {
      logger.warn('AI failed to generate an executive summary.', response)
      return 'AI failed to generate a summary for this run.'
    }

    return response.summary
  } catch (error) {
    logger.error({ err: error }, 'Error in generateExecutiveSummary')
    return 'An unexpected error occurred while generating the executive summary.'
  }
}

```

## ðŸ“„ src/shared/agents/opportunityAgent.js
*Lines: 70, Size: 2.34 KB*

```javascript
// packages/ai-services/src/agents/opportunityAgent.js
import { truncateString } from '@headlines/utils-shared'
import { logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { opportunitySchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config/node'
import { getInstructionOpportunities } from '@headlines/prompts'

const getOppAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionOpportunities,
    zodSchema: opportunitySchema,
  })

export async function generateOpportunitiesFromEvent(
  synthesizedEvent,
  articlesInCluster
) {
  const opportunityGeneratorAgent = getOppAgent()

  const highestRelevanceArticle = articlesInCluster.reduce((max, current) =>
    (current.relevance_article || 0) > (max.relevance_article || 0) ? current : max
  )

  const fullText = articlesInCluster
    .map((a) => (a.articleContent?.contents || []).join('\n'))
    .join('\n\n')

  const inputText = `
        Synthesized Event Headline: ${synthesizedEvent.synthesized_headline}
        Synthesized Event Summary: ${synthesizedEvent.synthesized_summary}
        Key Individuals already identified: ${JSON.stringify(synthesizedEvent.key_individuals)}
        Source Article Snippets: ${truncateString(fullText, LLM_CONTEXT_MAX_CHARS)}
    `

  const response = await opportunityGeneratorAgent.execute(inputText)

  if (response.error || !response.opportunities) {
    logger.warn(
      { event: synthesizedEvent.synthesized_headline, details: response },
      `Opportunity generation failed.`
    )
    return []
  }

  const validOpportunities = (response.opportunities || []).filter(
    (opp) =>
      opp.likelyMMDollarWealth === null || // Keep opportunities where wealth is unknown
      opp.likelyMMDollarWealth >= settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  )

  const opportunitiesWithSource = validOpportunities.map((opp) => ({
    ...opp,
    event_key: synthesizedEvent.event_key,
    sourceArticleId: highestRelevanceArticle._id,
  }))

  logger.info(
    { details: opportunitiesWithSource },
    `[Opportunity Agent] Generated ${
      opportunitiesWithSource.length
    } opportunity/ies from event "${truncateString(
      synthesizedEvent.synthesized_headline,
      50
    )}"`
  )
  return opportunitiesWithSource
}

```

## ðŸ“„ src/shared/agents/synthesisAgent.js
*Lines: 97, Size: 2.96 KB*

```javascript
// packages/ai-services/src/shared/agents/synthesisAgent.js
import { truncateString, logger } from '@headlines/utils-shared'
import { AIAgent } from '../../lib/AIAgent.js'
import { synthesisSchema } from '@headlines/models/schemas' // CORRECTED PATH
import { settings, LLM_CONTEXT_MAX_CHARS } from '@headlines/config'
import { instructionSynthesize } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: settings.LLM_MODEL_SYNTHESIS,
    systemPrompt: [
      instructionSynthesize.whoYouAre,
      instructionSynthesize.whatYouDo,
      ...instructionSynthesize.guidelines,
      instructionSynthesize.outputFormatDescription,
    ].join('\n\n'),
    zodSchema: synthesisSchema,
  })

export async function synthesizeEvent(
  articlesInCluster,
  historicalContext,
  wikipediaContext,
  newsApiContext
) {
  const eventSynthesizerAgent = getAgent()

  const todayPayload = articlesInCluster.map((a) => ({
    headline: a.headline,
    source: a.newspaper,
    full_text: truncateString(
      (a.articleContent?.contents || []).join('\n'),
      LLM_CONTEXT_MAX_CHARS / (articlesInCluster.length || 1)
    ),
    key_individuals: a.key_individuals || [],
  }))

  const historyPayload = (historicalContext || []).map((h) => ({
    headline: h.headline,
    source: h.newspaper,
    published: h.createdAt,
    summary: h.assessment_article || '',
  }))

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT (Internal Database) ]': historyPayload,
    '[ PUBLIC WIKIPEDIA CONTEXT ]': wikipediaContext || 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': newsApiContext || 'Not available.',
  }

  logger.trace({ synthesis_context: userContent }, '--- SYNTHESIS CONTEXT ---')

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    logger.error('Failed to synthesize event.', { response })
    return { error: 'Synthesis failed' }
  }
  return response
}

export async function synthesizeFromHeadline(article) {
  const eventSynthesizerAgent = getAgent()
  logger.warn(
    { headline: article.headline },
    `Salvaging high-signal headline with failed enrichment...`
  )

  const todayPayload = [
    {
      headline: article.headline,
      source: article.newspaper,
      full_text:
        "NOTE: Full article text could not be retrieved. Synthesize based on the headline's explicit claims and your general knowledge.",
      key_individuals: article.key_individuals || [],
    },
  ]

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT ]': [],
    '[ PUBLIC WIKIPEDIA CONTEXT ]': 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': 'Not available.',
  }

  logger.trace({ synthesis_context: userContent }, '--- SALVAGE SYNTHESIS CONTEXT ---')

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    logger.error('Failed to salvage headline.', { response })
    return null
  }
  return response
}

```
