# 📁 PROJECT DIRECTORY STRUCTURE

Total: 60 files, 7 directories

```
headlines/
├── 📁 src/
│   ├── 📁 chains/
│   │   ├── 📄 articleChain.js
│   │   ├── 📄 articlePreAssessmentChain.js
│   │   ├── 📄 batchHeadlineChain.js
│   │   ├── 📄 clusteringChain.js
│   │   ├── 📄 contactFinderChain.js
│   │   ├── 📄 contactResolverChain.js
│   │   ├── 📄 countryCorrectionChain.js
│   │   ├── 📄 disambiguationChain.js
│   │   ├── 📄 emailIntroChain.js
│   │   ├── 📄 emailSubjectChain.js
│   │   ├── 📄 entityCanonicalizerChain.js
│   │   ├── 📄 entityExtractorChain.js
│   │   ├── 📄 executiveSummaryChain.js
│   │   ├── 📄 headlineChain.js
│   │   ├── 📄 index.js
│   │   ├── 📄 judgeChain.js
│   │   ├── 📄 opportunityChain.js
│   │   ├── 📄 sectionClassifierChain.js
│   │   ├── 📄 selectorRepairChain.js
│   │   ├── 📄 synthesisChain.js
│   │   ├── 📄 translateChain.js
│   │   └── 📄 watchlistSuggestionChain.js
│   ├── 📁 embeddings/
│   │   ├── 📄 embeddings.js
│   │   └── 📄 vectorSearch.js
│   ├── 📁 lib/
│   │   ├── 📄 langchain.js
│   │   └── 📄 safeInvoke.js
│   ├── 📁 rag/
│   │   ├── 📄 generation.js
│   │   ├── 📄 orchestrator.js
│   │   ├── 📄 planner.js
│   │   ├── 📄 prompts.js
│   │   ├── 📄 retrieval.js
│   │   └── 📄 validation.js
│   ├── 📁 schemas/
│   │   ├── 📄 articleAssessmentSchema.js
│   │   ├── 📄 articlePreAssessmentSchema.js
│   │   ├── 📄 batchArticleAssessmentSchema.js
│   │   ├── 📄 batchHeadlineAssessmentSchema.js
│   │   ├── 📄 canonicalizerSchema.js
│   │   ├── 📄 clusterSchema.js
│   │   ├── 📄 countryCorrectionSchema.js
│   │   ├── 📄 disambiguationSchema.js
│   │   ├── 📄 emailIntroSchema.js
│   │   ├── 📄 emailSubjectSchema.js
│   │   ├── 📄 enrichContactSchema.js
│   │   ├── 📄 entitySchema.js
│   │   ├── 📄 executiveSummarySchema.js
│   │   ├── 📄 findContactSchema.js
│   │   ├── 📄 headlineAssessmentSchema.js
│   │   ├── 📄 index.js
│   │   ├── 📄 judgeSchema.js
│   │   ├── 📄 opportunitySchema.js
│   │   ├── 📄 sectionClassifierSchema.js
│   │   ├── 📄 selectorRepairSchema.js
│   │   ├── 📄 synthesisSchema.js
│   │   ├── 📄 translateSchema.js
│   │   └── 📄 watchlistSuggestionSchema.js
│   ├── 📁 search/
│   │   ├── 📄 search.js
│   │   ├── 📄 serpapi.js
│   │   └── 📄 wikipedia.js
│   └── 📄 index.js
└── 📄 package.json
```

# 📋 PROJECT METADATA

**Generated**: 2025-09-21T14:17:28.663Z
**Repository Path**: /home/mark/Repos/projects/headlines/packages/ai-services
**Total Files**: 60
**Package**: @headlines/ai-services@1.0.0
**Description**: Centralized, LangChain-powered AI and external service logic.



---


## 📄 package.json
*Lines: 27, Size: 722 Bytes*

```json
{
  "name": "@headlines/ai-services",
  "version": "1.0.0",
  "description": "Centralized, LangChain-powered AI and external service logic.",
  "main": "src/index.js",
  "type": "module",
  "license": "ISC",
  "dependencies": {
    "@headlines/config": "workspace:*",
    "@headlines/utils-server": "workspace:*",
    "@headlines/models": "workspace:*",
    "@headlines/prompts": "workspace:*",
    "@langchain/community": "*",
    "@langchain/core": "*",
    "@langchain/openai": "*",
    "@langchain/pinecone": "*",
    "@xenova/transformers": "^2.17.2",
    "axios": "^1.7.2",
    "langchain": "*",
    "newsapi": "^2.4.1",
    "p-limit": "^5.0.0",
    "serpapi": "^2.1.0",
    "sharp": "^0.34.4",
    "zod": "*"
  }
}

```

## 📄 src/chains/articleChain.js
*Lines: 64, Size: 2.1 KB*

```javascript
'use server'
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { settings } from '@headlines/config/server'
import {
  getInstructionArticle,
  shotsInputArticle,
  shotsOutputArticle,
} from '@headlines/prompts'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { logger } from '@headlines/utils-server/logger'
import { articleAssessmentSchema } from '../schemas/index.js'

const instructions = getInstructionArticle(settings)
const systemPrompt = [
  instructions.whoYouAre,
  instructions.whatYouDo,
  instructions.primaryMandate,
  instructions.analyticalFramework,
  instructions.scoring,
  instructions.outputFormatDescription,
  instructions.reiteration,
].join('\n\n')

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputArticle.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputArticle[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{article_text}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

async function invoke(input) {
  const result = await safeInvoke(chain, input, 'articleChain', articleAssessmentSchema)
  if (result.error) return result
  if (result.key_individuals?.length > 0) {
    const articleTextLower = input.article_text.toLowerCase()
    result.key_individuals = result.key_individuals.filter((ind) => {
      if (!ind.name) return false
      const isPresent = ind.name
        .split(' ')
        .filter((p) => p.length > 2)
        .some((p) => articleTextLower.includes(p.toLowerCase()))
      if (!isPresent)
        logger.warn({ individual: ind.name }, 'Discarding hallucinated key individual.')
      return isPresent
    })
  }
  return result
}

export const articleChain = { invoke }

```

## 📄 src/chains/articlePreAssessmentChain.js
*Lines: 29, Size: 1.14 KB*

```javascript
// packages/ai-services/src/chains/articlePreAssessmentChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionArticlePreAssessment } from '../../../prompts/src/index.js'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { articlePreAssessmentSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionArticlePreAssessment.whoYouAre,
  instructionArticlePreAssessment.whatYouDo,
  instructionArticlePreAssessment.classificationFramework,
  instructionArticlePreAssessment.outputFormatDescription,
  instructionArticlePreAssessment.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{input}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const articlePreAssessmentChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'articlePreAssessmentChain', articlePreAssessmentSchema),
}

```

## 📄 src/chains/batchHeadlineChain.js
*Lines: 30, Size: 1.24 KB*

```javascript
// packages/ai-services/src/chains/batchHeadlineChain.js (version 1.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionBatchHeadlineAssessment } from '../../../prompts/src/index.js'
import { getHeadlineModel } from '../lib/langchain.js' // Use the specific model for headlines
import { safeInvoke } from '../lib/safeInvoke.js'
import { batchHeadlineAssessmentSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionBatchHeadlineAssessment.whoYouAre,
  instructionBatchHeadlineAssessment.whatYouDo,
  instructionBatchHeadlineAssessment.primaryMandate,
  instructionBatchHeadlineAssessment.analyticalFramework,
  instructionBatchHeadlineAssessment.outputFormatDescription,
  instructionBatchHeadlineAssessment.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{headlines_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHeadlineModel(), new JsonOutputParser()])

export const batchHeadlineChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'batchHeadlineChain', batchHeadlineAssessmentSchema),
}

```

## 📄 src/chains/clusteringChain.js
*Lines: 28, Size: 1.01 KB*

```javascript
// packages/ai-services/src/chains/clusteringChain.js (version 3.2 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCluster } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { clusterSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionCluster.whoYouAre,
  instructionCluster.whatYouDo,
  ...instructionCluster.guidelines,
  instructionCluster.outputFormatDescription,
  instructionCluster.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{articles_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const clusteringChain = {
  invoke: (input) => safeInvoke(chain, input, 'clusteringChain', clusterSchema),
}

```

## 📄 src/chains/contactFinderChain.js
*Lines: 28, Size: 1.02 KB*

```javascript
// packages/ai-services/src/chains/contactFinderChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionContacts } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { findContactSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionContacts.whoYouAre,
  instructionContacts.whatYouDo,
  ...instructionContacts.guidelines,
  instructionContacts.outputFormatDescription,
  instructionContacts.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{snippets}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const contactFinderChain = {
  invoke: (input) => safeInvoke(chain, input, 'contactFinderChain', findContactSchema),
}

```

## 📄 src/chains/contactResolverChain.js
*Lines: 28, Size: 1.03 KB*

```javascript
// packages/ai-services/src/chains/contactResolverChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEnrichContact } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { enrichContactSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEnrichContact.whoYouAre,
  instructionEnrichContact.whatYouDo,
  ...instructionEnrichContact.guidelines,
  instructionEnrichContact.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const contactResolverChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'contactResolverChain', enrichContactSchema),
}

```

## 📄 src/chains/countryCorrectionChain.js
*Lines: 40, Size: 2.07 KB*

```javascript
// packages/ai-services/src/chains/countryCorrectionChain.js
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { countryCorrectionSchema } from '../schemas/index.js'

const systemPrompt = `You are a data cleaning expert. Your sole task is to analyze a given text string that is supposed to represent a country and extract the single, correct, UN-recognized sovereign country name from it.

**CRITICAL INSTRUCTIONS:**
1.  Analyze the input string.
2.  Identify the most likely country. For example, "Denmark (Aarhus)" should be "Denmark". "London" should be "United Kingdom".
4.  Anything starting with "Central Europe" should be "Europe".
5.  "Denmark & Sweden" should be "Scandinavia"
6.  "International" should be "Global"
7. "Nordic Region" should be "Scandinavia" (also if followed by something between brackets)
8. "Pan-Europe" should be "Europe"
9. "Sweden & Norway" should be "Scandinavia"
10. "United States" should be "United States of America"
11. "UK" should be "United Kingdom"
12. anything starting with "Unknown" should simply be "Unknown"
13.  If a valid country name can be determined, return it.
14.  If the input is ambiguous or does not contain a clear country, you MUST return null.
15.  You MUST respond ONLY with a valid JSON object in this format: {{"country": "Correct Country Name"}} or {{"country": null}}`;

// DEFINITIVE FIX: The template variable must match the key used in the input object.
// Langchain expects the input variable to be directly in the template string.
const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Location String: "{location_string}"'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const countryCorrectionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'countryCorrectionChain', countryCorrectionSchema),
}

```

## 📄 src/chains/disambiguationChain.js
*Lines: 28, Size: 1.03 KB*

```javascript
// packages/ai-services/src/chains/disambiguationChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionDisambiguation } from '../../../prompts/src/index.js'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { disambiguationSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionDisambiguation.whoYouAre,
  instructionDisambiguation.whatYouDo,
  ...instructionDisambiguation.guidelines,
  instructionDisambiguation.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{inputText}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const disambiguationChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'disambiguationChain', disambiguationSchema),
}

```

## 📄 src/chains/emailIntroChain.js
*Lines: 28, Size: 1.06 KB*

```javascript
// packages/ai-services/src/chains/emailIntroChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailIntro } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailIntroSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEmailIntro.whoYouAre,
  instructionEmailIntro.whatYouDo,
  ...instructionEmailIntro.guidelines,
  instructionEmailIntro.outputFormatDescription,
  instructionEmailIntro.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Client and Event Data: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const emailIntroChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailIntroChain', emailIntroSchema),
}

```

## 📄 src/chains/emailSubjectChain.js
*Lines: 28, Size: 1.07 KB*

```javascript
// packages/ai-services/src/chains/emailSubjectChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEmailSubject } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { emailSubjectSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEmailSubject.whoYouAre,
  instructionEmailSubject.whatYouDo,
  ...instructionEmailSubject.guidelines,
  instructionEmailSubject.outputFormatDescription,
  instructionEmailSubject.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const emailSubjectChain = {
  invoke: (input) => safeInvoke(chain, input, 'emailSubjectChain', emailSubjectSchema),
}

```

## 📄 src/chains/entityCanonicalizerChain.js
*Lines: 28, Size: 1.04 KB*

```javascript
// packages/ai-services/src/chains/entityCanonicalizerChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionCanonicalizer } from '../../../prompts/src/index.js'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { canonicalizerSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionCanonicalizer.whoYouAre,
  instructionCanonicalizer.whatYouDo,
  ...instructionCanonicalizer.guidelines,
  instructionCanonicalizer.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{entity_name}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const entityCanonicalizerChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'entityCanonicalizerChain', canonicalizerSchema),
}

```

## 📄 src/chains/entityExtractorChain.js
*Lines: 27, Size: 999 Bytes*

```javascript
// packages/ai-services/src/chains/entityExtractorChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionEntity } from '../../../prompts/src/index.js'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { entitySchema } from '../schemas/index.js'

const systemPrompt = [
  instructionEntity.whoYouAre,
  instructionEntity.whatYouDo,
  ...instructionEntity.guidelines,
  instructionEntity.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{article_text}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const entityExtractorChain = {
  invoke: (input) => safeInvoke(chain, input, 'entityExtractorChain', entitySchema),
}

```

## 📄 src/chains/executiveSummaryChain.js
*Lines: 29, Size: 1.11 KB*

```javascript
// packages/ai-services/src/chains/executiveSummaryChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionExecutiveSummary } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { executiveSummarySchema } from '../schemas/index.js'

const systemPrompt = [
  instructionExecutiveSummary.whoYouAre,
  instructionExecutiveSummary.whatYouDo,
  ...instructionExecutiveSummary.guidelines,
  instructionExecutiveSummary.outputFormatDescription,
  instructionExecutiveSummary.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Run Data: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const executiveSummaryChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'executiveSummaryChain', executiveSummarySchema),
}

```

## 📄 src/chains/headlineChain.js
*Lines: 85, Size: 2.74 KB*

```javascript
// packages/ai-services/src/chains/headlineChain.js (version 4.2.1 - Confirmed Final)
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts'
import { AIMessage, HumanMessage } from '@langchain/core/messages'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import {
  instructionHeadlines,
  shotsInputHeadlines,
  shotsOutputHeadlines,
} from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { headlineAssessmentSchema } from '../schemas/index.js'
import { settings } from '../../../config/src/server.js'

const systemPrompt = [
  instructionHeadlines.whoYouAre,
  instructionHeadlines.whatYouDo,
  instructionHeadlines.primaryMandate,
  instructionHeadlines.analyticalFramework,
  instructionHeadlines.outputFormatDescription,
].join('\n\n')

const messages = [
  SystemMessagePromptTemplate.fromTemplate(systemPrompt),
  ...shotsInputHeadlines.flatMap((input, i) => [
    new HumanMessage(input),
    new AIMessage(shotsOutputHeadlines[i]),
  ]),
  HumanMessagePromptTemplate.fromTemplate('{headlineWithContext}'),
]

const prompt = ChatPromptTemplate.fromMessages(messages)
const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

function prepareInput({ article, hits }) {
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`
  if (hits.length > 0) {
    const hitStrings = hits
      .map(
        (hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`
      )
      .join(' ')
    headlineWithContext = `${hitStrings} ${headlineWithContext}`
  }
  return { headlineWithContext }
}

async function invoke({ article, hits }) {
  const input = prepareInput({ article, hits })
  const result = await safeInvoke(chain, input, 'headlineChain', headlineAssessmentSchema)

  if (result.error) {
    return {
      relevance_headline: 0,
      assessment_headline: 'AI assessment failed.',
      headline_en: article.headline,
    }
  }

  const assessment = result.assessment?.[0]
  if (assessment && hits.length > 0) {
    let score = assessment.relevance_headline
    if (settings.WATCHLIST_SCORE_BOOST > 0) {
      score = Math.min(100, score + settings.WATCHLIST_SCORE_BOOST)
      assessment.assessment_headline = `Watchlist boost (+${settings.WATCHLIST_SCORE_BOOST}). ${assessment.assessment_headline}`
    }
    assessment.relevance_headline = score
  }

  return (
    assessment || {
      relevance_headline: 0,
      assessment_headline: 'AI assessment failed.',
      headline_en: article.headline,
    }
  )
}

export const headlineChain = { invoke }

```

## 📄 src/chains/index.js
*Lines: 47, Size: 2.99 KB*

```javascript
// packages/ai-services/src/chains/index.js (version 3.1 - Final)
import { articleChain as ac } from './articleChain.js'
import { articlePreAssessmentChain as apac } from './articlePreAssessmentChain.js'
import { clusteringChain as cc } from './clusteringChain.js'
import { contactFinderChain as cfc } from './contactFinderChain.js'
import { contactResolverChain as crc } from './contactResolverChain.js'
import { disambiguationChain as dc } from './disambiguationChain.js'
import { emailIntroChain as eic } from './emailIntroChain.js'
import { emailSubjectChain as esc } from './emailSubjectChain.js'
import { entityCanonicalizerChain as ecc } from './entityCanonicalizerChain.js'
import { entityExtractorChain as eec } from './entityExtractorChain.js'
import { executiveSummaryChain as exsc } from './executiveSummaryChain.js'
import { headlineChain as hc } from './headlineChain.js'
import { judgeChain as jc } from './judgeChain.js'
import { opportunityChain as oc } from './opportunityChain.js'
import { sectionClassifierChain as scc } from './sectionClassifierChain.js'
import { selectorRepairChain as src } from './selectorRepairChain.js'
import { synthesisChain as sc } from './synthesisChain.js'
import { watchlistSuggestionChain as wsc } from './watchlistSuggestionChain.js'
import { batchHeadlineChain as bhc } from './batchHeadlineChain.js'
import { translateChain as tc } from './translateChain.js'
import { countryCorrectionChain as ccc } from './countryCorrectionChain.js'

// DEFINITIVE FIX: Export each chain's invoke method as a standalone async function
// to comply with "use server" constraints.
export const articleChain = async (input) => ac.invoke(input)
export const articlePreAssessmentChain = async (input) => apac.invoke(input)
export const clusteringChain = async (input) => cc.invoke(input)
export const contactFinderChain = async (input) => cfc.invoke(input)
export const contactResolverChain = async (input) => crc.invoke(input)
export const disambiguationChain = async (input) => dc.invoke(input)
export const emailIntroChain = async (input) => eic.invoke(input)
export const emailSubjectChain = async (input) => esc.invoke(input)
export const entityCanonicalizerChain = async (input) => ecc.invoke(input)
export const entityExtractorChain = async (input) => eec.invoke(input)
export const executiveSummaryChain = async (input) => exsc.invoke(input)
export const headlineChain = async (input) => hc.invoke(input)
export const judgeChain = async (input) => jc.invoke(input)
export const opportunityChain = async (input) => oc.invoke(input)
export const sectionClassifierChain = async (input) => scc.invoke(input)
export const selectorRepairChain = async (input) => src.invoke(input)
export const synthesisChain = async (input) => sc.invoke(input)
export const watchlistSuggestionChain = async (input) => wsc.invoke(input)
export const batchHeadlineChain = async (input) => bhc.invoke(input)
export const translateChain = async (input) => tc.invoke(input)
export const countryCorrectionChain = async (input) => ccc.invoke(input)

```

## 📄 src/chains/judgeChain.js
*Lines: 28, Size: 1022 Bytes*

```javascript
// packages/ai-services/src/chains/judgeChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionJudge } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { judgeSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionJudge.whoYouAre,
  instructionJudge.whatYouDo,
  ...instructionJudge.guidelines,
  instructionJudge.outputFormatDescription,
  instructionJudge.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Data for review: {payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const judgeChain = {
  invoke: (input) => safeInvoke(chain, input, 'judgeChain', judgeSchema),
}

```

## 📄 src/chains/opportunityChain.js
*Lines: 29, Size: 1.08 KB*

```javascript
// packages/ai-services/src/chains/opportunityChain.js (version 2.4 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getInstructionOpportunities } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { opportunitySchema } from '../schemas/index.js'
import { settings } from '../../../config/src/server.js'

const instructions = getInstructionOpportunities(settings)
const systemPrompt = [
  instructions.whoYouAre,
  instructions.whatYouDo,
  ...instructions.guidelines,
  instructions.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_text}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const opportunityChain = {
  invoke: (input) => safeInvoke(chain, input, 'opportunityChain', opportunitySchema),
}

```

## 📄 src/chains/sectionClassifierChain.js
*Lines: 34, Size: 1.95 KB*

```javascript
// packages/ai-services/src/chains/sectionClassifierChain.js (version 2.3.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { getUtilityModel } from '../lib/langchain.js' // Correctly import the getter function
import { safeInvoke } from '../lib/safeInvoke.js'
import { sectionClassifierSchema } from '../schemas/index.js'

const INSTRUCTION = {
  whoYouAre:
    'You are a master website navigation analyst. Your task is to analyze a list of hyperlinks (anchor text and href) from a webpage and classify each one into one of four categories.',
  guidelines: [
    '**Categories:**',
    '1.  **"news_section"**: A link to a major category or section of news (e.g., "Business", "Technology", "World News", "/erhverv", "/økonomi").',
    '2.  **"article_headline"**: A link to a specific news article or story. The text is usually a full sentence or a descriptive title.',
    '3.  **"navigation"**: A link to a functional page on the site (e.g., "About Us", "Contact", "Login").',
    '4.  **"other"**: Any other type of link (advertisements, privacy policies, etc.).',
    '**Instructions:**',
    '-   You will receive a JSON array of link objects.',
    '-   You MUST return a JSON object with a single key, "classifications".',
    '-   The "classifications" array MUST contain one classification object for EACH link in the input, in the EXACT SAME ORDER.',
  ],
}

const systemPrompt = [INSTRUCTION.whoYouAre, ...INSTRUCTION.guidelines].join('\n\n')
const fullPrompt = `${systemPrompt}\n\nUser Input:\n{links_json_string}`
const prompt = ChatPromptTemplate.fromTemplate(fullPrompt)
const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()]) // Call the getter function

export const sectionClassifierChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'sectionClassifierChain', sectionClassifierSchema),
}

```

## 📄 src/chains/selectorRepairChain.js
*Lines: 29, Size: 1.08 KB*

```javascript
// packages/ai-services/src/chains/selectorRepairChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSelectorRepair } from '../../../prompts/src/index.js'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { selectorRepairSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionSelectorRepair.whoYouAre,
  instructionSelectorRepair.whatYouDo,
  ...instructionSelectorRepair.guidelines,
  instructionSelectorRepair.outputFormatDescription,
  instructionSelectorRepair.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{payload_json_string}'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const selectorRepairChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'selectorRepairChain', selectorRepairSchema),
}

```

## 📄 src/chains/synthesisChain.js
*Lines: 27, Size: 1018 Bytes*

```javascript
// packages/ai-services/src/chains/synthesisChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionSynthesize } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { synthesisSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionSynthesize.whoYouAre,
  instructionSynthesize.whatYouDo,
  ...instructionSynthesize.guidelines,
  instructionSynthesize.outputFormatDescription,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', '{context_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const synthesisChain = {
  invoke: (input) => safeInvoke(chain, input, 'synthesisChain', synthesisSchema),
}

```

## 📄 src/chains/translateChain.js
*Lines: 29, Size: 1.07 KB*

```javascript
// packages/ai-services/src/chains/translateChain.js (version 1.0.0)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionTranslate } from '../../../prompts/src/index.js'
import { getUtilityModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { translateSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionTranslate.whoYouAre,
  instructionTranslate.whatYouDo,
  ...instructionTranslate.guidelines,
  instructionTranslate.outputFormatDescription,
  instructionTranslate.reiteration,
].join('\\n\\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Target Language: {language}\\n\\nHTML Content:\\n```{html_content}```'],
])

const chain = RunnableSequence.from([prompt, getUtilityModel(), new JsonOutputParser()])

export const translateChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'translateChain', translateSchema),
}

```

## 📄 src/chains/watchlistSuggestionChain.js
*Lines: 29, Size: 1.15 KB*

```javascript
// packages/ai-services/src/chains/watchlistSuggestionChain.js (version 2.3 - Final)
import { ChatPromptTemplate } from '@langchain/core/prompts'
import { JsonOutputParser } from '@langchain/core/output_parsers'
import { RunnableSequence } from '@langchain/core/runnables'
import { instructionWatchlistSuggestion } from '../../../prompts/src/index.js'
import { getHighPowerModel } from '../lib/langchain.js'
import { safeInvoke } from '../lib/safeInvoke.js'
import { watchlistSuggestionSchema } from '../schemas/index.js'

const systemPrompt = [
  instructionWatchlistSuggestion.whoYouAre,
  instructionWatchlistSuggestion.whatYouDo,
  ...instructionWatchlistSuggestion.guidelines,
  instructionWatchlistSuggestion.outputFormatDescription,
  instructionWatchlistSuggestion.reiteration,
].join('\n\n')

const prompt = ChatPromptTemplate.fromMessages([
  ['system', systemPrompt],
  ['human', 'Events Data: {events_json_string}'],
])

const chain = RunnableSequence.from([prompt, getHighPowerModel(), new JsonOutputParser()])

export const watchlistSuggestionChain = {
  invoke: (input) =>
    safeInvoke(chain, input, 'watchlistSuggestionChain', watchlistSuggestionSchema),
}

```

## 📄 src/embeddings/embeddings.js
*Lines: 235, Size: 7.98 KB*

```javascript
// src/lib/embeddings.js (Enhanced version with query expansion and caching)
"use server";
import { pipeline } from '@xenova/transformers';

// In-memory cache for embeddings (consider Redis for production)
const embeddingCache = new Map();
const MAX_CACHE_SIZE = 1000;

// Singleton pattern to ensure we only load the model once per server instance
class EmbeddingPipeline {
    static task = 'feature-extraction';
    static model = 'Xenova/all-MiniLM-L6-v2';
    static instance = null;
    
    static async getInstance() {
        if (this.instance === null) {
            const { pipeline } = await import('@xenova/transformers');
            this.instance = await pipeline(this.task, this.model);
        }
        return this.instance;
    }
}

/**
 * Creates a cache key from text
 * @param {string} text 
 * @returns {string}
 */
function createCacheKey(text) {
    return `embed_${text.toLowerCase().trim().replace(/\s+/g, '_')}`;
}

/**
 * Manages cache size to prevent memory bloat
 */
function manageCacheSize() {
    if (embeddingCache.size >= MAX_CACHE_SIZE) {
        // Remove oldest 20% of entries (FIFO-ish)
        const keysToRemove = Array.from(embeddingCache.keys()).slice(0, Math.floor(MAX_CACHE_SIZE * 0.2));
        keysToRemove.forEach(key => embeddingCache.delete(key));
        console.log(`[Embedding Cache] Cleaned ${keysToRemove.length} entries`);
    }
}

/**
 * Generates an embedding for a given text with caching
 * @param {string} text The text to embed
 * @returns {Promise<Array<number>>} A promise that resolves to the embedding vector
 */
export async function generateEmbedding(text) {
    if (!text || text.trim().length === 0) {
        throw new Error('Text cannot be empty for embedding generation');
    }
    
    const cleanText = text.trim();
    const cacheKey = createCacheKey(cleanText);
    
    // Check cache first
    if (embeddingCache.has(cacheKey)) {
        console.log(`[Embedding Cache] Hit for text: "${cleanText.substring(0, 50)}..."`);
        return embeddingCache.get(cacheKey);
    }
    
    try {
        const extractor = await EmbeddingPipeline.getInstance();
        const output = await extractor(cleanText, { pooling: 'mean', normalize: true });
        const embedding = Array.from(output.data);
        
        // Cache the result
        manageCacheSize();
        embeddingCache.set(cacheKey, embedding);
        
        console.log(`[Embedding] Generated embedding for text: "${cleanText.substring(0, 50)}..." (${embedding.length} dimensions)`);
        return embedding;
        
    } catch (error) {
        console.error(`[Embedding Error] Failed to generate embedding: ${error.message}`);
        throw new Error(`Failed to generate embedding: ${error.message}`);
    }
}

/**
 * Generates multiple query variations to improve RAG recall
 * @param {string} originalQuery 
 * @returns {Promise<Array<Array<number>>>} Array of embeddings for different query variations
 */
export async function generateQueryEmbeddings(originalQuery) {
    const variations = generateQueryVariations(originalQuery);
    const embeddingPromises = variations.map(query => generateEmbedding(query));
    
    try {
        const embeddings = await Promise.all(embeddingPromises);
        console.log(`[Query Expansion] Generated ${embeddings.length} query variations for: "${originalQuery}" ->`, variations);
        return embeddings;
    } catch (error) {
        console.error(`[Query Expansion Error] ${error.message}`);
        // Fallback to original query only
        return [await generateEmbedding(originalQuery)];
    }
}

/**
 * Creates query variations to improve semantic search recall
 * @param {string} query 
 * @returns {Array<string>}
 */
function generateQueryVariations(query) {
    const originalQuery = query.trim();
    const variations = new Set([originalQuery]);

    // CORRECTED: Smartly strip disambiguation tags for broader searches
    const coreEntity = originalQuery.replace(/\s*\((company|person)\)$/, '').trim();
    if (coreEntity !== originalQuery) {
        variations.add(coreEntity);
    }

    // Pattern for "Who founded X?"
    const हूंFounderMatch = coreEntity.toLowerCase().match(/^(?:who|what)\s+(?:is|was|founded|created)\s+(.+)/);
    if ( हूंFounderMatch) {
        let subject = हूंFounderMatch[1].replace(/\?/g, '').replace(/^(the|a|an)\s/,'').trim();
        variations.add(subject);
        variations.add(`${subject} founder`);
        variations.add(`founder of ${subject}`);
        variations.add(`${subject} history`);
    } else {
        // General question pattern
        const questionMatch = coreEntity.toLowerCase().match(/^(who|what|when|where|why|how)\s(is|are|was|were|did|does|do)\s(.+)/);
        if (questionMatch) {
            let subject = questionMatch[3].replace(/\?/g, '').trim();
            variations.add(subject);
            
            const simplified = subject.replace(/^(the|a|an)\s/,'').split(' of ');
            if (simplified.length > 1) {
                variations.add(`${simplified[1].trim()} ${simplified[0].trim()}`);
            }
        }
    }
    
    // Add generic variations for the core entity
    if (hasProperNouns(coreEntity)) {
        variations.add(`${coreEntity} background details`);
        variations.add(`Information about ${coreEntity}`);
    }
    
    // Return the top 4 most distinct variations
    return Array.from(variations).slice(0, 4);
}


/**
 * Simple check for proper nouns (capitalized words not at the start of a sentence)
 * @param {string} text 
 * @returns {boolean}
 */
function hasProperNouns(text) {
    // Looks for words starting with an uppercase letter
    return /\b[A-Z][a-z]+/.test(text);
}

/**
 * Batch embedding generation for efficiency
 * @param {Array<string>} texts 
 * @returns {Promise<Array<Array<number>>>}
 */
export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) {
        return [];
    }
    
    const embeddings = [];
    const extractor = await EmbeddingPipeline.getInstance();
    
    // Process in batches to avoid memory issues
    const BATCH_SIZE = 10;
    for (let i = 0; i < texts.length; i += BATCH_SIZE) {
        const batch = texts.slice(i, i + BATCH_SIZE);
        const batchPromises = batch.map(text => {
            const cacheKey = createCacheKey(text);
            if (embeddingCache.has(cacheKey)) {
                return Promise.resolve(embeddingCache.get(cacheKey));
            }
            return extractor(text, { pooling: 'mean', normalize: true })
                .then(output => {
                    const embedding = Array.from(output.data);
                    embeddingCache.set(cacheKey, embedding);
                    return embedding;
                });
        });
        
        const batchEmbeddings = await Promise.all(batchPromises);
        embeddings.push(...batchEmbeddings);
        
        console.log(`[Batch Embedding] Processed batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(texts.length/BATCH_SIZE)}`);
    }
    
    return embeddings;
}

/**
 * Calculate cosine similarity between two embeddings
 * @param {Array<number>} embedding1 
 * @param {Array<number>} embedding2 
 * @returns {Promise<number>} Similarity score between 0 and 1
 */
export async function calculateSimilarity(embedding1, embedding2) {
    if (embedding1.length !== embedding2.length) {
        throw new Error('Embeddings must have the same dimensions');
    }
    
    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;
    
    for (let i = 0; i < embedding1.length; i++) {
        dotProduct += embedding1[i] * embedding2[i];
        norm1 += embedding1[i] * embedding1[i];
        norm2 += embedding2[i] * embedding2[i];
    }
    
    if (norm1 === 0 || norm2 === 0) return 0;
    
    return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
}

/**
 * Get cache statistics for monitoring
 * @returns {Promise<Object>}
 */
export async function getCacheStats() {
    return {
        size: embeddingCache.size,
        maxSize: MAX_CACHE_SIZE,
        utilizationPercent: Math.round((embeddingCache.size / MAX_CACHE_SIZE) * 100)
    };
}
```

## 📄 src/embeddings/vectorSearch.js
*Lines: 70, Size: 2.3 KB*

```javascript
// packages/ai-services/src/vectorSearch.js (version 1.1.0)
'use server'
import { Pinecone } from '@pinecone-database/pinecone'
import { logger } from '@headlines/utils-server'
import { generateEmbedding } from './embeddings.js'
import { env } from '@headlines/config/server'

const { PINECONE_API_KEY, PINECONE_INDEX_NAME } = env

const SIMILARITY_THRESHOLD = 0.65
const MAX_CONTEXT_ARTICLES = 3

let pineconeIndex
if (PINECONE_API_KEY) {
  const pc = new Pinecone({ apiKey: PINECONE_API_KEY })
  pineconeIndex = pc.index(PINECONE_INDEX_NAME)
} else {
  logger.warn(
    'Pinecone API Key not found. RAG/vector search functionality will be disabled.'
  )
}

/**
 * Finds historical articles similar to a given query text by querying Pinecone.
 * @param {string} queryText - The text to search for (e.g., a headline or comma-separated entities).
 * @returns {Promise<Array<Object>>} A promise that resolves to an array of relevant historical articles.
 */
export async function findSimilarArticles(queryText) {
  if (!pineconeIndex) return []
  logger.info('RAG: Searching for historical context in Pinecone...')
  if (!queryText || typeof queryText !== 'string' || queryText.trim().length === 0)
    return []

  try {
    const queryEmbedding = await generateEmbedding(queryText)

    const queryResponse = await pineconeIndex.query({
      topK: MAX_CONTEXT_ARTICLES,
      vector: queryEmbedding,
      includeMetadata: true,
    })

    const relevantMatches = queryResponse.matches.filter(
      (match) => match.score >= SIMILARITY_THRESHOLD
    )

    if (relevantMatches.length > 0) {
      const retrievedArticlesForLogging = relevantMatches
        .map(
          (match) => `  - [Score: ${match.score.toFixed(3)}] "${match.metadata.headline}"`
        )
        .join('\n')
      logger.info(
        `RAG: Found ${relevantMatches.length} relevant historical articles:\n${retrievedArticlesForLogging}`
      )
      return relevantMatches.map((match) => ({
        headline: match.metadata.headline,
        newspaper: match.metadata.newspaper,
        assessment_article: match.metadata.summary,
      }))
    } else {
      logger.info('RAG: Found no relevant historical articles in Pinecone.')
      return []
    }
  } catch (error) {
    logger.error({ err: error }, 'RAG: Pinecone query or embedding generation failed.')
    return []
  }
}

```

## 📄 src/index.js
*Lines: 75, Size: 2.19 KB*

```javascript
import { callLanguageModel } from './lib/langchain.js'
import * as chains from './chains/index.js'
import * as search from './search/search.js'
import * as wikipedia from './search/wikipedia.js'
import * as embeddings from './embeddings/embeddings.js'
import * as vectorSearch from './embeddings/vectorSearch.js'
import { logger } from '@headlines/utils-server'
import { processChatRequest } from './rag/orchestrator.js'
export async function performAiSanityCheck(settings) {
  try {
    logger.info('🔬 Performing AI service sanity check (OpenAI)...')
    const answer = await callLanguageModel({
      modelName: settings.LLM_MODEL_UTILITY,
      prompt: 'What is in one word the name of the capital of France',
      isJson: false,
    })
    if (
      answer &&
      typeof answer === 'string' &&
      answer.trim().toLowerCase().includes('paris')
    ) {
      logger.info('✅ AI service sanity check passed.')
      return true
    } else {
      logger.fatal(
        { details: { expected: 'paris', received: answer } },
        `OpenAI sanity check failed.`
      )
      return false
    }
  } catch (error) {
    if (error.status === 401 || error.message?.includes('Incorrect API key')) {
      logger.fatal(`OpenAI sanity check failed due to INVALID API KEY (401).`)
    } else {
      logger.fatal(
        { err: error },
        'OpenAI sanity check failed with an unexpected API error.'
      )
    }
    return false
  }
}
export { processChatRequest, callLanguageModel }
export const {
  articleChain,
  articlePreAssessmentChain,
  clusteringChain,
  contactFinderChain,
  contactResolverChain,
  disambiguationChain,
  emailIntroChain,
  emailSubjectChain,
  entityCanonicalizerChain,
  entityExtractorChain,
  executiveSummaryChain,
  headlineChain,
  batchHeadlineChain,
  judgeChain,
  opportunityChain,
  sectionClassifierChain,
  selectorRepairChain,
  synthesisChain,
  watchlistSuggestionChain,
  translateChain,
  countryCorrectionChain,
} = chains
export const {
  findAlternativeSources,
  performGoogleSearch,
  findNewsApiArticlesForEvent,
} = search
export const { fetchWikipediaSummary } = wikipedia
export const { generateEmbedding } = embeddings
export const { findSimilarArticles } = vectorSearch

```

## 📄 src/lib/langchain.js
*Lines: 79, Size: 2.56 KB*

```javascript
'use server'
import { ChatOpenAI } from '@langchain/openai'
import { env, settings } from '@headlines/config/server'
import { logger } from '@headlines/utils-server/logger'
import { tokenTracker } from '@headlines/utils-server/tokenTracker'
import { safeExecute } from '@headlines/utils-server/helpers'
import OpenAI from 'openai'

const modelConfig = { response_format: { type: 'json_object' } }
export const getHeadlineModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_HEADLINE_ASSESSMENT }).bind(modelConfig)
export const getHighPowerModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_ARTICLE_ASSESSMENT }).bind(modelConfig)
export const getUtilityModel = () =>
  new ChatOpenAI({ modelName: settings.LLM_MODEL_UTILITY }).bind(modelConfig)

const baseClient = new OpenAI({
  apiKey: env.OPENAI_API_KEY,
  timeout: 120 * 1000,
  maxRetries: 3,
})

export async function callLanguageModel({
  modelName,
  prompt,
  systemPrompt,
  userContent,
  isJson = true,
  fewShotInputs = [],
  fewShotOutputs = [],
  temperature = 0.0,
}) {
  const messages = []
  if (systemPrompt) {
    const systemContent =
      typeof systemPrompt === 'object' ? JSON.stringify(systemPrompt) : systemPrompt
    messages.push({ role: 'system', content: systemContent })
  }
  fewShotInputs.forEach((input, i) => {
    const shotContent = typeof input === 'string' ? input : JSON.stringify(input)
    if (shotContent) {
      messages.push({ role: 'user', content: shotContent })
      messages.push({ role: 'assistant', content: fewShotOutputs[i] })
    }
  })
  const finalUserContent = userContent || prompt
  messages.push({ role: 'user', content: finalUserContent })
  logger.trace(
    { payload: { model: modelName, messages_count: messages.length } },
    'Sending payload to LLM.'
  )
  const result = await safeExecute(() =>
    baseClient.chat.completions.create({
      model: modelName,
      messages: messages,
      temperature,
      response_format: isJson ? { type: 'json_object' } : undefined,
    })
  )
  if (!result) return { error: 'API call failed' }
  if (result.usage) {
    tokenTracker.recordUsage(modelName, result.usage)
  }
  const responseContent = result.choices[0].message.content
  logger.trace({ chars: responseContent.length }, 'Received LLM response.')
  if (isJson) {
    try {
      return JSON.parse(responseContent)
    } catch (parseError) {
      logger.error(
        { err: parseError, details: responseContent },
        `LLM response JSON Parse Error for model ${modelName}`
      )
      return { error: 'JSON Parsing Error' }
    }
  }
  return responseContent
}

```

## 📄 src/lib/safeInvoke.js
*Lines: 82, Size: 2.53 KB*

```javascript
'use server'
import { logger } from '@headlines/utils-server/logger'
import { getRedisClient } from '@headlines/utils-server/redisClient'
import { createHash } from 'crypto'

const MAX_RETRIES = 1
const CACHE_TTL_SECONDS = 60 * 60 * 24
const inMemoryCache = new Map()

function createCacheKey(agentName, input) {
  const hash = createHash('sha256')
  hash.update(JSON.stringify(input))
  return `ai_cache:${agentName}:${hash.digest('hex')}`
}

export async function safeInvoke(chain, input, agentName, zodSchema) {
  const redis = await getRedisClient()
  const cacheKey = createCacheKey(agentName, input)
  if (redis) {
    try {
      const cachedResult = await redis.get(cacheKey)
      if (cachedResult) {
        logger.trace({ agent: agentName }, `[Redis Cache HIT] for ${agentName}.`)
        return JSON.parse(cachedResult)
      }
    } catch (err) {
      logger.error({ err, agent: agentName }, `Redis GET failed for ${agentName}.`)
    }
  } else if (inMemoryCache.has(cacheKey)) {
    logger.trace({ agent: agentName }, `[In-Memory Cache HIT] for ${agentName}.`)
    return inMemoryCache.get(cacheKey)
  }
  for (let attempt = 0; attempt <= MAX_RETRIES; attempt++) {
    try {
      const result = await chain.invoke(input)
      const validation = zodSchema.safeParse(result)
      if (!validation.success) {
        logger.error(
          {
            details: validation.error.flatten(),
            agent: agentName,
            input,
            output: result,
          },
          `Zod validation failed for ${agentName}.`
        )
        throw new Error('Zod validation failed')
      }
      const dataToCache = validation.data
      if (redis) {
        try {
          await redis.set(cacheKey, JSON.stringify(dataToCache), {
            EX: CACHE_TTL_SECONDS,
          })
        } catch (err) {
          logger.error({ err, agent: agentName }, `Redis SET failed for ${agentName}.`)
        }
      } else {
        inMemoryCache.set(cacheKey, dataToCache)
      }
      return dataToCache
    } catch (error) {
      if (
        (error.message.includes('JSON') ||
          error.message.includes('Zod validation failed')) &&
        attempt < MAX_RETRIES
      ) {
        logger.warn(
          { agent: agentName, attempt: attempt + 1 },
          `LLM output validation failed for ${agentName}. Retrying...`
        )
        continue
      }
      logger.error(
        { err: error, agent: agentName },
        `LangChain invocation failed for ${agentName}.`
      )
      return { error: `Agent ${agentName} failed: ${error.message}` }
    }
  }
}

```

## 📄 src/rag/generation.js
*Lines: 128, Size: 4.31 KB*

```javascript
// packages/ai-services/src/lib/rag/generation.js (Corrected for OpenAI)
import { getSynthesizerPrompt } from './prompts'
import { checkGroundedness } from './validation'
// CORRECTED IMPORT: Use the generic callLanguageModel instead of the Groq-specific one.
import { callLanguageModel } from '../lib/langchain.js'

// CORRECTED MODEL: Switched from a Groq-specific model to a powerful OpenAI model.
const SYNTHESIZER_MODEL = 'gpt-4o'

function assembleContext(ragResults, wikiResults, searchResults) {
  const dbContext =
    ragResults.length > 0
      ? ragResults
          .map(
            (match) =>
              `- [Similarity: ${match.score.toFixed(3)}] ${match.metadata.headline}: ${match.metadata.summary}`
          )
          .join('\n')
      : 'None'

  const wikiContext =
    wikiResults.length > 0
      ? wikiResults
          .map(
            (res) => `- [Quality: ${res.validation.quality}] ${res.title}: ${res.summary}`
          )
          .join('\n')
      : 'None'

  const searchContext =
    searchResults.length > 0
      ? searchResults
          .map((res) => `- [${res.title}](${res.link}): ${res.snippet}`)
          .join('\n')
      : 'None'

  return `---
Internal Database Context:
${dbContext}
---
Wikipedia Context:
${wikiContext}
---
Search Results Context:
${searchContext}
---`
}

function formatThoughts(plan, context, groundednessResult) {
  const thoughts = `
**THOUGHT PROCESS: THE PLAN**
${plan.plan.map((step) => `- ${step}`).join('\n')}

**REASONING:**
${plan.reasoning}

**RETRIEVED CONTEXT:**
- **Internal RAG Search:** ${context.ragResults.length} item(s) found.
${context.ragResults.map((r) => `  - [Score: ${r.score.toFixed(2)}] ${r.metadata.headline}`).join('\n')}

- **Wikipedia Search:** ${context.wikiResults.length} article(s) found.
${context.wikiResults.map((w) => `  - **Query:** "${w.query}"\n    - **Result:** ${w.title}: ${w.summary.substring(0, 100)}...`).join('\n')}

- **Web Search:** ${context.searchResults.length} result(s) found.
${context.searchResults.map((s) => `  - **Query:** "${plan.user_query}"\n    - **Result:** ${s.title}: ${s.snippet.substring(0, 100)}...`).join('\n')}

**FINAL CHECK:**
- **Groundedness Passed:** CONFIRMED
`
  return thoughts.trim().replace(/\n\n+/g, '\n\n')
}

export async function generateFinalResponse({ plan, context }) {
  const fullContextString = assembleContext(
    context.ragResults,
    context.wikiResults,
    context.searchResults
  )

  console.log('[RAG Generation] Calling Synthesizer Agent with OpenAI...')
  // CORRECTED CALL: Replaced callGroqWithRetry with the standard callLanguageModel.
  const synthesizerResponse = await callLanguageModel({
    modelName: SYNTHESIZER_MODEL,
    systemPrompt: getSynthesizerPrompt(),
    userContent: `CONTEXT:\n${fullContextString}\n\nPLAN:\n${JSON.stringify(
      plan.plan,
      null,
      2
    )}\n\nUSER'S QUESTION: "${plan.user_query}"`,
    isJson: false, // CRITICAL: The synthesizer prompt returns Markdown, not JSON.
    temperature: 0.1,
  })

  const rawResponse = synthesizerResponse
  if (typeof rawResponse !== 'string') {
    console.error(
      '[RAG Generation] Synthesizer Agent failed to return a valid string response.',
      rawResponse
    )
    return {
      answer: 'The AI synthesizer failed to generate a response.',
      thoughts: 'An error occurred during the final synthesis step.',
    }
  }

  const groundednessResult = await checkGroundedness(rawResponse, fullContextString)
  const thoughts = formatThoughts(plan, context, groundednessResult)

  let finalAnswer
  if (groundednessResult.is_grounded) {
    let responseWithSpans = rawResponse.replace(/<rag>/g, '<span class="rag-source">')
    responseWithSpans = responseWithSpans.replace(/<\/rag>/g, '</span>')
    responseWithSpans = responseWithSpans.replace(/<wiki>/g, '<span class="wiki-source">')
    responseWithSpans = responseWithSpans.replace(/<\/wiki>/g, '</span>')
    responseWithSpans = responseWithSpans.replace(
      /<search>/g,
      '<span class="llm-source">'
    )
    finalAnswer = responseWithSpans.replace(/<\/search>/g, '</span>')
  } else {
    console.warn('[RAG Pipeline] Groundedness check failed. Returning safe response.')
    finalAnswer =
      'I was unable to construct a reliable answer from the available sources. The context may be insufficient or conflicting.'
  }

  return { answer: finalAnswer, thoughts }
}

```

## 📄 src/rag/orchestrator.js
*Lines: 49, Size: 1.78 KB*

```javascript
'use server';

// src/lib/rag/orchestrator.js (version 4.2)
import { retrieveContextForQuery } from './retrieval'
import { assessContextQuality } from './validation'
import { generateFinalResponse } from './generation'
import { runPlannerAgent } from './planner'

/**
 * Main orchestrator for the Agentic RAG chat pipeline.
 * @param {Array<object>} messages - The chat messages from the client.
 * @returns {Promise<string>} The final, validated text response.
 */
export async function processChatRequest(messages) {
  console.log('--- [RAG Pipeline Start] ---')

  // 1. Planning Phase
  console.log('[RAG Pipeline] Step 1: Planning Phase Started...')
  const plan = await runPlannerAgent(messages)
  console.log('[RAG Pipeline] Step 1: Planning Phase Completed.')

  // 2. Retrieval & Validation Phase
  console.log('[RAG Pipeline] Step 2: Retrieval Phase Started...')
  const initialContext = await retrieveContextForQuery(plan, messages, 'ragOnly')
  const initialQuality = assessContextQuality(initialContext.ragResults, [], [])

  let finalContext = initialContext

  if (initialQuality.hasHighConfidenceRAG) {
    console.log('[RAG Pipeline] High confidence RAG hit. Short-circuiting retrieval.')
  } else {
    console.log('[RAG Pipeline] RAG context insufficient. Proceeding to full retrieval.')
    // Perform the remaining retrieval steps
    finalContext = await retrieveContextForQuery(plan, messages, 'full')
  }
  console.log('[RAG Pipeline] Step 2: Retrieval Phase Completed.')

  // 3. Synthesis Phase
  console.log('[RAG Pipeline] Step 3: Synthesis Phase Started...')
  const finalResponse = await generateFinalResponse({
    plan,
    context: finalContext,
  })
  console.log('[RAG Pipeline] Step 3: Synthesis Phase Completed.')

  console.log('--- [RAG Pipeline End] ---')
  return finalResponse
}

```

## 📄 src/rag/planner.js
*Lines: 92, Size: 3.27 KB*

```javascript
// packages/ai-services/src/lib/rag/planner.js (Corrected for OpenAI)
'use server'

// CORRECTED IMPORT: Use the generic callLanguageModel instead of the Groq-specific one.
import { callLanguageModel } from '../lib/langchain.js'
import { PLANNER_PROMPT } from './prompts'

// CORRECTED MODEL: Switched from a Groq-specific model to a powerful OpenAI model.
const PLANNER_MODEL = 'gpt-4o'

/**
 * A resilient parser that attempts to fix common LLM-induced JSON errors.
 * @param {string} jsonString - The potentially malformed JSON string from the LLM.
 * @returns {object} A valid JavaScript object.
 * @throws {Error} If the JSON is irreparable.
 */
function resilientJSONParse(jsonString) {
  try {
    return JSON.parse(jsonString)
  } catch (e) {
    console.warn(
      '[Planner Agent] Initial JSON.parse failed. Attempting cleanup.',
      e.message
    )
    // Attempt to fix common errors, like unescaped quotes in search queries
    let cleanedString = jsonString.replace(/""/g, '"\\"')

    // Find the start and end of the JSON object
    const startIndex = cleanedString.indexOf('{')
    const endIndex = cleanedString.lastIndexOf('}')
    if (startIndex === -1 || endIndex === -1) {
      throw new Error('Could not find a valid JSON object in the string.')
    }
    cleanedString = cleanedString.substring(startIndex, endIndex + 1)

    try {
      return JSON.parse(cleanedString)
    } catch (finalError) {
      console.error('[Planner Agent] Resilient JSON parsing failed.', finalError.message)
      throw new Error('Failed to parse LLM JSON response after cleanup.')
    }
  }
}

/**
 * Runs the Planner Agent to decompose the user's query into a logical plan
 * and a set of optimized search queries.
 * @param {Array<object>} messages - The conversation history.
 * @returns {Promise<object>} An object containing the plan and search queries.
 */
export async function runPlannerAgent(messages) {
  const userQuery = messages[messages.length - 1].content
  const conversationHistory =
    messages.length > 1
      ? messages
          .slice(-5, -1)
          .map((m) => `${m.role}: ${m.content}`)
          .join('\n')
      : 'No history.'

  const prompt = PLANNER_PROMPT.replace(
    '{CONVERSATION_HISTORY}',
    conversationHistory
  ).replace('{USER_QUERY}', userQuery)

  console.log('[Planner Agent] Generating plan with OpenAI...')
  // CORRECTED CALL: Replaced callGroqWithRetry with the standard callLanguageModel.
  const response = await callLanguageModel({
    modelName: PLANNER_MODEL,
    systemPrompt: prompt, // The full prompt is now treated as a system prompt for this agent
    userContent: 'Generate the plan based on the instructions.', // A simple user message to trigger the system prompt
    isJson: true,
    temperature: 0.0,
  })

  // The response from callLanguageModel is already a parsed JSON object or an error object.
  if (response.error) {
    throw new Error(`Planner Agent failed: ${response.error}`)
  }

  const planObject = response // No need for resilientJSONParse

  console.groupCollapsed('[Planner Agent] Plan Generated')
  console.log('User Query:', planObject.user_query)
  console.log('Reasoning:', planObject.reasoning)
  console.log('Plan Steps:', planObject.plan)
  console.log('Search Queries:', planObject.search_queries)
  console.groupEnd()

  return planObject
}

```

## 📄 src/rag/prompts.js
*Lines: 102, Size: 5.82 KB*

```javascript
// src/lib/rag/prompts.js (version 5.5)

export const PLANNER_PROMPT = `You are an expert AI Planner. Your job is to analyze the user's query and conversation history to create a step-by-step plan for an AI Synthesizer Agent to follow. You also create a list of optimized search queries for a Retrieval Agent.

**Conversation History:**
{CONVERSATION_HISTORY}

**Latest User Query:**
"{USER_QUERY}"

**Your Task:**
1.  **Analyze the User's Intent:** Understand what the user is truly asking for.
2.  **Formulate a Plan:** Create a clear, step-by-step plan for the Synthesizer Agent.
3.  **Generate Search Queries:** Create an array of 1-3 optimized, self-contained search queries. **CRITICAL JSON RULE:** If a query within the 'search_queries' array requires double quotes, you MUST escape them with a backslash. For example: ["\\"Troels Holch Povlsen\\" sons", "Bestseller founder"].

**Example 1:**
User Query: "Which Danish Rich List person is involved in Technology?"
History: (empty)
Your JSON Output:
{
  "user_query": "Which Danish Rich List person is involved in Technology?",
  "reasoning": "The user wants a list of wealthy Danes involved in technology. I need to identify these individuals from the context and then filter them based on their tech involvement.",
  "plan": [
    "Scan all context to identify every unique individual mentioned who is on the Danish Rich List.",
    "For each person, look for evidence of direct involvement in the technology sector.",
    "Filter out individuals with no clear connection to technology.",
    "Synthesize the findings into a helpful list of names, citing their connection to technology.",
    "If no one is found, state that clearly."
  ],
  "search_queries": ["Danish Rich List technology involvement", "Wealthy Danish tech investors", "Danish tech company founders"]
}

**Example 2:**
User Query: "Does Troels Holch Povlsen have sons?"
History: (assistant previously mentioned Bestseller's founder)
Your JSON Output:
{
  "user_query": "Does Troels Holch Povlsen have sons?",
  "reasoning": "The user is asking a direct factual question about a specific person's family. The search queries must be precise.",
  "plan": [
      "Scan context for any mention of 'Troels Holch Povlsen' and his family, specifically children or sons.",
      "Extract the names of his sons if mentioned.",
      "Synthesize a complete and helpful answer, stating the names of the sons and any additional relevant context provided."
  ],
  "search_queries": ["\\"Troels Holch Povlsen\\" sons", "\\"Troels Holch Povlsen\\" children", "\\"Bestseller\\" founder family"]
}

Respond ONLY with a valid JSON object with the specified structure.
`

export const getSynthesizerPrompt =
  () => `You are an elite, fact-based intelligence analyst. Your SOLE task is to execute the provided "PLAN" using only the "CONTEXT" to answer the "USER'S QUESTION". You operate under a strict "ZERO HALLUCINATION" protocol. Your response must be confident, direct, and sound like a human expert.

**PRIMARY DIRECTIVE:**
Synthesize information from all sources in the "CONTEXT" into a single, cohesive, and well-written answer. Directly address the user's question and enrich it with relevant surrounding details found in the context.

**EXAMPLE TONE:**
-   **Bad:** "According to the context, Bestseller was founded by Troels Holch Povlsen."
-   **Good:** "Bestseller was founded in 1975 by Troels Holch Povlsen and his wife, Merete Bech Povlsen. The company is now run by their son, Anders Holch Povlsen."

**CRITICAL RULES OF ENGAGEMENT:**
1.  **NO OUTSIDE KNOWLEDGE:** You are forbidden from using any information not present in the provided "CONTEXT".
2.  **DIRECT ATTRIBUTION:** You MUST still cite your sources inline for the UI. Wrap facts from the Internal DB with <rag>tags</rag>, from Wikipedia with <wiki>tags</wiki>, and from Search Results with <search>tags</search>. The user will not see these tags, but they are essential for the system.
3.  **BE CONFIDENT AND DIRECT:** Present the synthesized facts as a definitive answer.
4.  **INSUFFICIENT DATA:** If the context is insufficient to answer the question at all, respond with EXACTLY: "I do not have sufficient information in my sources to answer that question."

**DO NOT:**
-   Use phrases like "According to the context provided...", "The sources state...", or "Based on the information...".
-   Apologize for not knowing or mention your limitations.
-   Talk about your process in the final answer.
-   Speculate or infer beyond what is explicitly stated in the context.

Answer the question directly and authoritatively, as if you are a world-class analyst presenting your verified findings.`

export const GROUNDEDNESS_CHECK_PROMPT = `You are a meticulous fact-checker AI. Your task is to determine if the "Proposed Response" is strictly grounded in the "Provided Context". A response is grounded if and only if ALL of its claims can be directly verified from the context.

**Provided Context:**
---
{CONTEXT}
---

**Proposed Response:**
---
{RESPONSE}
---

Analyze the "Proposed Response" sentence by sentence.

**Respond ONLY with a valid JSON object with the following structure:**
{
  "is_grounded": boolean, // true if ALL claims in the response are supported by the context, otherwise false.
  "unsupported_claims": [
    // List any specific claims from the response that are NOT supported by the context.
    "Claim 1 that is not supported.",
    "Claim 2 that is not supported."
  ]
}

If the response is fully supported, "unsupported_claims" should be an empty array. If the "Proposed Response" states that it cannot answer the question, consider it grounded.`

export const FAILED_GROUNDEDNESS_PROMPT = `I could not form a reliable answer based on the available information. The initial response I generated may have contained information not supported by the sources. For accuracy, please ask a more specific question or try rephradist se your request.`

```

## 📄 src/rag/retrieval.js
*Lines: 182, Size: 5.31 KB*

```javascript
// packages/ai-services/src/rag/retrieval.js
'use server'

import OpenAI from 'openai'
import { Pinecone } from '@pinecone-database/pinecone'
import { generateQueryEmbeddings } from '../embeddings/embeddings.js'
import {
  fetchBatchWikipediaSummaries,
  validateWikipediaContent,
} from '../search/wikipedia.js'
import { getGoogleSearchResults } from '../search/serpapi.js'
import { env } from '@headlines/config/server'

let groq, pineconeIndex
function initializeClients() {
  if (!groq) {
    groq = new OpenAI({
      apiKey: env.GROQ_API_KEY,
      baseURL: 'https://api.groq.com/openai/v1',
    })
    const pc = new Pinecone({ apiKey: env.PINECONE_API_KEY })
    pineconeIndex = pc.index(env.PINECONE_INDEX_NAME)
  }
}

const ENTITY_EXTRACTOR_MODEL = 'llama3-70b-8192'
const SIMILARITY_THRESHOLD = 0.38
const ENTITY_EXTRACTOR_PROMPT_FOR_HISTORY = `You are an entity extractor. Your job is to identify all specific people and companies mentioned in a given text.
Respond ONLY with a valid JSON object with the following structure:
{ "entities": ["Entity Name 1", "Entity Name 2"] }
`

async function extractEntitiesFromHistory(messages) {
  if (messages.length < 2) {
    return []
  }
  const historyText = messages
    .slice(-4)
    .map((m) => m.content)
    .join('\n')

  initializeClients()
  try {
    const entityResponse = await groq.chat.completions.create({
      model: ENTITY_EXTRACTOR_MODEL,
      messages: [
        { role: 'system', content: ENTITY_EXTRACTOR_PROMPT_FOR_HISTORY },
        {
          role: 'user',
          content: `Extract all key people and companies from this text:\n"${historyText}"`,
        },
      ],
      response_format: { type: 'json_object' },
    })

    const { entities } = JSON.parse(entityResponse.choices[0].message.content)
    const cleanEntities = entities.map((e) =>
      e.replace(/\s*\((person|company)\)$/, '').trim()
    )
    if (cleanEntities.length > 0) {
      console.log(
        `[RAG Retrieval] Entities from history for exclusion: ${cleanEntities.join(', ')}`
      )
    }
    return cleanEntities
  } catch (error) {
    console.error('Could not extract entities from history:', error)
    return []
  }
}

async function fetchPineconeContext(queries, exclude_entities = []) {
  initializeClients()
  const queryEmbeddings = await Promise.all(
    queries.map((q) => generateQueryEmbeddings(q))
  )
  const allQueryEmbeddings = queryEmbeddings.flat()

  const filter =
    exclude_entities.length > 0
      ? { key_individuals: { $nin: exclude_entities } }
      : undefined

  if (filter) {
    console.log('[RAG Retrieval] Applying Pinecone filter to exclude:', exclude_entities)
  }

  const pineconePromises = allQueryEmbeddings.map((embedding) =>
    pineconeIndex.query({
      topK: 5,
      vector: embedding,
      includeMetadata: true,
      filter: filter,
    })
  )
  const pineconeResponses = await Promise.all(pineconePromises)

  const uniqueMatches = new Map()
  pineconeResponses.forEach((response) => {
    response?.matches?.forEach((match) => {
      if (
        !uniqueMatches.has(match.id) ||
        match.score > uniqueMatches.get(match.id).score
      ) {
        uniqueMatches.set(match.id, match)
      }
    })
  })

  const results = Array.from(uniqueMatches.values())
    .filter((match) => match.score >= SIMILARITY_THRESHOLD)
    .sort((a, b) => b.score - a.score)
    .slice(0, 5)

  console.groupCollapsed(`[RAG Retrieval] Pinecone Results (${results.length})`)
  results.forEach((match) => {
    console.log(`- Score: ${match.score.toFixed(4)} | ID: ${match.id}`)
    console.log(`  Headline: ${match.metadata.headline}`)
  })
  console.groupEnd()

  return results
}

async function fetchValidatedWikipediaContext(entities) {
  const wikiResults = await fetchBatchWikipediaSummaries(entities)
  const validWikiResults = []
  for (const res of wikiResults.filter((r) => r.success)) {
    const validation = await validateWikipediaContent(res.summary)
    if (validation.valid) {
      validWikiResults.push({ ...res, validation })
    }
  }

  console.groupCollapsed(`[RAG Retrieval] Wikipedia Results (${validWikiResults.length})`)
  validWikiResults.forEach((res) => {
    console.log(`- Title: ${res.title}`)
    console.log(`  Summary: ${res.summary.substring(0, 200)}...`)
  })
  console.groupEnd()

  return validWikiResults
}

export async function retrieveContextForQuery(plan, messages, mode = 'full') {
  const { search_queries, user_query } = plan
  const entitiesToExclude = await extractEntitiesFromHistory(messages)

  const pineconeResults = await fetchPineconeContext(search_queries, entitiesToExclude)

  if (mode === 'ragOnly') {
    return {
      ragResults: pineconeResults,
      wikiResults: [],
      searchResults: [],
    }
  }

  const [wikipediaResults, searchResultsObj] = await Promise.all([
    fetchValidatedWikipediaContext(search_queries),
    getGoogleSearchResults(user_query),
  ])

  const searchResults = searchResultsObj.success ? searchResultsObj.results : []

  console.groupCollapsed(
    `[RAG Retrieval] SerpAPI Google Search Results (${searchResults.length})`
  )
  searchResults.forEach((res) => {
    console.log(`- Title: ${res.title}`)
    console.log(`  Link: ${res.link}`)
    console.log(`  Snippet: ${res.snippet}`)
  })
  console.groupEnd()

  return {
    ragResults: pineconeResults,
    wikiResults: wikipediaResults,
    searchResults: searchResults,
  }
}

```

## 📄 src/rag/validation.js
*Lines: 152, Size: 4.92 KB*

```javascript
// src/lib/rag/validation.js (version 3.0)
import OpenAI from 'openai'
import { env } from '@/lib/env.mjs'
import { GROUNDEDNESS_CHECK_PROMPT } from './prompts'

// --- Constants ---
const HIGH_CONFIDENCE_THRESHOLD = 0.75
const SIMILARITY_THRESHOLD = 0.38
const GROUNDEDNESS_MODEL = 'openai/gpt-oss-120b'

let groq
function getGroqClient() {
  if (!groq) {
    groq = new OpenAI({
      apiKey: env.GROQ_API_KEY,
      baseURL: 'https://api.groq.com/openai/v1',
    })
  }
  return groq
}

// --- Internal Helper Functions ---
function simpleEntityExtractor(text, sourceIdentifier) {
  const match = text.match(/([A-Z][a-z]+(?:\s[A-Z][a-z]+)*)/)
  if (match) return [{ name: match[0], facts: [text] }]
  return [{ name: sourceIdentifier, facts: [text] }]
}

function extractEntitiesFromRAG(ragResults) {
  if (!ragResults) return []
  return ragResults.flatMap((r) =>
    simpleEntityExtractor(r.metadata?.summary || '', r.metadata?.headline || 'RAG Source')
  )
}

function extractEntitiesFromWiki(wikiResults) {
  if (!wikiResults) return []
  return wikiResults.flatMap((w) =>
    simpleEntityExtractor(w.summary || '', w.title || 'Wiki Source')
  )
}

function entitySimilarity(entityA, entityB) {
  const nameA = entityA.name.toLowerCase()
  const nameB = entityB.name.toLowerCase()
  if (nameA.includes(nameB) || nameB.includes(nameA)) return 0.9
  return 0
}

function factsConflict(factsA, factsB) {
  // Placeholder for a future NLI model implementation.
  return false
}

// --- Exported Validation Functions ---

export function assessContextQuality(ragResults, wikiResults, searchResults) {
  const ragScore = ragResults.length > 0 ? Math.max(...ragResults.map((r) => r.score)) : 0
  const highQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'high'
  ).length
  const mediumQualityWiki = wikiResults.filter(
    (r) => r.validation?.quality === 'medium'
  ).length
  const wikiScore = highQualityWiki > 0 ? 0.7 : mediumQualityWiki > 0 ? 0.5 : 0
  const searchScore = searchResults.length > 0 ? 0.6 : 0 // Assign a moderate score if search results exist

  const combinedScore = Math.max(ragScore, wikiScore, searchScore)

  return {
    hasHighConfidenceRAG: ragScore >= HIGH_CONFIDENCE_THRESHOLD,
    hasSufficientContext: combinedScore >= SIMILARITY_THRESHOLD,
    ragResultCount: ragResults.length,
    wikiResultCount: wikiResults.length,
    searchResultCount: searchResults.length,
    highQualityWikiCount: highQualityWiki,
    maxSimilarity: ragScore,
    combinedConfidence: combinedScore,
    hasMultipleSources:
      (ragResults.length > 0 ? 1 : 0) +
        (wikiResults.length > 0 ? 1 : 0) +
        (searchResults.length > 0 ? 1 : 0) >
      1,
    hasHighQualityContent: ragScore >= HIGH_CONFIDENCE_THRESHOLD || highQualityWiki > 0,
  }
}

export function crossValidateSources(ragResults, wikiResults) {
  // This function is becoming less critical with the strict generation prompt,
  // but we'll keep its structure.
  const validation = { conflicts: [], confirmations: [], reliability: 'unknown' }

  if (
    (!ragResults || ragResults.length === 0) &&
    (!wikiResults || wikiResults.length === 0)
  ) {
    validation.reliability = 'single_source' // or 'no_source'
    return validation
  }

  // Simplified logic for now
  if (ragResults.length > 0 && wikiResults.length > 0) {
    validation.reliability = 'confirmed' // Assume confirmation if both exist
  } else {
    validation.reliability = 'single_source'
  }

  return validation
}

export async function checkGroundedness(responseText, contextString) {
  console.log('[RAG Validation] Performing Groundedness Check...')
  if (
    responseText.trim() ===
    'I do not have sufficient information in my sources to answer that question.'
  ) {
    console.log('[RAG Validation] PASSED: Bot correctly stated insufficient info.')
    return { is_grounded: true, unsupported_claims: [] }
  }

  try {
    const client = getGroqClient()
    const prompt = GROUNDEDNESS_CHECK_PROMPT.replace('{CONTEXT}', contextString).replace(
      '{RESPONSE}',
      responseText
    )

    const response = await client.chat.completions.create({
      model: GROUNDEDNESS_MODEL,
      messages: [{ role: 'system', content: prompt }],
      response_format: { type: 'json_object' },
      temperature: 0.0,
    })

    const result = JSON.parse(response.choices[0].message.content)

    if (result.is_grounded) {
      console.log('[RAG Validation] PASSED: Response is grounded in sources.')
    } else {
      console.warn('[RAG Validation] FAILED: Response contains unsupported claims.')
      console.groupCollapsed('Unsupported Claims Details')
      result.unsupported_claims.forEach((claim) => console.warn(`- ${claim}`))
      console.groupEnd()
    }
    return result
  } catch (error) {
    console.error('[RAG Validation] Error during verification:', error)
    // Fail safe: if the check fails, assume the response is not grounded.
    return { is_grounded: false, unsupported_claims: ['Fact-checking system failed.'] }
  }
}

```

## 📄 src/schemas/articleAssessmentSchema.js
*Lines: 24, Size: 753 Bytes*

```javascript
// packages/ai-services/src/schemas/articleAssessmentSchema.js (version 1.1)
import { z } from 'zod'

export const articleAssessmentSchema = z.object({
  reasoning: z.object({
    event_type: z.string(),
    is_liquidity_event: z.boolean(),
    beneficiary: z.string(),
  }),
  // NEW FIELD: Added classification to the schema.
  classification: z.enum(['New wealth', 'Wealth detection', 'Interview', 'IPO', 'Other']),
  relevance_article: z.number().min(0).max(100),
  assessment_article: z.string().min(1),
  amount: z.number().nullable().optional(),
  key_individuals: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      company: z.string().nullable(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/schemas/articlePreAssessmentSchema.js
*Lines: 7, Size: 223 Bytes*

```javascript
// packages/ai-services/src/schemas/articlePreAssessmentSchema.js (version 1.0)
import { z } from 'zod'

export const articlePreAssessmentSchema = z.object({
  classification: z.enum(['private', 'public', 'corporate']),
})

```

## 📄 src/schemas/batchArticleAssessmentSchema.js
*Lines: 8, Size: 285 Bytes*

```javascript
// packages/ai-services/src/schemas/batchArticleAssessmentSchema.js (version 1.0)
import { z } from 'zod'
import { articleAssessmentSchema } from './articleAssessmentSchema.js'

export const batchArticleAssessmentSchema = z.object({
  assessments: z.array(articleAssessmentSchema),
})

```

## 📄 src/schemas/batchHeadlineAssessmentSchema.js
*Lines: 9, Size: 371 Bytes*

```javascript
// packages/ai-services/src/schemas/batchHeadlineAssessmentSchema.js (version 1.0)
import { z } from 'zod'
import { headlineAssessmentSchema } from './headlineAssessmentSchema.js'

// The batch schema reuses the single assessment schema
export const batchHeadlineAssessmentSchema = z.object({
  assessments: z.array(headlineAssessmentSchema.shape.assessment.element),
})

```

## 📄 src/schemas/canonicalizerSchema.js
*Lines: 7, Size: 188 Bytes*

```javascript
// packages/ai-services/src/schemas/canonicalizerSchema.js (version 1.0)
import { z } from 'zod'

export const canonicalizerSchema = z.object({
  canonical_name: z.string().nullable(),
})

```

## 📄 src/schemas/clusterSchema.js
*Lines: 12, Size: 250 Bytes*

```javascript
// packages/ai-services/src/schemas/clusterSchema.js (version 1.0)
import { z } from 'zod'

export const clusterSchema = z.object({
  events: z.array(
    z.object({
      event_key: z.string(),
      article_ids: z.array(z.string()),
    })
  ),
})

```

## 📄 src/schemas/countryCorrectionSchema.js
*Lines: 7, Size: 269 Bytes*

```javascript
// packages/ai-services/src/schemas/countryCorrectionSchema.js
import { z } from 'zod';

export const countryCorrectionSchema = z.object({
  country: z.string().nullable().describe("The single, corrected, UN-recognized country name, or null if not determinable."),
});

```

## 📄 src/schemas/disambiguationSchema.js
*Lines: 7, Size: 186 Bytes*

```javascript
// packages/ai-services/src/schemas/disambiguationSchema.js (version 1.0)
import { z } from 'zod'

export const disambiguationSchema = z.object({
  best_title: z.string().nullable(),
})

```

## 📄 src/schemas/emailIntroSchema.js
*Lines: 10, Size: 240 Bytes*

```javascript
// packages/ai-services/src/schemas/emailIntroSchema.js (version 2.0)
import { z } from 'zod'

export const emailIntroSchema = z.object({
  greeting: z.string(),
  body: z.string(),
  bullets: z.array(z.string()),
  signoff: z.string(),
})

```

## 📄 src/schemas/emailSubjectSchema.js
*Lines: 7, Size: 184 Bytes*

```javascript
// packages/ai-services/src/schemas/emailSubjectSchema.js (version 1.0)
import { z } from 'zod'

export const emailSubjectSchema = z.object({
  subject_headline: z.string().min(1),
})

```

## 📄 src/schemas/enrichContactSchema.js
*Lines: 14, Size: 335 Bytes*

```javascript
// packages/ai-services/src/schemas/enrichContactSchema.js (version 1.0)
import { z } from 'zod'

export const enrichContactSchema = z.object({
  enriched_contacts: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      company: z.string(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/schemas/entitySchema.js
*Lines: 8, Size: 191 Bytes*

```javascript
// packages/ai-services/src/schemas/entitySchema.js (version 1.0)
import { z } from 'zod'

export const entitySchema = z.object({
  reasoning: z.string(),
  entities: z.array(z.string()),
})

```

## 📄 src/schemas/executiveSummarySchema.js
*Lines: 7, Size: 176 Bytes*

```javascript
// packages/ai-services/src/schemas/executiveSummarySchema.js (version 1.0)
import { z } from 'zod'

export const executiveSummarySchema = z.object({
  summary: z.string(),
})

```

## 📄 src/schemas/findContactSchema.js
*Lines: 7, Size: 183 Bytes*

```javascript
// packages/ai-services/src/schemas/findContactSchema.js (version 1.0)
import { z } from 'zod'

export const findContactSchema = z.object({
  email: z.string().email().nullable(),
})

```

## 📄 src/schemas/headlineAssessmentSchema.js
*Lines: 13, Size: 362 Bytes*

```javascript
// packages/ai-services/src/schemas/headlineAssessmentSchema.js (version 1.0)
import { z } from 'zod'

const singleAssessmentSchema = z.object({
  headline_en: z.string(),
  relevance_headline: z.number().min(0).max(100),
  assessment_headline: z.string(),
})

export const headlineAssessmentSchema = z.object({
  assessment: z.array(singleAssessmentSchema),
})

```

## 📄 src/schemas/index.js
*Lines: 25, Size: 1.02 KB*

```javascript
// packages/ai-services/src/schemas/index.js (version 2.1)
export * from './articleAssessmentSchema.js'
export * from './articlePreAssessmentSchema.js'
export * from './batchArticleAssessmentSchema.js'
export * from './canonicalizerSchema.js'
export * from './clusterSchema.js'
export * from './disambiguationSchema.js'
export * from './emailIntroSchema.js'
export * from './emailSubjectSchema.js'
export * from './enrichContactSchema.js'
export * from './entitySchema.js'
export * from './executiveSummarySchema.js'
export * from './findContactSchema.js'
export * from './headlineAssessmentSchema.js'
export * from './judgeSchema.js'
export * from './opportunitySchema.js'
export * from './selectorRepairSchema.js'
export * from './synthesisSchema.js'
export * from './watchlistSuggestionSchema.js'
export * from './sectionClassifierSchema.js'
export * from './batchHeadlineAssessmentSchema.js'
export * from './translateSchema.js'
// DEFINITIVE FIX: Add the missing export for the new schema.
export * from './countryCorrectionSchema.js'

```

## 📄 src/schemas/judgeSchema.js
*Lines: 14, Size: 404 Bytes*

```javascript
// packages/ai-services/src/schemas/judgeSchema.js (version 1.0)
import { z } from 'zod'

const verdictSchema = z.object({
  identifier: z.string(),
  quality: z.enum(['Excellent', 'Good', 'Acceptable', 'Marginal', 'Poor', 'Irrelevant']),
  commentary: z.string(),
})

export const judgeSchema = z.object({
  event_judgements: z.array(verdictSchema),
  opportunity_judgements: z.array(verdictSchema),
})

```

## 📄 src/schemas/opportunitySchema.js
*Lines: 22, Size: 929 Bytes*

```javascript
// packages/ai-services/src/schemas/opportunitySchema.js (version 2.1)
import { z } from 'zod'

export const opportunitySchema = z.object({
  opportunities: z.array(
    // DEFINITIVE FIX: Use .passthrough() to allow the AI to include extra fields
    // without causing a validation error. We will only use the fields we define.
    z.object({
      reachOutTo: z.string().describe("The full name of the individual or family to contact."),
      contactDetails: z.object({
        email: z.string().email().nullable(),
        role: z.string().nullable(),
        company: z.string().nullable(),
      }),
      basedIn: z.string().nullable(),
      whyContact: z.array(z.string()).describe("An array of concise, one-sentence reasons for contact."),
      likelyMMDollarWealth: z.number().nullable(),
      event_key: z.string().describe("The unique key of the source event for this opportunity."),
    }).passthrough()
  ),
})

```

## 📄 src/schemas/sectionClassifierSchema.js
*Lines: 14, Size: 411 Bytes*

```javascript
// packages/ai-services/src/schemas/sectionClassifierSchema.js (version 1.0)
import { z } from 'zod'

export const sectionClassifierSchema = z.object({
  classifications: z.array(
    z.object({
      classification: z.enum(['news_section', 'article_headline', 'navigation', 'other']),
      reasoning: z
        .string()
        .describe('A brief explanation for the classification choice.'),
    })
  ),
})

```

## 📄 src/schemas/selectorRepairSchema.js
*Lines: 13, Size: 415 Bytes*

```javascript
// packages/ai-services/src/schemas/selectorRepairSchema.js (version 1.0)
import { z } from 'zod'

export const selectorRepairSchema = z.object({
  reasoning: z.string(),
  suggested_selectors: z.object({
    headlineSelector: z.string().optional(),
    linkSelector: z.string().optional().nullable(),
    headlineTextSelector: z.string().optional().nullable(),
    articleSelector: z.string().optional(),
  }),
})

```

## 📄 src/schemas/synthesisSchema.js
*Lines: 27, Size: 819 Bytes*

```javascript
// packages/ai-services/src/schemas/synthesisSchema.js (version 1.1)
import { z } from 'zod'

export const synthesisSchema = z.object({
  events: z.array(
    z.object({
      headline: z.string().min(1),
      summary: z.string().min(1),
      advisor_summary: z
        .string()
        .min(1)
        .describe('The one-sentence actionable summary for wealth advisors.'),
      // DEFINITIVE FIX: Add the new classification field to the schema.
      eventClassification: z.string().min(1).describe("The event's classification type."),
      country: z.string().min(1),
      key_individuals: z.array(
        z.object({
          name: z.string(),
          role_in_event: z.string(),
          company: z.string().nullable(),
          email_suggestion: z.string().nullable(),
        })
      ),
    })
  ),
})

```

## 📄 src/schemas/translateSchema.js
*Lines: 7, Size: 179 Bytes*

```javascript
// packages/ai-services/src/schemas/translateSchema.js (version 1.0.0)
import { z } from 'zod'

export const translateSchema = z.object({
  translated_html: z.string().min(1),
})

```

## 📄 src/schemas/watchlistSuggestionSchema.js
*Lines: 16, Size: 496 Bytes*

```javascript
// packages/ai-services/src/schemas/watchlistSuggestionSchema.js (version 2.0.0 - With Search Terms)
import { z } from 'zod'

export const watchlistSuggestionSchema = z.object({
  suggestions: z.array(
    z.object({
      name: z.string(),
      type: z.enum(['person', 'family', 'company']),
      country: z.string(),
      rationale: z.string(),
      sourceEvent: z.string(),
      searchTerms: z.array(z.string()).describe("An array of 2-4 unique, lowercase search terms."),
    })
  ),
})

```

## 📄 src/search/search.js
*Lines: 86, Size: 3.11 KB*

```javascript
'use server'
import axios from 'axios'
import NewsAPI from 'newsapi'
import { env } from '@headlines/config/server'
import { logger, apiCallTracker } from '@headlines/utils-server'

const { SERPER_API_KEY, NEWSAPI_API_KEY } = env
const serperClient = SERPER_API_KEY
  ? axios.create({
      baseURL: 'https://google.serper.dev',
      headers: { 'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json' },
    })
  : null
const newsapi = NEWSAPI_API_KEY ? new NewsAPI(NEWSAPI_API_KEY) : null

if (!serperClient)
  logger.warn(
    'SERPER_API_KEY not found. Google Search dependent functions will be disabled.'
  )
if (!newsapi)
  logger.warn('NEWSAPI_API_KEY not found. NewsAPI dependent functions will be disabled.')

async function withRetry(apiCall, serviceName, maxRetries = 2) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await apiCall()
    } catch (error) {
      const isRetryable = error.response && error.response.status >= 500
      if (!isRetryable || attempt === maxRetries) {
        logger.error(
          { err: error?.response?.data || error },
          `${serviceName} search failed.`
        )
        return { success: false, error: error.message, results: [] }
      }
      const delay = 1000 * Math.pow(2, attempt - 1)
      logger.warn(`[${serviceName}] Attempt ${attempt} failed. Retrying in ${delay}ms...`)
      await new Promise((res) => setTimeout(res, delay))
    }
  }
}

export async function findAlternativeSources(headline) {
  if (!serperClient) return { success: false, results: [] }
  return withRetry(async () => {
    apiCallTracker.recordCall('serper_news')
    const response = await serperClient.post('/news', { q: headline })
    return { success: true, results: response.data.news || [] }
  }, 'Serper News')
}
export async function performGoogleSearch(query) {
  if (!serperClient) return { success: false, snippets: 'SERPER_API_KEY not configured.' }
  return withRetry(async () => {
    apiCallTracker.recordCall('serper_search')
    const response = await serperClient.post('/search', { q: query })
    const organicResults = response.data.organic || []
    if (organicResults.length > 0) {
      const snippets = organicResults
        .slice(0, 5)
        .map((res) => `- ${res.title}: ${res.snippet}`)
        .join('\n')
      return { success: true, snippets }
    }
    return { success: false, snippets: 'No search results found.' }
  }, 'Serper Search')
}
export async function findNewsApiArticlesForEvent(headline) {
  if (!newsapi) return { success: false, snippets: 'NewsAPI key not configured.' }
  return withRetry(async () => {
    apiCallTracker.recordCall('newsapi_search')
    const response = await newsapi.v2.everything({
      q: `"${headline}"`,
      pageSize: 5,
      sortBy: 'relevancy',
      language: 'en,da,sv,no',
    })
    if (response.articles && response.articles.length > 0) {
      const snippets = response.articles
        .map((a) => `- ${a.title} (${a.source.name}): ${a.description || ''}`)
        .join('\n')
      return { success: true, snippets }
    }
    return { success: false, snippets: 'No related articles found.' }
  }, 'NewsAPI')
}

```

## 📄 src/search/serpapi.js
*Lines: 57, Size: 1.61 KB*

```javascript
// packages/ai-services/src/search/serpapi.js
'use server'

import { getJson } from 'serpapi'
import { env } from '@headlines/config/server'

const searchCache = new Map()
const CACHE_TTL = 1000 * 60 * 60 // 1 hour

export async function getGoogleSearchResults(query) {
  if (!query) {
    return { success: false, error: 'Query is required.' }
  }

  const cacheKey = `serpapi_${query.toLowerCase().trim()}`
  if (searchCache.has(cacheKey)) {
    const cached = searchCache.get(cacheKey)
    if (Date.now() - cached.timestamp < CACHE_TTL) {
      console.log(`[SerpAPI Cache] Hit for query: "${query}"`)
      return cached.data
    }
  }

  console.log(`[SerpAPI] Performing live search for: "${query}"`)

  try {
    const response = await getJson({
      api_key: env.SERPAPI_API_KEY,
      engine: 'google',
      q: query,
      location: 'United States',
      gl: 'us',
      hl: 'en',
    })

    const organicResults = response.organic_results || []
    const answerBox = response.answer_box ? [response.answer_box] : []

    const formattedResults = [...answerBox, ...organicResults]
      .map((item) => ({
        title: item.title,
        link: item.link,
        snippet: item.snippet || item.answer || item.result,
        source: 'Google Search',
      }))
      .filter((item) => item.snippet)
      .slice(0, 5)

    const result = { success: true, results: formattedResults }
    searchCache.set(cacheKey, { data: result, timestamp: Date.now() })
    return result
  } catch (error) {
    console.error('[SerpAPI Error]', error)
    return { success: false, error: `Failed to fetch search results: ${error.message}` }
  }
}

```

## 📄 src/search/wikipedia.js
*Lines: 107, Size: 3.62 KB*

```javascript
'use server'; 
// DEFINITIVE FIX: Import the logger directly. It is a server-safe module.
import { logger, apiCallTracker } from '@headlines/utils-server'
import { disambiguationChain } from '../chains/index.js'

const WIKI_API_ENDPOINT = 'https://en.wikipedia.org/w/api.php'
const WIKI_SUMMARY_LENGTH = 750

async function fetchWithRetry(url, options, maxRetries = 2) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url, options)
      if (!response.ok) throw new Error(`API returned status ${response.status}`)
      return response
    } catch (error) {
      if (attempt === maxRetries) throw error
      const delay = 1000 * Math.pow(2, attempt - 1)
      logger.warn(
        `[Wikipedia Fetch] Attempt ${attempt} failed for ${url}. Retrying in ${delay}ms...`
      )
      await new Promise((res) => setTimeout(res, delay))
    }
  }
}

export async function fetchWikipediaSummary(query) {
  if (!query) return { success: false, error: 'Query cannot be empty.' }
  try {
    apiCallTracker.recordCall('wikipedia')
    const searchParams = new URLSearchParams({
      action: 'query',
      list: 'search',
      srsearch: query,
      srlimit: '5',
      format: 'json',
    })
    const searchResponse = await fetchWithRetry(
      `${WIKI_API_ENDPOINT}?${searchParams.toString()}`
    )
    const searchData = await searchResponse.json()
    const searchResults = searchData.query.search
    if (!searchResults || searchResults.length === 0)
      throw new Error(`No search results for "${query}".`)

    const userContent = `Original Query: "${query}"\n\nSearch Results:\n${JSON.stringify(searchResults.map((r) => ({ title: r.title, snippet: r.snippet })))}`
    const disambiguationResponse = await disambiguationChain({ inputText: userContent })

    if (disambiguationResponse.error || !disambiguationResponse.best_title) {
      throw new Error(
        disambiguationResponse.error ||
          `AI agent could not disambiguate a relevant page for "${query}".`
      )
    }
    const { best_title } = disambiguationResponse

    const summaryParams = new URLSearchParams({
      action: 'query',
      prop: 'extracts',
      exintro: 'true',
      explaintext: 'true',
      titles: best_title,
      format: 'json',
      redirects: '1',
    })
    const summaryResponse = await fetchWithRetry(
      `${WIKI_API_ENDPOINT}?${summaryParams.toString()}`
    )
    const summaryData = await summaryResponse.json()
    const pages = summaryData.query.pages
    const pageId = Object.keys(pages)[0]
    const summary = pages[pageId]?.extract
    if (!summary) throw new Error(`Could not extract summary for page "${best_title}".`)

    const conciseSummary =
      summary.length > WIKI_SUMMARY_LENGTH
        ? summary.substring(0, WIKI_SUMMARY_LENGTH) + '...'
        : summary
    return { success: true, summary: conciseSummary, title: best_title, query }
  } catch (error) {
    logger.warn(`Wikipedia lookup for "${query}" failed: ${error.message}`)
    return { success: false, error: error.message, query }
  }
}

export async function fetchBatchWikipediaSummaries(queries) {
  const promises = queries.map((q) => fetchWikipediaSummary(q))
  return Promise.all(promises)
}

export async function validateWikipediaContent(text) {
  const lowerText = text.toLowerCase()
  const isDisambiguation =
    lowerText.includes('may refer to:') || lowerText.includes('is a list of')
  if (isDisambiguation) {
    return {
      valid: false,
      quality: 'low',
      reason: 'Disambiguation page content detected.',
    }
  }
  return {
    valid: true,
    quality: 'high',
    reason: 'Content appears to be a valid summary.',
  }
}

```
