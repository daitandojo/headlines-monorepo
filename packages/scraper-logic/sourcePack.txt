# üìÅ PROJECT DIRECTORY STRUCTURE

Total: 29 files, 7 directories

```
headlines-monorepo/
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ ai/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ index.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ push/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ client.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ scraper/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ extractors/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ reusable/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ simple.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ source-specific/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cvcPortfolio.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cvcPortfolioContent.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ finansDk.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ jyllandsPosten.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ okonomiskUgebrev.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ politiken.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ verdane.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ index.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ constants.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ contentScraper.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dynamicExtractor.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ headlineScraper.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ index.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ newsApiScraper.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ orchestrator.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ selectorOptimizer.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test-helpers.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ browser.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ browserManager.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ core.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ index.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ next.js
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ node.js
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test-orchestrator.js
‚îî‚îÄ‚îÄ üìÑ package.json
```

# üìã PROJECT METADATA

**Generated**: 2025-10-10T00:11:02.006Z
**Repository Path**: /home/mark/Repos/projects/headlines-monorepo/packages/scraper-logic
**Total Files**: 29
**Package**: @headlines/scraper-logic@2.0.0




---


## üìÑ package.json
*Lines: 42, Size: 1.21 KB*

```json
{
  "name": "@headlines/scraper-logic",
  "version": "2.0.0",
  "main": "src/node.js",
  "type": "module",
  "license": "ISC",
  "exports": {
    ".": "./src/node.js",
    "./node": "./src/node.js",
    "./next": "./src/next.js",
    "./config.js": "./src/config.js",
    "./browser.js": "./src/browser.js",
    "./browserManager.js": "./src/browserManager.js",
    "./scraper/index.js": "./src/scraper/index.js",
    "./scraper/selectorOptimizer.js": "./src/scraper/selectorOptimizer.js",
    "./push/client.js": "./src/push/client.js",
    "./ai/index.js": "./src/ai/index.js",
    "./test-orchestrator": "./src/test-orchestrator.js"
  },
  "dependencies": {
    "@headlines/ai-services": "workspace:*",
    "@headlines/config": "workspace:*",
    "@headlines/models": "workspace:*",
    "@headlines/prompts": "workspace:*",
    "@headlines/utils-shared": "workspace:*",
    "@mozilla/readability": "^0.6.0",
    "axios": "^1.7.2",
    "cheerio": "^1.0.0-rc.12",
    "date-fns": "*",
    "jsdom": "^24.1.1",
    "lodash": "*",
    "newsapi": "^2.4.1",
    "p-limit": "^5.0.0",
    "pusher": "^5.2.0",
    "rss-parser": "^3.13.0",
    "web-push": "^3.6.7",
    "zod": "^3.23.8"
  },
  "peerDependencies": {
    "playwright": "^1.45.1"
  }
}
```

## üìÑ src/ai/index.js
*Lines: 136, Size: 5.32 KB*

```javascript
// packages/scraper-logic/src/ai/index.js (version 6.1.0)
import { getConfig } from '../config.js'
import { callLanguageModel } from '../../../ai-services/src/index.js'
import { AIAgent } from './AIAgent.js'
import { assessArticleContent } from './agents/articleAgent.js'
import { articleAssessmentSchema } from './schemas/articleAssessmentSchema.js'
import { preAssessArticle } from './agents/articlePreAssessmentAgent.js'
import { articlePreAssessmentSchema } from './schemas/articlePreAssessmentSchema.js'
import { clusterArticlesIntoEvents } from './agents/clusteringAgent.js'
import { clusterSchema } from './schemas/clusterSchema.js'
import { resolveVagueContact, findContactDetails } from './agents/contactAgent.js'
import { enrichContactSchema } from './schemas/enrichContactSchema.js'
import { findContactSchema } from './schemas/findContactSchema.js'
import {
  generateEmailSubjectLine,
  generatePersonalizedIntro,
} from './agents/emailAgents.js'
import { emailSubjectSchema } from './schemas/emailSubjectSchema.js'
import { emailIntroSchema } from './schemas/emailIntroSchema.js'
import {
  extractEntities,
  entityCanonicalizerAgent as getEntityCanonicalizerAgent,
} from './agents/entityAgent.js'
import { entitySchema } from './schemas/entitySchema.js'
import { canonicalizerSchema } from './schemas/canonicalizerSchema.js'
import { assessHeadlinesInBatches } from './agents/headlineAgent.js'
import { headlineAssessmentSchema } from './schemas/headlineAssessmentSchema.js'
import { judgePipelineOutput } from './agents/judgeAgent.js'
import { judgeSchema } from './schemas/judgeSchema.js'
import { generateOpportunitiesFromEvent } from './agents/opportunityAgent.js'
import { opportunitySchema } from './schemas/opportunitySchema.js'
import { suggestNewSelector } from './agents/selectorRepairAgent.js'
import { selectorRepairSchema } from './schemas/selectorRepairSchema.js'
import { synthesizeEvent, synthesizeFromHeadline } from './agents/synthesisAgent.js'
import { synthesisSchema } from './schemas/synthesisSchema.js'
import { generateWatchlistSuggestions } from './agents/watchlistAgent.js'
import { watchlistSuggestionSchema } from './schemas/watchlistSuggestionSchema.js'
import { disambiguationSchema } from './schemas/disambiguationSchema.js'
import { batchAssessArticles } from './agents/batchArticleAgent.js'
import { batchArticleAssessmentSchema } from './schemas/batchArticleAssessmentSchema.js'
import { classifyLinks as sectionClassifierAgent } from './agents/sectionClassifierAgent.js'
import { generateExecutiveSummary } from './agents/executiveSummaryAgent.js'

let isApiKeyInvalid = false
export async function performAiSanityCheck() {
  try {
    getConfig().logger.info('üî¨ Performing AI service sanity check (OpenAI)...')
    const answer = await callLanguageModel({
      modelName: 'gpt-3.5-turbo', // Use a standard, widely available model for the check
      prompt: 'What is in one word the name of the capital of France',
      isJson: false,
    })
    if (
      answer &&
      typeof answer === 'string' &&
      answer.trim().toLowerCase().includes('paris')
    ) {
      getConfig().logger.info('‚úÖ AI service sanity check passed.')
      return true
    } else {
      getConfig().logger.fatal(
        { details: { expected: 'paris', received: answer } },
        `OpenAI sanity check failed.`
      )
      return false
    }
  } catch (error) {
    if (error.status === 401 || error.message?.includes('Incorrect API key')) {
      getConfig().logger.fatal(`OpenAI sanity check failed due to INVALID API KEY (401).`)
    } else {
      getConfig().logger.fatal(
        { err: error },
        'OpenAI sanity check failed with an unexpected API error.'
      )
    }
    isApiKeyInvalid = true
    return false
  }
}
export async function checkModelPermissions(requiredModels) {
  getConfig().logger.info('üî¨ Verifying permissions for configured OpenAI models...')
  try {
    // DEFINITIVE FIX: The OpenAI client for checking models is part of the ai-services package, not here.
    // This function is also not strictly necessary for the app to run, so we can simplify.
    // For now, we will assume permissions are correct if the sanity check passes.
    getConfig().logger.warn(
      'Model permission check is currently a no-op, relying on sanity check.'
    )
    return true
  } catch (error) {
    getConfig().logger.fatal({ err: error }, 'Failed to verify model permissions.')
    isApiKeyInvalid = true
    return false
  }
}
export {
  AIAgent,
  callLanguageModel,
  assessArticleContent,
  articleAssessmentSchema,
  preAssessArticle,
  articlePreAssessmentSchema,
  clusterArticlesIntoEvents,
  clusterSchema,
  resolveVagueContact,
  findContactDetails,
  enrichContactSchema,
  findContactSchema,
  generateEmailSubjectLine,
  generatePersonalizedIntro,
  emailSubjectSchema,
  emailIntroSchema,
  extractEntities,
  getEntityCanonicalizerAgent as entityCanonicalizerAgent,
  entitySchema,
  canonicalizerSchema,
  assessHeadlinesInBatches,
  headlineAssessmentSchema,
  judgePipelineOutput,
  judgeSchema,
  generateOpportunitiesFromEvent,
  opportunitySchema,
  suggestNewSelector,
  selectorRepairSchema,
  synthesizeEvent,
  synthesizeFromHeadline,
  synthesisSchema,
  generateWatchlistSuggestions,
  watchlistSuggestionSchema,
  disambiguationSchema,
  batchAssessArticles,
  batchArticleAssessmentSchema,
  sectionClassifierAgent,
  generateExecutiveSummary,
}

```

## üìÑ src/browser.js
*Lines: 163, Size: 5.03 KB*

```javascript
// packages/scraper-logic/src/browser.js
import fs from 'fs/promises'
import path from 'path'
import { getConfig } from './config.js'
import { browserManager } from './browserManager.js'

const CONSENT_SELECTORS = [
  'button:has-text("Accepteer alles")',
  'button:has-text("Alles accepteren")',
  'button:has-text("Toestemming geven")',
  'button:has-text("Akkoord")',
  'button:has-text("Accept all")',
  'button:has-text("Accept All")',
  'button:has-text("I accept")',
  'button:has-text("Accept")',
  'button:has-text("Godkend alle")',
  'button:has-text("Tillad alle")',
]

async function ensureDebugDirectory() {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) {
    getConfig().logger.warn('Debug HTML directory not configured. Saving disabled.')
    return null
  }
  try {
    await fs.mkdir(debugDir, { recursive: true })
    return debugDir
  } catch (error) {
    getConfig().logger.warn('Failed to create debug directory: ' + error.message)
    return null
  }
}

async function saveDebugHtml(page, caller, prefix, url) {
  const debugDir = await ensureDebugDirectory()
  if (!debugDir) return null
  try {
    const html = await page.content()
    const urlPart = new URL(url).hostname.replace(/[^a-z0-9]/gi, '_')
    const filename = `${prefix}_${caller}_${urlPart}.html`
    const filePath = path.join(debugDir, filename)
    await fs.writeFile(filePath, html)
    getConfig().logger.warn(`[Playwright:${caller}] Saved debug HTML to ${filePath}`)
    return filePath
  } catch (error) {
    getConfig().logger.error(
      `[Playwright:${caller}] Failed to save debug HTML: ${error.message}`
    )
    return null
  }
}

async function handleConsent(page, caller) {
  for (const selector of CONSENT_SELECTORS) {
    try {
      const button = page.locator(selector).first()
      if (await button.isVisible({ timeout: 1500 })) {
        await button.click({ timeout: 2000 })
        getConfig().logger.info(
          `[Playwright:${caller}] Clicked consent button with selector: "${selector}"`
        )
        await page.waitForTimeout(1500)
        return true
      }
    } catch (e) {
      // Selector not found, continue
    }
  }
  getConfig().logger.trace(`[Playwright:${caller}] No actionable consent modal found.`)
  return false
}

export async function fetchPageWithPlaywright(url, caller = 'Unknown', options = {}) {
  const { timeout = 60000, waitForSelector } = options
  let page = null
  try {
    getConfig().logger.trace(
      `[Playwright:${caller}] Requesting new page for: ${url} (Timeout: ${timeout}ms)`
    )
    page = await browserManager.newPage()

    await page.goto(url, { waitUntil: 'domcontentloaded', timeout })
    await handleConsent(page, caller)

    if (waitForSelector) {
      getConfig().logger.info(
        `[Playwright:${caller}] Waiting for selector "${waitForSelector}"...`
      )
      await page.waitForSelector(waitForSelector, { timeout: timeout - 5000 })
      getConfig().logger.info(`[Playwright:${caller}] Selector found. Page is ready.`)
    } else {
      await page
        .waitForLoadState('networkidle', { timeout: 5000 })
        .catch(() =>
          getConfig().logger.trace(
            `[Playwright:${caller}] Network idle timeout reached, proceeding anyway.`
          )
        )
    }

    return await page.content()
  } catch (error) {
    let reason = error.message.split('\n')[0]
    if (error.message.includes('net::ERR')) {
      reason = `Network Error: ${reason}`
    } else if (error.name === 'TimeoutError') {
      reason = `Timeout after ${timeout / 1000}s. The page may be too slow or blocked.`
    } else if (page) {
      const pageContent = await page.content()
      if (pageContent.includes('captcha') || pageContent.includes('challenge-platform')) {
        reason = 'Potential CAPTCHA or bot detection wall encountered.'
      }
    }

    getConfig().logger.error(
      `[Playwright:${caller}] Critical failure during fetch for ${url}: ${reason}`
    )
    if (page) {
      await saveDebugHtml(page, caller, 'CRITICAL_FAIL', url)
    }
    return null
  } finally {
    if (page) {
      await page.close()
      getConfig().logger.trace(`[Playwright:${caller}] Page closed for: ${url}`)
    }
  }
}

export async function fetchPageContentFromPopup(pageUrl, buttonSelector) {
  let page = null
  try {
    page = await browserManager.newPage()
    await page.goto(pageUrl, { waitUntil: 'networkidle' })
    await handleConsent(page, 'PopupFetcher')

    await page.waitForSelector(buttonSelector, { timeout: 10000 })
    const button = page.locator(buttonSelector).first()
    await button.click()

    await page.waitForSelector('.popup-overlay--opened', {
      state: 'visible',
      timeout: 5000,
    })

    const popupElement = await page.locator('.popup__box')
    const popupHtml = await popupElement.innerHTML()
    return popupHtml
  } catch (error) {
    getConfig().logger.error(
      { err: error, url: pageUrl, selector: buttonSelector },
      'Failed to fetch content from popup.'
    )
    return null
  } finally {
    if (page) {
      await page.close()
    }
  }
}

```

## üìÑ src/browserManager.js
*Lines: 62, Size: 1.87 KB*

```javascript
// packages/scraper-logic/src/browserManager.js
import playwright from 'playwright'
import { getConfig } from './config.js'

const BROWSER_HEADERS = {
  Accept:
    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9,nl-NL;q=0.8,nl;q=0.7',
  'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

class BrowserManager {
  constructor() {
    this.browser = null
    this.context = null
  }

  async initialize() {
    if (this.browser) {
      return
    }
    const { logger } = getConfig()
    logger.info('[BrowserManager] Initializing persistent browser instance...')
    try {
      this.browser = await playwright.chromium.launch({
        headless: true,
        args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'],
      })
      this.context = await this.browser.newContext({
        userAgent: BROWSER_HEADERS['User-Agent'],
        extraHTTPHeaders: BROWSER_HEADERS,
        viewport: { width: 1920, height: 1080 },
      })
      logger.info('[BrowserManager] ‚úÖ Browser instance ready.')
    } catch (error) {
      logger.fatal({ err: error }, '[BrowserManager] CRITICAL: Failed to launch browser.')
      throw error
    }
  }

  async newPage() {
    if (!this.context) {
      throw new Error('BrowserManager not initialized. Call initialize() first.')
    }
    return this.context.newPage()
  }

  async close() {
    if (this.browser) {
      await this.browser.close()
      this.browser = null
      this.context = null
      getConfig().logger.info('[BrowserManager] Persistent browser instance closed.')
    }
  }
}

// Export a singleton instance of the manager
export const browserManager = new BrowserManager()

```

## üìÑ src/config.js
*Lines: 23, Size: 567 Bytes*

```javascript
// packages/scraper-logic/src/config.js (version 1.0.0)
// This module holds the shared configuration for the scraper logic,
// which will be injected by the consuming application (pipeline or admin).

let _config = {
  // A simple console logger as a fallback.
  logger: console,
  paths: {
    debugHtmlDir: null,
  },
  configStore: null,
  utilityFunctions: null,
};

export function configure(appConfig) {
  // Merge the provided app config with the existing config.
  _config = { ..._config, ...appConfig };
}

export function getConfig() {
  return _config;
}

```

## üìÑ src/core.js
*Lines: 11, Size: 514 Bytes*

```javascript
// This file serves as the core, shared module for the scraper-logic package.
// It is environment-agnostic. Environment-specific entry points (next.js, node.js)
// will re-export from this file and add any necessary guards.

// Currently, all exports are handled via subpaths (e.g., /browser, /scraper/index.js).
// If you add a function that should be available from the root of the package,
// export it from here.

// Example of a future export:
// export { someSharedScraperUtil } from './some-util-file.js';

```

## üìÑ src/index.js
*Lines: 3, Size: 76 Bytes*

```javascript
// This is the default, Node.js-safe entry point.
export * from './core.js'

```

## üìÑ src/next.js
*Lines: 10, Size: 349 Bytes*

```javascript
    
// packages/scraper-logic/src/next.js (version 2.0.0)
// ARCHITECTURAL REFACTORING: This file is now a "dead-end" shim.
// It is guarded and exports nothing, preventing the Next.js/Vercel build
// from ever bundling heavy, incompatible Node.js dependencies like Playwright.
import 'server-only';

// This file intentionally exports nothing.

  
```

## üìÑ src/node.js
*Lines: 12, Size: 403 Bytes*

```javascript
// packages/scraper-logic/src/node.js
// This file serves as the explicit, Node.js-only entry point.

export * from './core.js'
export * from './browser.js'
export * from './browserManager.js' // Export the new manager
export * from './scraper/index.js'
export * from './scraper/selectorOptimizer.js'
export * from './push/client.js'
export * from './ai/index.js'
export * from './test-orchestrator.js'

```

## üìÑ src/push/client.js
*Lines: 26, Size: 940 Bytes*

```javascript
// packages/scraper-logic/src/push/client.js (version 2.0.0)
import webpush from 'web-push'
import { getConfig } from '../config.js';
import { env } from '../../../config/src/index.js'

let isPushConfigured = false

function configurePush() {
    if (isPushConfigured) return;

    const { VAPID_SUBJECT, VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY } = env
    if (VAPID_PUBLIC_KEY && VAPID_PRIVATE_KEY && VAPID_SUBJECT) {
      try {
        webpush.setVapidDetails(VAPID_SUBJECT, VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY)
        isPushConfigured = true
        getConfig().logger.info('‚úÖ Centralized push notification service (VAPID) configured.')
      } catch (error) {
        getConfig().logger.error({ err: error }, '‚ùå Failed to configure VAPID details.')
      }
    } else {
      getConfig().logger.warn('VAPID keys not fully configured. Push notifications will be disabled.')
    }
}

export { webpush, isPushConfigured, configurePush }

```

## üìÑ src/scraper/constants.js
*Lines: 10, Size: 454 Bytes*

```javascript
// src/modules/scraper/constants.js (version 1.0)
export const BROWSER_HEADERS = {
  Accept:
    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9',
  'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
}

```

## üìÑ src/scraper/contentScraper.js
*Lines: 397, Size: 11.94 KB*

```javascript
// packages/scraper-logic/src/scraper/contentScraper.js
import * as cheerio from 'cheerio'
import fs from 'fs/promises'
import path from 'path'
import { JSDOM, VirtualConsole } from 'jsdom'
import { Readability } from '@mozilla/readability'

import { getConfig } from '../config.js'
import { fetchPageWithPlaywright } from '../browser.js'
import { contentExtractorRegistry } from './extractors/index.js'
import { settings } from '@headlines/config/node'

/**
 * Content quality metrics for scoring extracted content
 * These weights can be tuned per-source or globally based on performance
 */
const QUALITY_WEIGHTS = {
  length: 0.3,
  paragraphs: 0.2,
  linkDensity: 0.15,
  punctuation: 0.15,
  wordVariety: 0.2,
}

/**
 * Get quality weights for a specific source (allows per-source tuning)
 * @param {Object} source - Source configuration
 * @returns {Object} Quality weights
 */
function getQualityWeights(source) {
  return source.qualityWeights || QUALITY_WEIGHTS
}

/**
 * Saves HTML to debug directory for troubleshooting
 * @param {string} filename - Name of the file to save
 * @param {string} html - HTML content to save
 * @returns {Promise<string|null>} Path to saved file or null
 */
async function saveDebugHtml(filename, html) {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) return null

  try {
    await fs.mkdir(debugDir, { recursive: true })
    const filePath = path.join(debugDir, filename)
    await fs.writeFile(filePath, html)
    return filePath
  } catch (error) {
    config.logger.error({ err: error, file: filename }, 'Failed to save debug HTML.')
    return null
  }
}

/**
 * Cleans extracted text by normalizing whitespace and removing artifacts
 * @param {string} text - Raw extracted text
 * @returns {string} Cleaned text
 */
function cleanText(text) {
  if (!text) return ''

  return text
    .replace(/\s+/g, ' ') // Normalize whitespace
    .replace(/\n{3,}/g, '\n\n') // Limit consecutive newlines
    .replace(/[^\S\n]+/g, ' ') // Remove non-newline whitespace duplicates
    .trim()
}

/**
 * Calculates quality score for extracted content
 * @param {string} content - Extracted content
 * @param {cheerio.CheerioAPI} $ - Cheerio instance (for link density calculation)
 * @param {Object} weights - Quality weight configuration
 * @returns {number} Quality score between 0 and 1
 */
function calculateContentQuality(content, $, weights = QUALITY_WEIGHTS) {
  if (!content) return 0

  const words = content.split(/\s+/)
  const wordCount = words.length

  // Length score (optimal range: 500-5000 words)
  const lengthScore = Math.min(wordCount / 500, 5000 / Math.max(wordCount, 1)) / 5

  // Paragraph structure (looking for proper formatting)
  const paragraphs = content.split(/\n\n+/).filter((p) => p.trim().length > 50)
  const paragraphScore = Math.min(paragraphs.length / 10, 1)

  // Link density (lower is better for article content)
  // Reuse existing Cheerio instance to avoid re-parsing
  const linkTextLength = $('a').text().length
  const totalTextLength = $.text().length
  const linkDensity = totalTextLength > 0 ? linkTextLength / totalTextLength : 0
  const linkScore = Math.max(0, 1 - linkDensity * 2)

  // Punctuation (indicates proper sentences)
  const punctuationCount = (content.match(/[.!?]/g) || []).length
  const punctuationScore = Math.min(punctuationCount / (wordCount / 20), 1)

  // Word variety (unique words / total words)
  const uniqueWords = new Set(words.map((w) => w.toLowerCase()))
  const varietyScore = wordCount > 0 ? uniqueWords.size / wordCount : 0

  const totalScore =
    lengthScore * weights.length +
    paragraphScore * weights.paragraphs +
    linkScore * weights.linkDensity +
    punctuationScore * weights.punctuation +
    varietyScore * weights.wordVariety

  return totalScore
}

/**
 * Extracts content using Mozilla Readability.js
 * @param {string} url - Article URL
 * @param {string} html - Page HTML
 * @returns {Object|null} Extracted content with metadata
 */
function extractWithReadability(url, html) {
  try {
    const virtualConsole = new VirtualConsole()
    virtualConsole.on('cssParseError', () => {}) // Suppress CSS errors
    virtualConsole.on('jsdomError', () => {}) // Suppress JSDOM errors

    const doc = new JSDOM(html, {
      url,
      virtualConsole,
      resources: 'usable',
    })

    const reader = new Readability(doc.window.document, {
      charThreshold: settings.MIN_ARTICLE_CHARS || 500,
      classesToPreserve: ['caption', 'credit'],
    })

    const article = reader.parse()

    if (!article?.textContent) return null

    return {
      content: cleanText(article.textContent),
      title: article.title,
      excerpt: article.excerpt,
      byline: article.byline,
      length: article.length,
    }
  } catch (error) {
    getConfig().logger.warn(
      { err: error, url },
      'Readability.js failed to parse article.'
    )
    return null
  }
}

/**
 * Extracts content using CSS selectors with element deduplication
 * @param {cheerio.CheerioAPI} $ - Cheerio instance
 * @param {string|string[]} selectors - CSS selector(s)
 * @returns {Object} Extracted content with metadata
 */
function extractWithSelectors($, selectors) {
  const selectorArray = Array.isArray(selectors) ? selectors : [selectors].filter(Boolean)

  if (selectorArray.length === 0) {
    return { content: '', selectors: [] }
  }

  const usedSelectors = []
  const processedElements = new Set()
  const contentParts = []

  for (const selector of selectorArray) {
    try {
      const elements = $(selector)

      if (elements.length > 0) {
        usedSelectors.push(selector)

        elements.each((_, el) => {
          // Skip if we've already processed this element
          // This prevents duplicate content when selectors overlap
          if (processedElements.has(el)) {
            return
          }
          processedElements.add(el)

          // Clone the element to avoid modifying the original DOM
          const $el = $(el).clone()

          // Remove script, style, and navigation elements
          $el.find('script, style, nav, aside, footer, header').remove()

          const text = $el.text().replace(/\s+/g, ' ').trim()

          if (text && text.length > 50) {
            contentParts.push(text)
          }
        })
      }
    } catch (error) {
      getConfig().logger.warn({ selector, err: error }, 'Selector extraction failed')
    }
  }

  // Content parts are already deduplicated via element tracking
  const finalContent = contentParts.join('\n\n')

  return {
    content: cleanText(finalContent),
    selectors: usedSelectors,
  }
}

/**
 * Detects common content blockers (paywalls, cookie notices, etc.)
 * @param {cheerio.CheerioAPI} $ - Cheerio instance
 * @returns {Object} Blocker detection results
 */
function detectContentBlockers($) {
  const blockers = {
    paywall: false,
    subscription: false,
    cookieNotice: false,
    loginRequired: false,
  }

  const paywallIndicators = [
    '[class*="paywall"]',
    '[id*="paywall"]',
    '[class*="subscription"]',
    '[class*="premium"]',
    '[class*="subscriber"]',
    'meta[name="article:paywall"]',
  ]

  const loginIndicators = [
    '[class*="login-required"]',
    '[class*="sign-in"]',
    '*:contains("sign in to continue")',
  ]

  blockers.paywall = paywallIndicators.some((selector) => $(selector).length > 0)
  blockers.loginRequired = loginIndicators.some((selector) => $(selector).length > 0)
  blockers.cookieNotice = $('[class*="cookie"], [id*="cookie"]').length > 0

  return blockers
}

/**
 * Main content scraping function with intelligent extraction
 * @param {Object} article - Article metadata
 * @param {Object} source - Source configuration
 * @returns {Promise<Object>} Enriched article object
 */
export async function scrapeArticleContent(article, source) {
  const logger = getConfig().logger
  const startTime = Date.now()

  // 1. Try custom extractor first (highest priority)
  if (
    source.extractionMethod === 'custom' &&
    contentExtractorRegistry[source.extractorKey]
  ) {
    try {
      logger.trace({ extractorKey: source.extractorKey }, 'Using custom extractor')
      return await contentExtractorRegistry[source.extractorKey](article, source)
    } catch (error) {
      logger.error(
        { err: error, extractorKey: source.extractorKey },
        'Custom extractor failed'
      )
      // Continue to fallback methods
    }
  }

  // 2. Check RSS content (if sufficient quality)
  if (article.rssContent && article.rssContent.length >= settings.MIN_ARTICLE_CHARS) {
    const cleanedRss = cleanText(article.rssContent)
    if (cleanedRss.length >= settings.MIN_ARTICLE_CHARS) {
      article.articleContent = {
        contents: [cleanedRss],
        method: 'RSS Feed',
        quality: 'medium',
      }
      logger.trace({ headline: article.headline }, 'Using RSS content')
      return article
    }
  }

  // 3. Fetch full page HTML
  const html = await fetchPageWithPlaywright(article.link, 'ContentScraper')
  if (!html) {
    return {
      ...article,
      enrichment_error: 'Playwright failed to fetch page HTML',
      contentPreview: '',
    }
  }

  const $ = cheerio.load(html)

  // 4. Detect content blockers
  const blockers = detectContentBlockers($)

  // 5. Extract using multiple methods
  const selectorResult = extractWithSelectors($, source.articleSelector)
  const readabilityResult = extractWithReadability(article.link, html)

  // 6. Get quality weights (allows per-source tuning)
  const weights = getQualityWeights(source)

  // 7. Calculate quality scores (reuse $ instance for efficiency)
  const candidates = [
    {
      content: selectorResult.content,
      method: 'CSS Selectors',
      quality: calculateContentQuality(selectorResult.content, $, weights),
      metadata: { selectors: selectorResult.selectors },
    },
    {
      content: readabilityResult?.content || '',
      method: 'Readability.js',
      quality: calculateContentQuality(readabilityResult?.content || '', $, weights),
      metadata: {
        title: readabilityResult?.title,
        byline: readabilityResult?.byline,
      },
    },
  ].filter((c) => c.content.length >= settings.MIN_ARTICLE_CHARS)

  // 8. Select best extraction result
  candidates.sort((a, b) => b.quality - a.quality)
  const bestResult = candidates[0]

  // 9. Process results
  if (bestResult) {
    article.articleContent = {
      contents: [bestResult.content],
      method: bestResult.method,
      quality:
        bestResult.quality > 0.7 ? 'high' : bestResult.quality > 0.4 ? 'medium' : 'low',
      metadata: bestResult.metadata,
    }

    const duration = Date.now() - startTime
    logger.trace(
      {
        article: {
          headline: article.headline,
          chars: bestResult.content.length,
          method: bestResult.method,
          quality: bestResult.quality.toFixed(2),
          duration: `${duration}ms`,
        },
      },
      '‚úÖ Content enrichment successful'
    )
  } else {
    // 10. Handle extraction failure
    let errorReason = 'All extraction methods failed'
    const previewContent = selectorResult.content || readabilityResult?.content || ''

    if (blockers.paywall) {
      errorReason = `Paywall detected. Extracted only ${previewContent.length} chars.`
    } else if (blockers.loginRequired) {
      errorReason = `Login required. Content inaccessible.`
    } else if (
      previewContent.length > 0 &&
      previewContent.length < settings.MIN_ARTICLE_CHARS
    ) {
      errorReason = `Content too short (${previewContent.length} chars, minimum ${settings.MIN_ARTICLE_CHARS}).`
    }

    article.enrichment_error = errorReason
    article.contentPreview = previewContent ? previewContent.substring(0, 200) : ''

    // Save debug HTML for troubleshooting
    const filename = `${source.name.replace(/[^a-z0-9]/gi, '_').toLowerCase()}_${Date.now()}_fail.html`
    await saveDebugHtml(filename, html)

    logger.warn(
      {
        article: { headline: article.headline, link: article.link },
        reason: errorReason,
        blockers,
      },
      '‚ùå Content enrichment failed'
    )
  }

  return article
}

```

## üìÑ src/scraper/dynamicExtractor.js
*Lines: 51, Size: 1.83 KB*

```javascript
// packages/scraper-logic/src/scraper/dynamicExtractor.js (version 2.0.1)
import { getConfig } from '../config.js';

/**
 * A generic, data-driven extractor that uses declarative fields from a Source document
 * to extract headline and link information from a Cheerio element.
 * @param {import('cheerio').CheerioAPI} $ - The Cheerio instance.
 * @param {import('cheerio').Element} el - The current DOM element matching the headlineSelector.
 * @param {object} source - The full Source document from the database.
 * @returns {{headline: string, link: string}|null} The extracted article data or null if invalid.
 */
export function dynamicExtractor($, el, source) {
  try {
    const mainElement = $(el)

    // 1. Find the link element and extract the href.
    // If linkSelector is null, the mainElement itself is the link.
    const linkElement = source.linkSelector
      ? mainElement.find(source.linkSelector).first()
      : mainElement
    const link = linkElement.attr('href')

    if (!link) {
      return null // A link is mandatory
    }

    // 2. Find the headline text element and extract the text.
    // If headlineTextSelector is null, the mainElement contains the text.
    const textElement = source.headlineTextSelector
      ? mainElement.find(source.headlineTextSelector).first()
      : mainElement

    // 3. Extract the text and clean it by removing any nested HTML tags.
    let headline = textElement.text().trim().replace(/\s+/g, ' ')

    if (!headline) {
      return null // A headline is mandatory
    }

    // 4. Apply the headline template if it exists
    if (source.headlineTemplate) {
      headline = source.headlineTemplate.replace('{{TEXT}}', headline)
    }

    return { headline, link }
  } catch (error) {
    getConfig().logger.error({ err: error, source: source.name }, 'Error during dynamic extraction.')
    return null
  }
}

```

## üìÑ src/scraper/extractors/index.js
*Lines: 45, Size: 1.66 KB*

```javascript
// packages/scraper-logic/src/scraper/extractors/index.js (version 2.0.0)
// This file uses static imports to be compatible with both Node.js (pipeline) and Webpack (Next.js apps).

// Reusable Extractors
import { simpleExtractor } from './reusable/simple.js';

// Source-Specific Headline Extractors
import { cvcPortfolioExtractor } from './source-specific/cvcPortfolio.js';
import { finansDkExtractor } from './source-specific/finansDk.js';
import { jyllandsPostenExtractor } from './source-specific/jyllandsPosten.js';
import { okonomiskUgebrevExtractor } from './source-specific/okonomiskUgebrev.js';
import { politikenExtractor } from './source-specific/politiken.js';
import { verdaneExtractor } from './source-specific/verdane.js';

// Source-Specific Content Extractors
import { cvcPortfolioContentExtractor } from './source-specific/cvcPortfolioContent.js';

// --- Build Registries ---

export const extractorRegistry = {
  // Reusable
  simple: simpleExtractor,

  // Source-specific
  cvc_portfolio: cvcPortfolioExtractor,
  finans_dk: finansDkExtractor,
  jyllands_posten: jyllandsPostenExtractor,
  okonomisk_ugebrev: okonomiskUgebrevExtractor,
  politiken: politikenExtractor,
  verdane: verdaneExtractor,

  // Manual mapping for legacy keys
  gro_capital: simpleExtractor,
  eifo_dk: simpleExtractor,
  clearwater_dk: simpleExtractor,
  e24: simpleExtractor,
  quotenet_nl: simpleExtractor,
};

export const contentExtractorRegistry = {
  cvc_portfolio_content: cvcPortfolioContentExtractor,
};

console.log(`[Extractor Registry] Statically loaded ${Object.keys(extractorRegistry).length} headline extractors and ${Object.keys(contentExtractorRegistry).length} content extractors.`);

```

## üìÑ src/scraper/extractors/reusable/simple.js
*Lines: 11, Size: 374 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/reusable/simple.js (version 1.0.0)
export const simpleExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.text().trim().replace(/\s+/g, ' ')
  const link = element.attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

```

## üìÑ src/scraper/extractors/source-specific/cvcPortfolio.js
*Lines: 22, Size: 734 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/cvcPortfolio.js (version 1.0.0)
export const cvcPortfolioExtractor = ($, el, site) => {
  const element = $(el);
  if (element.hasClass('portfolio__card-holder--spotlight')) {
    return null;
  }
  const headingElement = element.find('h2.portfolio__card-heading');
  const companyName = headingElement.text().trim();
  const button = element.find('button.js-portfolio-card');
  
  if (companyName && button.length) {
    return { 
        headline: 'CVC Portfolio Company: ' + companyName, 
        link: site.sectionUrl, 
        source: site.name, 
        newspaper: site.name,
        customData: { dataKey: button.attr('data-key') } 
    };
  }
  return null;
};

```

## üìÑ src/scraper/extractors/source-specific/cvcPortfolioContent.js
*Lines: 30, Size: 1.17 KB*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/cvcPortfolioContent.js (version 1.0.0)
import { fetchPageContentFromPopup } from '../../../browser.js';
import { getConfig } from '../../../config.js';
import * as cheerio from 'cheerio';

export const cvcPortfolioContentExtractor = async (article, source) => {
    if (!article.customData?.dataKey) {
        return { ...article, enrichment_error: 'Missing data-key for popup interaction.' };
    }

    const buttonSelector = 'button[data-key="' + article.customData.dataKey + '"]';
    const popupHtml = await fetchPageContentFromPopup(source.sectionUrl, buttonSelector);

    if (!popupHtml) {
        return { ...article, enrichment_error: 'Failed to fetch popup HTML for content.' };
    }

    const $ = cheerio.load(popupHtml);
    const content = $('.rte').text().trim().replace(/\s+/g, ' ');

    if (content) {
        article.articleContent = { contents: [content] };
        getConfig().logger.trace({ article: { headline: article.headline } }, '‚úÖ CVC custom content extraction successful.');
    } else {
        article.enrichment_error = 'Could not find content in the CVC popup.';
    }

    return article;
}

```

## üìÑ src/scraper/extractors/source-specific/finansDk.js
*Lines: 8, Size: 268 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/finansDk.js (version 1.0.0)
export const finansDkExtractor = ($, el, site) => ({
  headline: $(el).text().trim(),
  link: $(el).closest('a').attr('href'),
  source: site.name,
  newspaper: site.name,
})

```

## üìÑ src/scraper/extractors/source-specific/jyllandsPosten.js
*Lines: 11, Size: 397 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/jyllandsPosten.js (version 1.0.0)
export const jyllandsPostenExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h3').text().trim()
  const link = element.find('a').attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

```

## üìÑ src/scraper/extractors/source-specific/okonomiskUgebrev.js
*Lines: 12, Size: 444 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/okonomiskUgebrev.js (version 1.0.0)
export const okonomiskUgebrevExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h5.elementor-heading-title').text().trim().replace(/\s+/g, ' ');
  const link = element.attr('href');
  
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name };
  }
  return null;
};

```

## üìÑ src/scraper/extractors/source-specific/politiken.js
*Lines: 8, Size: 391 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/politiken.js (version 1.0.0)
export const politikenExtractor = ($, el, site) => {
  const element = $(el);
  const h = element.find('h2, h3, h4').first().text().trim()
  const a = element.find('a[href*="/art"]').first().attr('href')
  return h && a ? { headline: h, link: a, source: site.name, newspaper: site.name } : null
}

```

## üìÑ src/scraper/extractors/source-specific/verdane.js
*Lines: 16, Size: 516 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/verdane.js (version 1.0.0)
export const verdaneExtractor = ($, el, site) => {
  const element = $(el);
  const linkEl = element.find('a.wp-block-klingit-the-product-block-link')
  const companyName = linkEl.find('h3.wp-block-post-title').text().trim()
  if (companyName) {
    return {
      headline: 'Verdane invests in ' + companyName,
      link: linkEl.attr('href'),
      source: site.name,
      newspaper: site.name,
    }
  }
  return null
}

```

## üìÑ src/scraper/headlineScraper.js
*Lines: 608, Size: 15.59 KB*

```javascript
// packages/scraper-logic/src/scraper/headlineScraper.js (version 6.0.0)
import * as cheerio from 'cheerio'
import axios from 'axios'
import fs from 'fs/promises'
import path from 'path'
import Parser from 'rss-parser'
import { Source } from '../../../models/src/index.js'
import { BROWSER_HEADERS } from './constants.js'
import { fetchPageWithPlaywright } from '../browser.js'
import { getConfig } from '../config.js'
import { extractorRegistry } from './extractors/index.js'
import { dynamicExtractor } from './dynamicExtractor.js'

/**
 * RSS parser with custom field mappings for content extraction
 */
const rssParser = new Parser({
  customFields: {
    item: [
      ['content:encoded', 'contentEncoded'],
      ['media:content', 'mediaContent'],
      ['description', 'description'],
    ],
  },
  timeout: 15000,
  maxRedirects: 3,
})

/**
 * Saves HTML to debug directory with timestamp for troubleshooting
 * @param {string} filename - Base filename
 * @param {string} html - HTML content
 * @param {Object} metadata - Additional context to save
 * @returns {Promise<string|null>} Path to saved file
 */
async function saveDebugHtml(filename, html, metadata = {}) {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) return null

  try {
    await fs.mkdir(debugDir, { recursive: true })

    // Add timestamp to prevent overwriting
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
    const baseFilename = filename.replace(/[^a-z0-9]/gi, '_').toLowerCase()
    const htmlPath = path.join(debugDir, `${baseFilename}_${timestamp}.html`)
    const metaPath = path.join(debugDir, `${baseFilename}_${timestamp}.json`)

    await fs.writeFile(htmlPath, html)

    // Save metadata for context
    if (Object.keys(metadata).length > 0) {
      await fs.writeFile(metaPath, JSON.stringify(metadata, null, 2))
    }

    return htmlPath
  } catch (error) {
    config.logger.error({ err: error, file: filename }, 'Failed to save debug HTML.')
    return null
  }
}

/**
 * Cleans and normalizes RSS content text
 * @param {string} htmlContent - Raw HTML content from RSS
 * @returns {string} Cleaned text content
 */
function cleanRssContent(htmlContent) {
  if (!htmlContent) return ''

  const $ = cheerio.load(htmlContent)

  // Remove common unwanted elements
  $('script, style, iframe, img').remove()

  return $.text()
    .replace(/\s+/g, ' ')
    .replace(/\n{3,}/g, '\n\n')
    .trim()
}

/**
 * Validates and normalizes article URLs
 * @param {string} url - Raw URL from feed or extraction
 * @param {string} baseUrl - Base URL for relative links
 * @returns {string|null} Normalized URL or null if invalid
 */
function normalizeUrl(url, baseUrl) {
  if (!url) return null

  try {
    // Handle relative URLs
    const normalized = new URL(url, baseUrl).href

    // Basic validation - must be http(s)
    if (!normalized.startsWith('http://') && !normalized.startsWith('https://')) {
      return null
    }

    return normalized
  } catch (error) {
    getConfig().logger.debug(
      { url, baseUrl, err: error.message },
      'URL normalization failed'
    )
    return null
  }
}

/**
 * Fetches and parses RSS feed with enhanced error handling
 * @param {Object} source - Source configuration
 * @returns {Promise<Object>} Articles array and error status
 */
async function fetchHeadlinesViaRss(source) {
  const logger = getConfig().logger
  const startTime = Date.now()

  try {
    logger.debug({ url: source.rssUrl }, 'Fetching RSS feed')

    const feed = await rssParser.parseURL(source.rssUrl)

    if (!feed.items || feed.items.length === 0) {
      throw new Error('RSS feed was empty or invalid.')
    }

    const articles = feed.items
      .map((item) => {
        // Try multiple content fields in order of preference
        const rssContentHtml =
          item.contentEncoded ||
          item.content ||
          item.description ||
          item.contentSnippet ||
          ''

        const rssContent = cleanRssContent(rssContentHtml)
        const link = normalizeUrl(item.link || item.guid, source.baseUrl)

        // Validate required fields
        if (!item.title?.trim() || !link) {
          return null
        }

        return {
          headline: item.title.trim(),
          link,
          rssContent: rssContent || null,
          pubDate: item.pubDate || item.isoDate || null,
        }
      })
      .filter(Boolean) // Remove null entries

    const duration = Date.now() - startTime

    logger.info(
      {
        source: source.name,
        count: articles.length,
        duration: `${duration}ms`,
      },
      'RSS feed parsed successfully'
    )

    return { articles, error: null }
  } catch (error) {
    const duration = Date.now() - startTime
    const failureReason = error.message || 'Unknown RSS error.'

    logger.warn(
      {
        url: source.rssUrl,
        reason: failureReason,
        duration: `${duration}ms`,
      },
      `RSS feed parsing failed for "${source.name}". Auto-disabling.`
    )

    // Auto-disable failing RSS feeds
    try {
      await Source.updateOne(
        { _id: source._id },
        {
          $set: {
            rssUrl: null,
            notes:
              `${source.notes || ''}\n[${new Date().toISOString()}] RSS URL auto-disabled: ${failureReason}`.trim(),
            lastRssError: {
              message: failureReason,
              timestamp: new Date(),
              attempts: (source.lastRssError?.attempts || 0) + 1,
            },
          },
        }
      )
      logger.info({ source: source.name }, 'RSS URL disabled in database')
    } catch (dbError) {
      logger.error(
        { err: dbError, source: source.name },
        'Failed to auto-disable RSS URL in database'
      )
    }

    return { articles: [], error: failureReason }
  }
}

/**
 * Fetches page using fast static method (axios)
 * @param {string} url - URL to fetch
 * @param {number} timeout - Request timeout in ms
 * @returns {Promise<Object>} HTML content and error status
 */
async function fetchPageStatic(url, timeout = 25000) {
  const logger = getConfig().logger
  const startTime = Date.now()

  try {
    const { data, status, headers } = await axios.get(url, {
      headers: BROWSER_HEADERS,
      timeout,
      maxRedirects: 5,
      validateStatus: (status) => status < 400, // Accept redirects but not errors
    })

    const duration = Date.now() - startTime

    logger.debug(
      {
        url,
        status,
        contentType: headers['content-type'],
        size: data.length,
        duration: `${duration}ms`,
      },
      'Static fetch successful'
    )

    return { html: data, error: null }
  } catch (error) {
    const duration = Date.now() - startTime
    const errorDetails = {
      message: error.message,
      code: error.code,
      status: error.response?.status,
      duration: `${duration}ms`,
    }

    logger.warn({ url, err: errorDetails }, 'Static fetch failed')

    return {
      html: null,
      error: `Static fetch failed: ${error.message}`,
    }
  }
}

/**
 * Fetches page using Playwright browser automation
 * @param {Object} source - Source configuration
 * @returns {Promise<Object>} HTML content and error status
 */
async function fetchWithPlaywrightWrapped(source) {
  const logger = getConfig().logger
  const startTime = Date.now()

  try {
    const html = await fetchPageWithPlaywright(source.sectionUrl, 'HeadlineScraper', {
      timeout: source.playwrightTimeoutMs,
      waitForSelector: source.waitForSelector,
    })

    if (!html) {
      throw new Error('Playwright returned empty content')
    }

    const duration = Date.now() - startTime

    logger.debug(
      {
        url: source.sectionUrl,
        size: html.length,
        duration: `${duration}ms`,
      },
      'Playwright fetch successful'
    )

    return { html, error: null }
  } catch (error) {
    const duration = Date.now() - startTime

    logger.warn(
      {
        url: source.sectionUrl,
        err: error.message,
        duration: `${duration}ms`,
      },
      'Playwright fetch failed'
    )

    return {
      html: null,
      error: `Playwright failed: ${error.message}`,
    }
  }
}

/**
 * Extracts articles using JSON-LD structured data
 * @param {CheerioAPI} $ - Cheerio instance
 * @param {Object} source - Source configuration
 * @returns {Array} Extracted articles
 */
function extractFromJsonLd($, source) {
  const articles = []
  const processedUrls = new Set()

  $('script[type="application/ld+json"]').each((_, el) => {
    try {
      const jsonData = JSON.parse($(el).html())

      // Handle both direct objects and @graph arrays
      const potentialLists = [jsonData, ...(jsonData['@graph'] || [])]

      potentialLists.forEach((item) => {
        // Handle ItemList structures
        const items = item?.itemListElement || (Array.isArray(item) ? item : [item])
        const itemArray = Array.isArray(items) ? items : [items]

        itemArray.forEach((element) => {
          // Support multiple JSON-LD structures
          const headline =
            element.name ||
            element.headline ||
            element.item?.name ||
            element.item?.headline

          const url = element.url || element.item?.url || element['@id']

          if (headline && url) {
            const normalizedUrl = normalizeUrl(url, source.baseUrl)

            // Deduplicate by URL
            if (normalizedUrl && !processedUrls.has(normalizedUrl)) {
              processedUrls.add(normalizedUrl)
              articles.push({
                headline: headline.trim(),
                link: normalizedUrl,
                description: element.description || element.item?.description || null,
              })
            }
          }
        })
      })
    } catch (error) {
      getConfig().logger.debug(
        { err: error.message },
        'JSON-LD parsing error (non-critical)'
      )
    }
  })

  return articles
}

/**
 * Extracts articles using CSS selectors (declarative or custom extractors)
 * @param {CheerioAPI} $ - Cheerio instance
 * @param {Object} source - Source configuration
 * @returns {Array} Extracted articles
 */
function extractFromSelectors($, source) {
  const articles = []
  const processedUrls = new Set()

  // Normalize selectors to array
  const selectors = Array.isArray(source.headlineSelector)
    ? source.headlineSelector
    : [source.headlineSelector].filter(Boolean)

  if (selectors.length === 0) {
    getConfig().logger.warn({ source: source.name }, 'No headline selectors defined')
    return articles
  }

  // Get appropriate extractor function
  const extractorFn =
    source.extractionMethod === 'custom'
      ? extractorRegistry[source.extractorKey]
      : dynamicExtractor

  if (!extractorFn) {
    getConfig().logger.error(
      {
        source: source.name,
        method: source.extractionMethod,
        key: source.extractorKey,
      },
      'No valid extractor function found'
    )
    return articles
  }

  // Process each selector
  for (const selector of selectors) {
    try {
      const elements = $(selector)

      if (elements.length === 0) {
        getConfig().logger.debug(
          { selector, source: source.name },
          'Selector matched 0 elements'
        )
        continue
      }

      elements.each((_, el) => {
        try {
          const articleData = extractorFn($, el, source)

          if (!articleData?.headline || !articleData?.link) {
            return // Skip invalid extractions
          }

          const normalizedUrl = normalizeUrl(articleData.link, source.baseUrl)

          // Deduplicate and validate
          if (normalizedUrl && !processedUrls.has(normalizedUrl)) {
            processedUrls.add(normalizedUrl)
            articles.push({
              ...articleData,
              link: normalizedUrl,
            })
          }
        } catch (error) {
          getConfig().logger.debug(
            { err: error.message, selector },
            'Element extraction failed'
          )
        }
      })
    } catch (error) {
      getConfig().logger.warn(
        { err: error.message, selector },
        'Selector processing failed'
      )
    }
  }

  return articles
}

/**
 * Main function to scrape headlines from a source
 * @param {Object} source - Source configuration object
 * @returns {Promise<Object>} Scraping results with articles and metadata
 */
export async function scrapeSiteForHeadlines(source) {
  const logger = getConfig().logger
  const startTime = Date.now()

  // 1. Try RSS first if available
  if (source.rssUrl) {
    logger.info({ source: source.name }, 'Attempting RSS scrape')

    const rssResult = await fetchHeadlinesViaRss(source)

    if (rssResult.articles.length > 0) {
      const duration = Date.now() - startTime

      logger.info(
        {
          source: source.name,
          count: rssResult.articles.length,
          duration: `${duration}ms`,
        },
        '‚úÖ RSS scrape successful'
      )

      return {
        articles: rssResult.articles,
        success: true,
        resultCount: rssResult.articles.length,
        error: null,
        method: 'RSS',
        duration,
      }
    }

    logger.warn(
      { source: source.name },
      'RSS scrape failed. Falling back to HTML scraping.'
    )
  }

  // 2. Fall back to HTML scraping
  const fetcher = source.isStatic
    ? () => fetchPageStatic(source.sectionUrl, source.staticTimeoutMs)
    : () => fetchWithPlaywrightWrapped(source)

  const fetcherName = source.isStatic ? 'STATIC (axios)' : 'PLAYWRIGHT (browser)'

  logger.info({ source: source.name, method: fetcherName }, 'Initiating HTML scrape')

  const { html, error: fetchError } = await fetcher()

  if (!html) {
    const duration = Date.now() - startTime

    logger.error(
      {
        source: source.name,
        error: fetchError,
        duration: `${duration}ms`,
      },
      '‚ùå Page fetch failed'
    )

    return {
      articles: [],
      success: false,
      error: fetchError,
      debugHtml: null,
      method: fetcherName,
      duration,
    }
  }

  // 3. Parse HTML and extract articles
  const $ = cheerio.load(html)
  let articles = []

  try {
    if (source.extractionMethod === 'json-ld') {
      articles = extractFromJsonLd($, source)
    } else {
      // 'declarative' or 'custom'
      articles = extractFromSelectors($, source)
    }
  } catch (error) {
    logger.error(
      {
        source: source.name,
        method: source.extractionMethod,
        err: error.message,
      },
      'Article extraction failed'
    )
  }

  // 4. Deduplicate articles by URL (final safety check)
  const uniqueArticles = Array.from(new Map(articles.map((a) => [a.link, a])).values())

  const duration = Date.now() - startTime

  // 5. Handle extraction failure
  if (uniqueArticles.length === 0) {
    const debugPath = await saveDebugHtml(`${source.name}_headline_fail`, html, {
      source: source.name,
      url: source.sectionUrl,
      method: source.extractionMethod,
      selectors: source.headlineSelector,
      timestamp: new Date().toISOString(),
      error: 'Extracted 0 headlines',
    })

    logger.error(
      {
        source: source.name,
        debugPath,
        duration: `${duration}ms`,
      },
      '‚ùå Extracted 0 headlines'
    )

    return {
      articles: [],
      success: false,
      error: 'Extracted 0 headlines.',
      debugHtml: html,
      method: fetcherName,
      duration,
    }
  }

  // 6. Success!
  logger.info(
    {
      source: source.name,
      count: uniqueArticles.length,
      method: source.extractionMethod,
      duration: `${duration}ms`,
    },
    '‚úÖ HTML scrape successful'
  )

  return {
    articles: uniqueArticles,
    success: true,
    resultCount: uniqueArticles.length,
    error: null,
    method: fetcherName,
    duration,
  }
}

```

## üìÑ src/scraper/index.js
*Lines: 12, Size: 395 Bytes*

```javascript
// packages/scraper-logic/src/scraper/index.js (Corrected Exports)
import { scrapeSiteForHeadlines } from './headlineScraper.js'
import { scrapeArticleContent } from './contentScraper.js'
import { testHeadlineExtraction, scrapeArticleContentForTest } from './test-helpers.js'

export {
  scrapeSiteForHeadlines,
  scrapeArticleContent,
  testHeadlineExtraction,
  scrapeArticleContentForTest,
}

```

## üìÑ src/scraper/newsApiScraper.js
*Lines: 424, Size: 11.87 KB*

```javascript
// packages/scraper-logic/src/scraper/newsApiScraper.js (version 3.0.0)
import NewsAPI from 'newsapi'
import { getConfig } from '../config.js'
import { Source, WatchlistEntity } from '../../../models/src/index.js'
import { env } from '../../../config/src/index.js'
import colors from 'ansi-colors'

/**
 * Configuration constants for NewsAPI
 */
const NEWSAPI_CONFIG = {
  MAX_QUERY_LENGTH: 490, // NewsAPI query length limit
  MAX_QUERIES_PER_REQUEST: 4, // Rate limit protection
  DEFAULT_LANGUAGES: 'en,da,sv,no',
  DEFAULT_PAGE_SIZE: 100,
  DEFAULT_TIME_WINDOW_HOURS: 24,
  REQUEST_TIMEOUT: 30000, // 30 seconds
  MAX_RETRIES: 2,
}

/**
 * Fetches and combines watchlist from active sources and entities
 * @returns {Promise<string[]>} Array of unique watchlist keywords
 */
async function getWatchlist() {
  const logger = getConfig().logger
  const startTime = Date.now()

  try {
    const [sources, richListTargets] = await Promise.all([
      Source.find({
        status: 'active',
        country: { $in: ['Denmark', 'Global PE', 'M&A Aggregators'] },
      })
        .select('name')
        .lean()
        .maxTimeMS(10000), // 10 second timeout

      WatchlistEntity.find({ status: 'active' }).select('name').lean().maxTimeMS(10000),
    ])

    // Extract and clean names
    const sourceNames = sources.map((s) => s.name.split('(')[0].trim()).filter(Boolean)

    const richListNames = richListTargets
      .map((t) => t.name.split('(')[0].trim())
      .filter(Boolean)

    // Combine and deduplicate
    const watchlist = [...new Set([...sourceNames, ...richListNames])].sort() // Alphabetical sort for consistent query generation

    const duration = Date.now() - startTime

    logger.info(
      {
        sources: sourceNames.length,
        entities: richListNames.length,
        total: watchlist.length,
        duration: `${duration}ms`,
      },
      'Watchlist compiled successfully'
    )

    logger.trace({ keywords: watchlist }, 'Full NewsAPI watchlist keywords')

    return watchlist
  } catch (error) {
    logger.error({ err: error }, 'Failed to fetch watchlist from database')
    throw error
  }
}

/**
 * Sanitizes a keyword for use in NewsAPI queries
 * @param {string} keyword - Raw keyword
 * @returns {string} Sanitized and quoted keyword
 */
function sanitizeKeyword(keyword) {
  if (!keyword || typeof keyword !== 'string') return null

  return keyword
    .replace(/&/g, ' ') // Replace ampersands
    .replace(/[()]/g, '') // Remove parentheses
    .replace(/\s+/g, ' ') // Normalize whitespace
    .trim()
}

/**
 * Builds optimized query batches from watchlist keywords
 * Respects NewsAPI query length limits and rate limiting
 * @param {string[]} watchlist - Array of keywords
 * @param {number} maxQueryLength - Maximum query length
 * @param {number} maxQueries - Maximum number of queries to return
 * @returns {string[]} Array of query strings
 */
function buildQueryBatches(
  watchlist,
  maxQueryLength = NEWSAPI_CONFIG.MAX_QUERY_LENGTH,
  maxQueries = NEWSAPI_CONFIG.MAX_QUERIES_PER_REQUEST
) {
  const logger = getConfig().logger
  const queries = []
  let currentBatch = []
  let skippedKeywords = []

  for (const keyword of watchlist) {
    const sanitizedKeyword = sanitizeKeyword(keyword)

    if (!sanitizedKeyword) {
      skippedKeywords.push(keyword)
      continue
    }

    const quotedKeyword = `"${sanitizedKeyword}"`

    // Check if adding this keyword would exceed the limit
    const potentialQuery = [...currentBatch, quotedKeyword].join(' OR ')

    if (potentialQuery.length > maxQueryLength) {
      // Save current batch if it has content
      if (currentBatch.length > 0) {
        queries.push(currentBatch.join(' OR '))
      }

      // Start new batch with current keyword
      // If single keyword is too long, truncate it
      if (quotedKeyword.length > maxQueryLength) {
        const truncated = `"${sanitizedKeyword.substring(0, maxQueryLength - 3)}"`
        logger.warn({ original: keyword, truncated }, 'Keyword too long, truncated')
        currentBatch = [truncated]
      } else {
        currentBatch = [quotedKeyword]
      }
    } else {
      currentBatch.push(quotedKeyword)
    }
  }

  // Add final batch
  if (currentBatch.length > 0) {
    queries.push(currentBatch.join(' OR '))
  }

  // Log skipped keywords
  if (skippedKeywords.length > 0) {
    logger.debug(
      { count: skippedKeywords.length, keywords: skippedKeywords },
      'Skipped invalid keywords'
    )
  }

  // Handle query limit
  const queriesToUse = queries.slice(0, maxQueries)
  const skippedQueries = queries.slice(maxQueries)

  if (skippedQueries.length > 0) {
    logger.warn(
      {
        total: queries.length,
        used: queriesToUse.length,
        skipped: skippedQueries.length,
      },
      `Watchlist generated ${queries.length} queries, using first ${maxQueries} to avoid rate limits`
    )

    // Visual breakdown of queries
    let logMessage = '[NewsAPI] Query Breakdown:\n'
    queries.forEach((q, i) => {
      const inUse = i < maxQueries
      const status = inUse ? colors.green('‚úì IN USE') : colors.gray('‚úó SKIPPED')
      const preview = q.length > 100 ? q.substring(0, 97) + '...' : q
      logMessage += `  ${status} Query ${i + 1} (${q.length} chars): ${preview}\n`
    })
    logger.info(logMessage)
  } else {
    logger.info(
      { count: queriesToUse.length },
      `Built ${queriesToUse.length} optimized queries`
    )
  }

  return queriesToUse
}

/**
 * Executes a single NewsAPI query with retry logic
 * @param {NewsAPI} newsapi - NewsAPI client instance
 * @param {Object} params - Query parameters
 * @param {number} retryCount - Current retry attempt
 * @returns {Promise<Object>} API response
 */
async function executeQuery(newsapi, params, retryCount = 0) {
  const logger = getConfig().logger

  try {
    const response = await Promise.race([
      newsapi.v2.everything(params),
      new Promise((_, reject) =>
        setTimeout(
          () => reject(new Error('Request timeout')),
          NEWSAPI_CONFIG.REQUEST_TIMEOUT
        )
      ),
    ])

    return response
  } catch (error) {
    if (retryCount < NEWSAPI_CONFIG.MAX_RETRIES) {
      const delay = Math.pow(2, retryCount) * 1000 // Exponential backoff

      logger.warn(
        {
          attempt: retryCount + 1,
          maxRetries: NEWSAPI_CONFIG.MAX_RETRIES,
          delay: `${delay}ms`,
          err: error.message,
        },
        'Query failed, retrying...'
      )

      await new Promise((resolve) => setTimeout(resolve, delay))
      return executeQuery(newsapi, params, retryCount + 1)
    }

    throw error
  }
}

/**
 * Normalizes and validates article data from NewsAPI response
 * @param {Object} rawArticle - Raw article from NewsAPI
 * @returns {Object|null} Normalized article or null if invalid
 */
function normalizeArticle(rawArticle) {
  if (!rawArticle?.title || !rawArticle?.url) {
    return null
  }

  // Filter out removed/deleted articles
  if (rawArticle.title === '[Removed]' || rawArticle.url.includes('removed')) {
    return null
  }

  return {
    headline: rawArticle.title.trim(),
    link: rawArticle.url,
    source: rawArticle.source?.name || 'Unknown',
    newspaper: rawArticle.source?.name || 'Unknown',
    description: rawArticle.description || null,
    publishedAt: rawArticle.publishedAt || null,
    author: rawArticle.author || null,
    imageUrl: rawArticle.urlToImage || null,
  }
}

/**
 * Main function to scrape articles from NewsAPI
 * @param {Object} options - Configuration options
 * @returns {Promise<Array>} Array of unique articles
 */
export async function scrapeNewsAPI(options = {}) {
  const logger = getConfig().logger
  const startTime = Date.now()

  // Validate API key
  if (!env.NEWSAPI_API_KEY) {
    logger.error('NEWSAPI_API_KEY is not configured')
    return []
  }

  const newsapi = new NewsAPI(env.NEWSAPI_API_KEY)

  // Merge options with defaults
  const config = {
    languages: options.languages || NEWSAPI_CONFIG.DEFAULT_LANGUAGES,
    pageSize: options.pageSize || NEWSAPI_CONFIG.DEFAULT_PAGE_SIZE,
    timeWindowHours: options.timeWindowHours || NEWSAPI_CONFIG.DEFAULT_TIME_WINDOW_HOURS,
    maxQueries: options.maxQueries || NEWSAPI_CONFIG.MAX_QUERIES_PER_REQUEST,
  }

  try {
    // 1. Build watchlist and queries
    logger.info('üì∞ Starting NewsAPI scrape')

    const watchlist = await getWatchlist()

    if (watchlist.length === 0) {
      logger.warn('Watchlist is empty, skipping NewsAPI scrape')
      return []
    }

    const queryBatches = buildQueryBatches(
      watchlist,
      NEWSAPI_CONFIG.MAX_QUERY_LENGTH,
      config.maxQueries
    )

    if (queryBatches.length === 0) {
      logger.warn('No valid queries generated from watchlist')
      return []
    }

    logger.info(
      { queries: queryBatches.length },
      `Dispatching ${queryBatches.length} batched queries`
    )

    // 2. Calculate time window
    const fromDate = new Date(
      Date.now() - config.timeWindowHours * 60 * 60 * 1000
    ).toISOString()

    // 3. Execute all queries with retry logic
    const queryPromises = queryBatches.map((query, index) =>
      executeQuery(newsapi, {
        q: query,
        language: config.languages,
        sortBy: 'publishedAt',
        from: fromDate,
        pageSize: config.pageSize,
      }).catch((error) => {
        logger.error(
          {
            queryIndex: index + 1,
            err: error.message,
          },
          'Query execution failed'
        )
        return { status: 'error', articles: [], error }
      })
    )

    const allResponses = await Promise.all(queryPromises)

    // 4. Process responses
    let allArticles = []
    let successfulQueries = 0
    let failedQueries = 0

    for (const [index, response] of allResponses.entries()) {
      if (response.status === 'ok') {
        successfulQueries++
        allArticles.push(...response.articles)

        logger.debug(
          {
            query: index + 1,
            articles: response.articles.length,
            totalResults: response.totalResults,
          },
          'Query successful'
        )
      } else {
        failedQueries++
        logger.error(
          {
            query: index + 1,
            code: response.code,
            message: response.message,
          },
          'Query returned error status'
        )
      }
    }

    // 5. Normalize and deduplicate articles
    const normalizedArticles = allArticles.map(normalizeArticle).filter(Boolean)

    const uniqueArticles = Array.from(
      new Map(normalizedArticles.map((a) => [a.link, a])).values()
    )

    const duration = Date.now() - startTime

    // 6. Log results
    if (uniqueArticles.length === 0) {
      logger.info(
        {
          duration: `${duration}ms`,
          queries: { successful: successfulQueries, failed: failedQueries },
        },
        'No new articles found matching watchlist'
      )
      return []
    }

    logger.info(
      {
        articles: {
          raw: allArticles.length,
          normalized: normalizedArticles.length,
          unique: uniqueArticles.length,
        },
        queries: { successful: successfulQueries, failed: failedQueries },
        duration: `${duration}ms`,
      },
      `‚úÖ NewsAPI scrape completed: ${uniqueArticles.length} unique articles`
    )

    return uniqueArticles
  } catch (error) {
    const duration = Date.now() - startTime

    // Handle specific error types
    if (error.name?.includes('rateLimited') || error.code === 'rateLimited') {
      logger.warn(
        { duration: `${duration}ms` },
        'NewsAPI rate limit reached (expected on free tier). Some watchlist items may have been missed.'
      )
    } else if (error.code === 'apiKeyInvalid') {
      logger.error('NewsAPI key is invalid or expired')
    } else if (error.code === 'parametersMissing') {
      logger.error({ err: error }, 'Invalid query parameters')
    } else {
      logger.error(
        { err: error, duration: `${duration}ms` },
        'Critical error during NewsAPI scraping'
      )
    }

    return []
  }
}

```

## üìÑ src/scraper/orchestrator.js
*Lines: 105, Size: 3.46 KB*

```javascript
// packages/scraper-logic/src/scraper/orchestrator.js (version 5.0.0)
import pLimit from 'p-limit'
import { sleep } from '@shared/utils-shared'
import { getConfig } from '../config.js'
import { scrapeSiteForHeadlines } from './headlineScraper.js'
// NEWSAPI REWORK: The direct import of scrapeNewsAPI is removed as it's no longer used for proactive scraping.
// import { scrapeNewsAPI } from './newsApiScraper.js'
import { updateSourceAnalyticsBatch } from '../../../data-access/src/index.js'
import { env } from '../../../config/src/index.js'

async function performStandardScraping(sourcesToScrape) {
  if (sourcesToScrape.length === 0) {
    return { scrapedArticles: [], scraperHealth: [] }
  }

  const limit = pLimit(env.CONCURRENCY_LIMIT || 3)
  getConfig().logger.info(
    `Pipeline will now scrape ${sourcesToScrape.length} active standard sources.`
  )

  let allArticles = []
  const scraperHealthMap = new Map()

  const promises = sourcesToScrape.map((source) =>
    limit(async () => {
      getConfig().logger.info(`[Scraping] -> Starting scrape for "${source.name}"...`)
      const result = await scrapeSiteForHeadlines(source)
      const foundCount = result.resultCount !== undefined ? result.resultCount : 0
      getConfig().logger.info(
        `[Scraping] <- Finished scrape for "${source.name}". Success: ${result.success}, Found: ${foundCount}`
      )
      return { source, result }
    })
  )
  const results = await Promise.all(promises)

  const bulkUpdateOps = []

  for (const { source, result } of results) {
    const healthReport = {
      source: source.name,
      success: result.success && result.resultCount > 0,
      count: result.resultCount || 0,
      error: result.error,
      debugHtml: result.debugHtml,
      failedSelector: result.success ? null : source.headlineSelector,
    }
    scraperHealthMap.set(source.name, healthReport)

    if (healthReport.success) {
      allArticles.push(
        ...result.articles.map((a) => ({
          ...a,
          source: source.name,
          newspaper: source.name,
          country: source.country,
        }))
      )
      bulkUpdateOps.push({
        updateOne: {
          filter: { _id: source._id },
          update: { $set: { lastScrapedAt: new Date(), lastSuccessAt: new Date() } },
        },
      })
    } else {
      getConfig().logger.warn(
        `[Scraping] ‚ùå FAILED for "${source.name}": ${result.error || 'Extracted 0 headlines.'}.`
      )
      bulkUpdateOps.push({
        updateOne: {
          filter: { _id: source._id },
          update: { $set: { lastScrapedAt: new Date() } },
        },
      })
    }
  }

  if (bulkUpdateOps.length > 0) {
    await updateSourceAnalyticsBatch(bulkUpdateOps)
  }

  return {
    scrapedArticles: allArticles,
    scraperHealth: Array.from(scraperHealthMap.values()),
  }
}

// NEWSAPI REWORK: The main orchestrator is simplified. It no longer calls scrapeNewsAPI.
// Its sole responsibility is now to manage the standard scraping process. This aligns
// with the new strategy of using external APIs only for enrichment, not discovery.
export async function scrapeAllHeadlines(sourcesToScrape) {
  const { scrapedArticles, scraperHealth } =
    await performStandardScraping(sourcesToScrape)

  const uniqueArticles = Array.from(
    new Map(scrapedArticles.map((a) => [a.link, a])).values()
  )

  getConfig().logger.info(
    `Scraping complete. Found ${uniqueArticles.length} unique articles from standard sources.`
  )

  return { allArticles: uniqueArticles, scraperHealth }
}

```

## üìÑ src/scraper/selectorOptimizer.js
*Lines: 98, Size: 3.19 KB*

```javascript
// packages/scraper-logic/src/scraper/selectorOptimizer.js (version 4.2)
import * as cheerio from 'cheerio';

const NEGATIVE_TAGS = ['nav', 'footer', 'aside', 'header', 'form', '.popup-overlay'];

/**
 * Finds clusters of repeated elements by analyzing class name frequency.
 * CRITICALLY, it filters out Tailwind-style classes with colons.
 */
function findRepeatingClassSelectors($) {
    const classCounts = {};
    $('*').each((_, el) => {
        const classes = $(el).attr('class');
        if (classes) {
            classes.trim().split(/\s+/).forEach(cls => {
                // DEFINITIVE FIX: Ignore any class containing a colon to prevent pseudo-class errors.
                if (cls.length > 5 && !cls.includes(':') && !cls.startsWith('js-')) {
                    classCounts[cls] = (classCounts[cls] || 0) + 1;
                }
            });
        }
    });

    return Object.entries(classCounts)
        .filter(([_, count]) => count > 3 && count < 100)
        .sort((a, b) => b[1] - a[1])
        .slice(0, 15) // Widen the search slightly
        .map(([cls]) => `.${cls}`);
}

/**
 * For a given container element, finds the most likely headline text.
 */
function analyzeContainer($container) {
    const headlineEl = $container.find('h1, h2, h3, h4, h5').first();
    let text = headlineEl.text().trim().replace(/\s+/g, ' ');

    if (!text) {
        // Fallback for non-heading elements
        text = $container.text().trim().replace(/\s+/g, ' ');
    }

    // Ensure it's a clickable container
    const isClickable = $container.is('a[href]') || $container.find('a[href]').length > 0 || $container.find('button[data-key]').length > 0;

    if (text && isClickable) {
        return { text };
    }
    return null;
}

export function heuristicallyFindSelectors(html) {
    const $ = cheerio.load(html);
    $(NEGATIVE_TAGS.join(',')).remove();

    const potentialListSelectors = findRepeatingClassSelectors($);
    const clusters = [];

    // Add the CVC-specific selector as a high-priority candidate, as it is a known good pattern.
    potentialListSelectors.unshift('.portfolio__card-holder');

    for (const selector of potentialListSelectors) {
        try {
            const elements = $(selector);
            if (elements.length < 3) continue;

            const samples = [];
            let validItems = 0;

            elements.each((_, el) => {
                const containerData = analyzeContainer($(el));
                if (containerData) {
                    samples.push(containerData.text);
                    validItems++;
                }
            });

            if (validItems > 2 && (validItems / elements.length) > 0.5) {
                clusters.push({
                    selector: selector,
                    score: validItems * (validItems / elements.length),
                    samples: samples,
                });
            }
        } catch (e) {
            // Silently ignore errors from invalid selectors that might still slip through
        }
    }

    if (clusters.length === 0) {
        return [];
    }
    
    const uniqueClusters = [...new Map(clusters.map(item => [item.selector, item])).values()];

    return uniqueClusters.sort((a, b) => b.score - a.score).slice(0, 5);
}

```

## üìÑ src/scraper/test-helpers.js
*Lines: 93, Size: 3.56 KB*

```javascript
// packages/scraper-logic/src/scraper/test-helpers.js (version 2.0)
import * as cheerio from 'cheerio'
import { dynamicExtractor } from './dynamicExtractor.js'
import { extractorRegistry } from './extractors/index.js'
import { fetchPageWithPlaywright } from '../browser.js'

export async function scrapeArticleContentForTest(articleUrl, articleSelectors) {
  if (!articleUrl || !articleSelectors || articleSelectors.length === 0) return ''
  try {
    const html = await fetchPageWithPlaywright(articleUrl, 'TestContentScraper')
    if (!html) return 'Error: Failed to fetch page HTML.';
    
    const $ = cheerio.load(html)
    const selectors = Array.isArray(articleSelectors) ? articleSelectors : [articleSelectors];
    let contentParts = [];
    
    for (const selector of selectors) {
        $(selector).each((_, el) => {
            contentParts.push($(el).text().trim());
        });
    }

    if(contentParts.length > 0) {
        const content = contentParts.join('\\n\\n').replace(/\\s\\s+/g, ' ');
        return content.substring(0, 1000) + (content.length > 1000 ? '...' : '');
    }
    return 'No content found with the provided selectors.';

  } catch (error) {
    console.error(`[Content Scrape Test Error] for ${articleUrl}: ${error.message}`)
    return `Error scraping content: ${error.message}`
  }
}

export async function testHeadlineExtraction(sourceConfig, html) {
  let pageHtml = html
  if (!pageHtml) {
    pageHtml = await fetchPageWithPlaywright(sourceConfig.sectionUrl, 'TestHeadlineScraper')
  }
  const $ = cheerio.load(pageHtml)
  const articles = []
  const selectors = Array.isArray(sourceConfig.headlineSelector) ? sourceConfig.headlineSelector : [sourceConfig.headlineSelector].filter(Boolean);

  for (const selector of selectors) {
    switch (sourceConfig.extractionMethod) {
      case 'json-ld':
        $('script[type="application/ld+json"]').each((_, el) => {
          try {
            const jsonData = JSON.parse($(el).html())
            const potentialLists = [jsonData, ...(jsonData['@graph'] || [])]
            potentialLists.forEach((list) => {
              const items = list?.itemListElement
              if (Array.isArray(items)) {
                items.forEach((item) => {
                  const headline = item.name || item.item?.name
                  const url = item.url || item.item?.url
                  if (headline && url) {
                    articles.push({ headline: headline.trim(), link: new URL(url, sourceConfig.baseUrl).href })
                  }
                })
              }
            })
          } catch (e) {}
        })
        break
      case 'declarative':
        $(selector).each((_, el) => {
          const articleData = dynamicExtractor($, el, sourceConfig)
          if (articleData?.headline && articleData?.link) {
            articleData.link = new URL(articleData.link, sourceConfig.baseUrl).href
            articles.push(articleData)
          }
        })
        break
      case 'custom':
      default:
        const customExtractor = extractorRegistry[sourceConfig.extractorKey]
        if (!customExtractor) {
          throw new Error(`No custom extractor found for key: '${sourceConfig.extractorKey}'`)
        }
        $(selector).each((_, el) => {
          const articleData = customExtractor($(el), sourceConfig)
          if (articleData?.headline && articleData?.link) {
            articleData.link = new URL(articleData.link, sourceConfig.baseUrl).href
            articles.push(articleData)
          }
        })
        break
    }
  }
  return Array.from(new Map(articles.map((a) => [a.link, a])).values())
}

```

## üìÑ src/test-orchestrator.js
*Lines: 52, Size: 1.66 KB*

```javascript
// packages/scraper-logic/src/test-orchestrator.js
import { testHeadlineExtraction, scrapeArticleContentForTest } from './scraper/index.js'
import { Source } from '@headlines/models'
import { browserManager } from './browserManager.js' // Import the manager

export async function testScraperRecipe(sourceConfig, articleUrl = null) {
  await browserManager.initialize() // Ensure browser is running for tests
  try {
    // Mode 1: Test a single article's content
    if (articleUrl && sourceConfig.articleSelector) {
      const content = await scrapeArticleContentForTest(
        articleUrl,
        sourceConfig.articleSelector
      )
      return { success: true, content: { preview: content, sourceUrl: articleUrl } }
    }

    // Mode 2: Test the full source recipe for headlines
    if (sourceConfig && sourceConfig.sectionUrl) {
      const headlines = await testHeadlineExtraction(sourceConfig)

      const success = headlines.length > 0
      await Source.findByIdAndUpdate(
        sourceConfig._id,
        {
          $set: {
            lastScrapedAt: new Date(),
            lastSuccessAt: success ? new Date() : undefined,
          },
          $inc: {
            'analytics.totalRuns': 1,
            'analytics.totalSuccesses': success ? 1 : 0,
            'analytics.totalFailures': success ? 0 : 1,
          },
        },
        { new: true }
      ).lean()

      return {
        success: true,
        headlines: {
          count: headlines.length,
          samples: headlines.slice(0, 10),
        },
      }
    }
    throw new Error('Invalid request payload for testScraperRecipe.')
  } finally {
    await browserManager.close() // Close browser after test
  }
}

```
