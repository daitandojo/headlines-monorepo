# ðŸ“ PROJECT DIRECTORY STRUCTURE

Total: 29 files, 7 directories

```
headlines/
â”œâ”€â”€ ðŸ“ src/
â”‚   â”œâ”€â”€ ðŸ“ ai/
â”‚   â”‚   â””â”€â”€ ðŸ“„ index.js
â”‚   â”œâ”€â”€ ðŸ“ push/
â”‚   â”‚   â””â”€â”€ ðŸ“„ client.js
â”‚   â”œâ”€â”€ ðŸ“ scraper/
â”‚   â”‚   â”œâ”€â”€ ðŸ“ extractors/
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ reusable/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ simple.js
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“ source-specific/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ cvcPortfolio.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ cvcPortfolioContent.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ finansDk.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ jyllandsPosten.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ okonomiskUgebrev.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“„ politiken.js
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ verdane.js
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“„ index.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ constants.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ contentScraper.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ dynamicExtractor.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ headlineScraper.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ index.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ newsApiScraper.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ orchestrator.js
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ selectorOptimizer.js
â”‚   â”‚   â””â”€â”€ ðŸ“„ test-helpers.js
â”‚   â”œâ”€â”€ ðŸ“„ browser.js
â”‚   â”œâ”€â”€ ðŸ“„ browserManager.js
â”‚   â”œâ”€â”€ ðŸ“„ config.js
â”‚   â”œâ”€â”€ ðŸ“„ core.js
â”‚   â”œâ”€â”€ ðŸ“„ index.js
â”‚   â”œâ”€â”€ ðŸ“„ next.js
â”‚   â”œâ”€â”€ ðŸ“„ node.js
â”‚   â””â”€â”€ ðŸ“„ test-orchestrator.js
â””â”€â”€ ðŸ“„ package.json
```

# ðŸ“‹ PROJECT METADATA

**Generated**: 2025-10-05T00:50:44.076Z
**Repository Path**: /home/mark/Repos/projects/headlines/packages/scraper-logic
**Total Files**: 29
**Package**: @headlines/scraper-logic@2.0.0




---


## ðŸ“„ package.json
*Lines: 42, Size: 1.21 KB*

```json
{
  "name": "@headlines/scraper-logic",
  "version": "2.0.0",
  "main": "src/node.js",
  "type": "module",
  "license": "ISC",
  "exports": {
    ".": "./src/node.js",
    "./node": "./src/node.js",
    "./next": "./src/next.js",
    "./config.js": "./src/config.js",
    "./browser.js": "./src/browser.js",
    "./browserManager.js": "./src/browserManager.js",
    "./scraper/index.js": "./src/scraper/index.js",
    "./scraper/selectorOptimizer.js": "./src/scraper/selectorOptimizer.js",
    "./push/client.js": "./src/push/client.js",
    "./ai/index.js": "./src/ai/index.js",
    "./test-orchestrator": "./src/test-orchestrator.js"
  },
  "dependencies": {
    "@headlines/ai-services": "workspace:*",
    "@headlines/config": "workspace:*",
    "@headlines/models": "workspace:*",
    "@headlines/prompts": "workspace:*",
    "@headlines/utils-shared": "workspace:*",
    "@mozilla/readability": "^0.6.0",
    "axios": "^1.7.2",
    "cheerio": "^1.0.0-rc.12",
    "date-fns": "*",
    "jsdom": "^24.1.1",
    "lodash": "*",
    "newsapi": "^2.4.1",
    "p-limit": "^5.0.0",
    "pusher": "^5.2.0",
    "rss-parser": "^3.13.0",
    "web-push": "^3.6.7",
    "zod": "^3.23.8"
  },
  "peerDependencies": {
    "playwright": "^1.45.1"
  }
}
```

## ðŸ“„ src/ai/index.js
*Lines: 136, Size: 5.32 KB*

```javascript
// packages/scraper-logic/src/ai/index.js (version 6.1.0)
import { getConfig } from '../config.js'
import { callLanguageModel } from '../../../ai-services/src/index.js'
import { AIAgent } from './AIAgent.js'
import { assessArticleContent } from './agents/articleAgent.js'
import { articleAssessmentSchema } from './schemas/articleAssessmentSchema.js'
import { preAssessArticle } from './agents/articlePreAssessmentAgent.js'
import { articlePreAssessmentSchema } from './schemas/articlePreAssessmentSchema.js'
import { clusterArticlesIntoEvents } from './agents/clusteringAgent.js'
import { clusterSchema } from './schemas/clusterSchema.js'
import { resolveVagueContact, findContactDetails } from './agents/contactAgent.js'
import { enrichContactSchema } from './schemas/enrichContactSchema.js'
import { findContactSchema } from './schemas/findContactSchema.js'
import {
  generateEmailSubjectLine,
  generatePersonalizedIntro,
} from './agents/emailAgents.js'
import { emailSubjectSchema } from './schemas/emailSubjectSchema.js'
import { emailIntroSchema } from './schemas/emailIntroSchema.js'
import {
  extractEntities,
  entityCanonicalizerAgent as getEntityCanonicalizerAgent,
} from './agents/entityAgent.js'
import { entitySchema } from './schemas/entitySchema.js'
import { canonicalizerSchema } from './schemas/canonicalizerSchema.js'
import { assessHeadlinesInBatches } from './agents/headlineAgent.js'
import { headlineAssessmentSchema } from './schemas/headlineAssessmentSchema.js'
import { judgePipelineOutput } from './agents/judgeAgent.js'
import { judgeSchema } from './schemas/judgeSchema.js'
import { generateOpportunitiesFromEvent } from './agents/opportunityAgent.js'
import { opportunitySchema } from './schemas/opportunitySchema.js'
import { suggestNewSelector } from './agents/selectorRepairAgent.js'
import { selectorRepairSchema } from './schemas/selectorRepairSchema.js'
import { synthesizeEvent, synthesizeFromHeadline } from './agents/synthesisAgent.js'
import { synthesisSchema } from './schemas/synthesisSchema.js'
import { generateWatchlistSuggestions } from './agents/watchlistAgent.js'
import { watchlistSuggestionSchema } from './schemas/watchlistSuggestionSchema.js'
import { disambiguationSchema } from './schemas/disambiguationSchema.js'
import { batchAssessArticles } from './agents/batchArticleAgent.js'
import { batchArticleAssessmentSchema } from './schemas/batchArticleAssessmentSchema.js'
import { classifyLinks as sectionClassifierAgent } from './agents/sectionClassifierAgent.js'
import { generateExecutiveSummary } from './agents/executiveSummaryAgent.js'

let isApiKeyInvalid = false
export async function performAiSanityCheck() {
  try {
    getConfig().logger.info('ðŸ”¬ Performing AI service sanity check (OpenAI)...')
    const answer = await callLanguageModel({
      modelName: 'gpt-3.5-turbo', // Use a standard, widely available model for the check
      prompt: 'What is in one word the name of the capital of France',
      isJson: false,
    })
    if (
      answer &&
      typeof answer === 'string' &&
      answer.trim().toLowerCase().includes('paris')
    ) {
      getConfig().logger.info('âœ… AI service sanity check passed.')
      return true
    } else {
      getConfig().logger.fatal(
        { details: { expected: 'paris', received: answer } },
        `OpenAI sanity check failed.`
      )
      return false
    }
  } catch (error) {
    if (error.status === 401 || error.message?.includes('Incorrect API key')) {
      getConfig().logger.fatal(`OpenAI sanity check failed due to INVALID API KEY (401).`)
    } else {
      getConfig().logger.fatal(
        { err: error },
        'OpenAI sanity check failed with an unexpected API error.'
      )
    }
    isApiKeyInvalid = true
    return false
  }
}
export async function checkModelPermissions(requiredModels) {
  getConfig().logger.info('ðŸ”¬ Verifying permissions for configured OpenAI models...')
  try {
    // DEFINITIVE FIX: The OpenAI client for checking models is part of the ai-services package, not here.
    // This function is also not strictly necessary for the app to run, so we can simplify.
    // For now, we will assume permissions are correct if the sanity check passes.
    getConfig().logger.warn(
      'Model permission check is currently a no-op, relying on sanity check.'
    )
    return true
  } catch (error) {
    getConfig().logger.fatal({ err: error }, 'Failed to verify model permissions.')
    isApiKeyInvalid = true
    return false
  }
}
export {
  AIAgent,
  callLanguageModel,
  assessArticleContent,
  articleAssessmentSchema,
  preAssessArticle,
  articlePreAssessmentSchema,
  clusterArticlesIntoEvents,
  clusterSchema,
  resolveVagueContact,
  findContactDetails,
  enrichContactSchema,
  findContactSchema,
  generateEmailSubjectLine,
  generatePersonalizedIntro,
  emailSubjectSchema,
  emailIntroSchema,
  extractEntities,
  getEntityCanonicalizerAgent as entityCanonicalizerAgent,
  entitySchema,
  canonicalizerSchema,
  assessHeadlinesInBatches,
  headlineAssessmentSchema,
  judgePipelineOutput,
  judgeSchema,
  generateOpportunitiesFromEvent,
  opportunitySchema,
  suggestNewSelector,
  selectorRepairSchema,
  synthesizeEvent,
  synthesizeFromHeadline,
  synthesisSchema,
  generateWatchlistSuggestions,
  watchlistSuggestionSchema,
  disambiguationSchema,
  batchAssessArticles,
  batchArticleAssessmentSchema,
  sectionClassifierAgent,
  generateExecutiveSummary,
}

```

## ðŸ“„ src/browser.js
*Lines: 163, Size: 5.03 KB*

```javascript
// packages/scraper-logic/src/browser.js
import fs from 'fs/promises'
import path from 'path'
import { getConfig } from './config.js'
import { browserManager } from './browserManager.js'

const CONSENT_SELECTORS = [
  'button:has-text("Accepteer alles")',
  'button:has-text("Alles accepteren")',
  'button:has-text("Toestemming geven")',
  'button:has-text("Akkoord")',
  'button:has-text("Accept all")',
  'button:has-text("Accept All")',
  'button:has-text("I accept")',
  'button:has-text("Accept")',
  'button:has-text("Godkend alle")',
  'button:has-text("Tillad alle")',
]

async function ensureDebugDirectory() {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) {
    getConfig().logger.warn('Debug HTML directory not configured. Saving disabled.')
    return null
  }
  try {
    await fs.mkdir(debugDir, { recursive: true })
    return debugDir
  } catch (error) {
    getConfig().logger.warn('Failed to create debug directory: ' + error.message)
    return null
  }
}

async function saveDebugHtml(page, caller, prefix, url) {
  const debugDir = await ensureDebugDirectory()
  if (!debugDir) return null
  try {
    const html = await page.content()
    const urlPart = new URL(url).hostname.replace(/[^a-z0-9]/gi, '_')
    const filename = `${prefix}_${caller}_${urlPart}.html`
    const filePath = path.join(debugDir, filename)
    await fs.writeFile(filePath, html)
    getConfig().logger.warn(`[Playwright:${caller}] Saved debug HTML to ${filePath}`)
    return filePath
  } catch (error) {
    getConfig().logger.error(
      `[Playwright:${caller}] Failed to save debug HTML: ${error.message}`
    )
    return null
  }
}

async function handleConsent(page, caller) {
  for (const selector of CONSENT_SELECTORS) {
    try {
      const button = page.locator(selector).first()
      if (await button.isVisible({ timeout: 1500 })) {
        await button.click({ timeout: 2000 })
        getConfig().logger.info(
          `[Playwright:${caller}] Clicked consent button with selector: "${selector}"`
        )
        await page.waitForTimeout(1500)
        return true
      }
    } catch (e) {
      // Selector not found, continue
    }
  }
  getConfig().logger.trace(`[Playwright:${caller}] No actionable consent modal found.`)
  return false
}

export async function fetchPageWithPlaywright(url, caller = 'Unknown', options = {}) {
  const { timeout = 60000, waitForSelector } = options
  let page = null
  try {
    getConfig().logger.trace(
      `[Playwright:${caller}] Requesting new page for: ${url} (Timeout: ${timeout}ms)`
    )
    page = await browserManager.newPage()

    await page.goto(url, { waitUntil: 'domcontentloaded', timeout })
    await handleConsent(page, caller)

    if (waitForSelector) {
      getConfig().logger.info(
        `[Playwright:${caller}] Waiting for selector "${waitForSelector}"...`
      )
      await page.waitForSelector(waitForSelector, { timeout: timeout - 5000 })
      getConfig().logger.info(`[Playwright:${caller}] Selector found. Page is ready.`)
    } else {
      await page
        .waitForLoadState('networkidle', { timeout: 5000 })
        .catch(() =>
          getConfig().logger.trace(
            `[Playwright:${caller}] Network idle timeout reached, proceeding anyway.`
          )
        )
    }

    return await page.content()
  } catch (error) {
    let reason = error.message.split('\n')[0]
    if (error.message.includes('net::ERR')) {
      reason = `Network Error: ${reason}`
    } else if (error.name === 'TimeoutError') {
      reason = `Timeout after ${timeout / 1000}s. The page may be too slow or blocked.`
    } else if (page) {
      const pageContent = await page.content()
      if (pageContent.includes('captcha') || pageContent.includes('challenge-platform')) {
        reason = 'Potential CAPTCHA or bot detection wall encountered.'
      }
    }

    getConfig().logger.error(
      `[Playwright:${caller}] Critical failure during fetch for ${url}: ${reason}`
    )
    if (page) {
      await saveDebugHtml(page, caller, 'CRITICAL_FAIL', url)
    }
    return null
  } finally {
    if (page) {
      await page.close()
      getConfig().logger.trace(`[Playwright:${caller}] Page closed for: ${url}`)
    }
  }
}

export async function fetchPageContentFromPopup(pageUrl, buttonSelector) {
  let page = null
  try {
    page = await browserManager.newPage()
    await page.goto(pageUrl, { waitUntil: 'networkidle' })
    await handleConsent(page, 'PopupFetcher')

    await page.waitForSelector(buttonSelector, { timeout: 10000 })
    const button = page.locator(buttonSelector).first()
    await button.click()

    await page.waitForSelector('.popup-overlay--opened', {
      state: 'visible',
      timeout: 5000,
    })

    const popupElement = await page.locator('.popup__box')
    const popupHtml = await popupElement.innerHTML()
    return popupHtml
  } catch (error) {
    getConfig().logger.error(
      { err: error, url: pageUrl, selector: buttonSelector },
      'Failed to fetch content from popup.'
    )
    return null
  } finally {
    if (page) {
      await page.close()
    }
  }
}

```

## ðŸ“„ src/browserManager.js
*Lines: 62, Size: 1.87 KB*

```javascript
// packages/scraper-logic/src/browserManager.js
import playwright from 'playwright'
import { getConfig } from './config.js'

const BROWSER_HEADERS = {
  Accept:
    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9,nl-NL;q=0.8,nl;q=0.7',
  'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

class BrowserManager {
  constructor() {
    this.browser = null
    this.context = null
  }

  async initialize() {
    if (this.browser) {
      return
    }
    const { logger } = getConfig()
    logger.info('[BrowserManager] Initializing persistent browser instance...')
    try {
      this.browser = await playwright.chromium.launch({
        headless: true,
        args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'],
      })
      this.context = await this.browser.newContext({
        userAgent: BROWSER_HEADERS['User-Agent'],
        extraHTTPHeaders: BROWSER_HEADERS,
        viewport: { width: 1920, height: 1080 },
      })
      logger.info('[BrowserManager] âœ… Browser instance ready.')
    } catch (error) {
      logger.fatal({ err: error }, '[BrowserManager] CRITICAL: Failed to launch browser.')
      throw error
    }
  }

  async newPage() {
    if (!this.context) {
      throw new Error('BrowserManager not initialized. Call initialize() first.')
    }
    return this.context.newPage()
  }

  async close() {
    if (this.browser) {
      await this.browser.close()
      this.browser = null
      this.context = null
      getConfig().logger.info('[BrowserManager] Persistent browser instance closed.')
    }
  }
}

// Export a singleton instance of the manager
export const browserManager = new BrowserManager()

```

## ðŸ“„ src/config.js
*Lines: 23, Size: 567 Bytes*

```javascript
// packages/scraper-logic/src/config.js (version 1.0.0)
// This module holds the shared configuration for the scraper logic,
// which will be injected by the consuming application (pipeline or admin).

let _config = {
  // A simple console logger as a fallback.
  logger: console,
  paths: {
    debugHtmlDir: null,
  },
  configStore: null,
  utilityFunctions: null,
};

export function configure(appConfig) {
  // Merge the provided app config with the existing config.
  _config = { ..._config, ...appConfig };
}

export function getConfig() {
  return _config;
}

```

## ðŸ“„ src/core.js
*Lines: 11, Size: 514 Bytes*

```javascript
// This file serves as the core, shared module for the scraper-logic package.
// It is environment-agnostic. Environment-specific entry points (next.js, node.js)
// will re-export from this file and add any necessary guards.

// Currently, all exports are handled via subpaths (e.g., /browser, /scraper/index.js).
// If you add a function that should be available from the root of the package,
// export it from here.

// Example of a future export:
// export { someSharedScraperUtil } from './some-util-file.js';

```

## ðŸ“„ src/index.js
*Lines: 3, Size: 76 Bytes*

```javascript
// This is the default, Node.js-safe entry point.
export * from './core.js'

```

## ðŸ“„ src/next.js
*Lines: 10, Size: 349 Bytes*

```javascript
    
// packages/scraper-logic/src/next.js (version 2.0.0)
// ARCHITECTURAL REFACTORING: This file is now a "dead-end" shim.
// It is guarded and exports nothing, preventing the Next.js/Vercel build
// from ever bundling heavy, incompatible Node.js dependencies like Playwright.
import 'server-only';

// This file intentionally exports nothing.

  
```

## ðŸ“„ src/node.js
*Lines: 12, Size: 403 Bytes*

```javascript
// packages/scraper-logic/src/node.js
// This file serves as the explicit, Node.js-only entry point.

export * from './core.js'
export * from './browser.js'
export * from './browserManager.js' // Export the new manager
export * from './scraper/index.js'
export * from './scraper/selectorOptimizer.js'
export * from './push/client.js'
export * from './ai/index.js'
export * from './test-orchestrator.js'

```

## ðŸ“„ src/push/client.js
*Lines: 26, Size: 940 Bytes*

```javascript
// packages/scraper-logic/src/push/client.js (version 2.0.0)
import webpush from 'web-push'
import { getConfig } from '../config.js';
import { env } from '../../../config/src/index.js'

let isPushConfigured = false

function configurePush() {
    if (isPushConfigured) return;

    const { VAPID_SUBJECT, VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY } = env
    if (VAPID_PUBLIC_KEY && VAPID_PRIVATE_KEY && VAPID_SUBJECT) {
      try {
        webpush.setVapidDetails(VAPID_SUBJECT, VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY)
        isPushConfigured = true
        getConfig().logger.info('âœ… Centralized push notification service (VAPID) configured.')
      } catch (error) {
        getConfig().logger.error({ err: error }, 'âŒ Failed to configure VAPID details.')
      }
    } else {
      getConfig().logger.warn('VAPID keys not fully configured. Push notifications will be disabled.')
    }
}

export { webpush, isPushConfigured, configurePush }

```

## ðŸ“„ src/scraper/constants.js
*Lines: 10, Size: 454 Bytes*

```javascript
// src/modules/scraper/constants.js (version 1.0)
export const BROWSER_HEADERS = {
  Accept:
    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9',
  'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
}

```

## ðŸ“„ src/scraper/contentScraper.js
*Lines: 123, Size: 4.1 KB*

```javascript
// packages/scraper-logic/src/scraper/contentScraper.js (version 5.0.0 - Unified Selector Strategy)
import * as cheerio from 'cheerio'
import fs from 'fs/promises'
import path from 'path'
import { JSDOM, VirtualConsole } from 'jsdom'
import { Readability } from '@mozilla/readability'

import { getConfig } from '../config.js'
import { fetchPageWithPlaywright } from '../browser.js'
import { contentExtractorRegistry } from './extractors/index.js'
import { settings } from '@headlines/config/node'

async function saveDebugHtml(filename, html) {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) return null
  try {
    await fs.mkdir(debugDir, { recursive: true })
    const filePath = path.join(debugDir, filename)
    await fs.writeFile(filePath, html)
    return filePath
  } catch (error) {
    getConfig().logger.error({ err: error, file: filename }, 'Failed to save debug HTML.')
    return null
  }
}

function extractWithReadability(url, html) {
  try {
    const virtualConsole = new VirtualConsole()
    virtualConsole.on('cssParseError', () => {})
    const doc = new JSDOM(html, {
      url,
      virtualConsole,
      resources: 'usable',
    })
    const reader = new Readability(doc.window.document)
    const article = reader.parse()
    return article?.textContent || null
  } catch (e) {
    getConfig().logger.warn({ err: e, url }, 'Readability.js failed to parse article.')
    return null
  }
}

export async function scrapeArticleContent(article, source) {
  if (
    source.extractionMethod === 'custom' &&
    contentExtractorRegistry[source.extractorKey]
  ) {
    return await contentExtractorRegistry[source.extractorKey](article, source)
  }

  if (article.rssContent && article.rssContent.length >= settings.MIN_ARTICLE_CHARS) {
    article.articleContent = { contents: [article.rssContent] }
    return article
  }

  const html = await fetchPageWithPlaywright(article.link, 'ContentScraper')
  if (!html) {
    return { ...article, enrichment_error: 'Playwright failed to fetch page HTML' }
  }

  const $ = cheerio.load(html)
  let contentParts = []
  let extractionMethod = 'None'
  let contentText = ''

  // DEFINITIVE FIX: Unified Selector Strategy
  // If articleSelector is defined, use it as the primary, authoritative method.
  const selectors = Array.isArray(source.articleSelector)
    ? source.articleSelector
    : [source.articleSelector].filter(Boolean)

  if (selectors.length > 0) {
    extractionMethod = 'Cheerio Multi-Selector'
    for (const selector of selectors) {
      // Grab text from each matching element and push to the array
      $(selector).each((_, el) => {
        const text = $(el).text().replace(/\s+/g, ' ').trim()
        if (text) {
          contentParts.push(text)
        }
      })
    }
    // Join the unique parts together. Using a Set handles cases where selectors might overlap.
    contentText = [...new Set(contentParts)].join('\n\n')
  }

  // Fallback to Readability.js ONLY if the selector-based method fails or is not configured.
  if (!contentText || contentText.length < settings.MIN_ARTICLE_CHARS) {
    const readabilityText = extractWithReadability(article.link, html)
    if (readabilityText && readabilityText.length > contentText.length) {
      contentText = readabilityText
      extractionMethod = 'Readability.js Fallback'
    }
  }

  if (contentText && contentText.length >= settings.MIN_ARTICLE_CHARS) {
    article.articleContent = { contents: [contentText] }
    getConfig().logger.trace(
      {
        article: {
          headline: article.headline,
          chars: contentText.length,
          method: extractionMethod,
        },
      },
      `âœ… Content enrichment successful.`
    )
  } else {
    const reason = !contentText
      ? `All extraction methods failed.`
      : `Content too short (` + (contentText ? contentText.length : 0) + ` chars).`
    article.enrichment_error = reason
    article.contentPreview = contentText ? contentText.substring(0, 100) : ''
    const filename =
      source.name.replace(/[^a-z0-9]/gi, '_').toLowerCase() + '_content_fail.html'
    await saveDebugHtml(filename, html)
  }
  return article
}

```

## ðŸ“„ src/scraper/dynamicExtractor.js
*Lines: 51, Size: 1.83 KB*

```javascript
// packages/scraper-logic/src/scraper/dynamicExtractor.js (version 2.0.1)
import { getConfig } from '../config.js';

/**
 * A generic, data-driven extractor that uses declarative fields from a Source document
 * to extract headline and link information from a Cheerio element.
 * @param {import('cheerio').CheerioAPI} $ - The Cheerio instance.
 * @param {import('cheerio').Element} el - The current DOM element matching the headlineSelector.
 * @param {object} source - The full Source document from the database.
 * @returns {{headline: string, link: string}|null} The extracted article data or null if invalid.
 */
export function dynamicExtractor($, el, source) {
  try {
    const mainElement = $(el)

    // 1. Find the link element and extract the href.
    // If linkSelector is null, the mainElement itself is the link.
    const linkElement = source.linkSelector
      ? mainElement.find(source.linkSelector).first()
      : mainElement
    const link = linkElement.attr('href')

    if (!link) {
      return null // A link is mandatory
    }

    // 2. Find the headline text element and extract the text.
    // If headlineTextSelector is null, the mainElement contains the text.
    const textElement = source.headlineTextSelector
      ? mainElement.find(source.headlineTextSelector).first()
      : mainElement

    // 3. Extract the text and clean it by removing any nested HTML tags.
    let headline = textElement.text().trim().replace(/\s+/g, ' ')

    if (!headline) {
      return null // A headline is mandatory
    }

    // 4. Apply the headline template if it exists
    if (source.headlineTemplate) {
      headline = source.headlineTemplate.replace('{{TEXT}}', headline)
    }

    return { headline, link }
  } catch (error) {
    getConfig().logger.error({ err: error, source: source.name }, 'Error during dynamic extraction.')
    return null
  }
}

```

## ðŸ“„ src/scraper/extractors/index.js
*Lines: 45, Size: 1.66 KB*

```javascript
// packages/scraper-logic/src/scraper/extractors/index.js (version 2.0.0)
// This file uses static imports to be compatible with both Node.js (pipeline) and Webpack (Next.js apps).

// Reusable Extractors
import { simpleExtractor } from './reusable/simple.js';

// Source-Specific Headline Extractors
import { cvcPortfolioExtractor } from './source-specific/cvcPortfolio.js';
import { finansDkExtractor } from './source-specific/finansDk.js';
import { jyllandsPostenExtractor } from './source-specific/jyllandsPosten.js';
import { okonomiskUgebrevExtractor } from './source-specific/okonomiskUgebrev.js';
import { politikenExtractor } from './source-specific/politiken.js';
import { verdaneExtractor } from './source-specific/verdane.js';

// Source-Specific Content Extractors
import { cvcPortfolioContentExtractor } from './source-specific/cvcPortfolioContent.js';

// --- Build Registries ---

export const extractorRegistry = {
  // Reusable
  simple: simpleExtractor,

  // Source-specific
  cvc_portfolio: cvcPortfolioExtractor,
  finans_dk: finansDkExtractor,
  jyllands_posten: jyllandsPostenExtractor,
  okonomisk_ugebrev: okonomiskUgebrevExtractor,
  politiken: politikenExtractor,
  verdane: verdaneExtractor,

  // Manual mapping for legacy keys
  gro_capital: simpleExtractor,
  eifo_dk: simpleExtractor,
  clearwater_dk: simpleExtractor,
  e24: simpleExtractor,
  quotenet_nl: simpleExtractor,
};

export const contentExtractorRegistry = {
  cvc_portfolio_content: cvcPortfolioContentExtractor,
};

console.log(`[Extractor Registry] Statically loaded ${Object.keys(extractorRegistry).length} headline extractors and ${Object.keys(contentExtractorRegistry).length} content extractors.`);

```

## ðŸ“„ src/scraper/extractors/reusable/simple.js
*Lines: 11, Size: 374 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/reusable/simple.js (version 1.0.0)
export const simpleExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.text().trim().replace(/\s+/g, ' ')
  const link = element.attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

```

## ðŸ“„ src/scraper/extractors/source-specific/cvcPortfolio.js
*Lines: 22, Size: 734 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/cvcPortfolio.js (version 1.0.0)
export const cvcPortfolioExtractor = ($, el, site) => {
  const element = $(el);
  if (element.hasClass('portfolio__card-holder--spotlight')) {
    return null;
  }
  const headingElement = element.find('h2.portfolio__card-heading');
  const companyName = headingElement.text().trim();
  const button = element.find('button.js-portfolio-card');
  
  if (companyName && button.length) {
    return { 
        headline: 'CVC Portfolio Company: ' + companyName, 
        link: site.sectionUrl, 
        source: site.name, 
        newspaper: site.name,
        customData: { dataKey: button.attr('data-key') } 
    };
  }
  return null;
};

```

## ðŸ“„ src/scraper/extractors/source-specific/cvcPortfolioContent.js
*Lines: 30, Size: 1.17 KB*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/cvcPortfolioContent.js (version 1.0.0)
import { fetchPageContentFromPopup } from '../../../browser.js';
import { getConfig } from '../../../config.js';
import * as cheerio from 'cheerio';

export const cvcPortfolioContentExtractor = async (article, source) => {
    if (!article.customData?.dataKey) {
        return { ...article, enrichment_error: 'Missing data-key for popup interaction.' };
    }

    const buttonSelector = 'button[data-key="' + article.customData.dataKey + '"]';
    const popupHtml = await fetchPageContentFromPopup(source.sectionUrl, buttonSelector);

    if (!popupHtml) {
        return { ...article, enrichment_error: 'Failed to fetch popup HTML for content.' };
    }

    const $ = cheerio.load(popupHtml);
    const content = $('.rte').text().trim().replace(/\s+/g, ' ');

    if (content) {
        article.articleContent = { contents: [content] };
        getConfig().logger.trace({ article: { headline: article.headline } }, 'âœ… CVC custom content extraction successful.');
    } else {
        article.enrichment_error = 'Could not find content in the CVC popup.';
    }

    return article;
}

```

## ðŸ“„ src/scraper/extractors/source-specific/finansDk.js
*Lines: 8, Size: 268 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/finansDk.js (version 1.0.0)
export const finansDkExtractor = ($, el, site) => ({
  headline: $(el).text().trim(),
  link: $(el).closest('a').attr('href'),
  source: site.name,
  newspaper: site.name,
})

```

## ðŸ“„ src/scraper/extractors/source-specific/jyllandsPosten.js
*Lines: 11, Size: 397 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/jyllandsPosten.js (version 1.0.0)
export const jyllandsPostenExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h3').text().trim()
  const link = element.find('a').attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

```

## ðŸ“„ src/scraper/extractors/source-specific/okonomiskUgebrev.js
*Lines: 12, Size: 444 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/okonomiskUgebrev.js (version 1.0.0)
export const okonomiskUgebrevExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h5.elementor-heading-title').text().trim().replace(/\s+/g, ' ');
  const link = element.attr('href');
  
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name };
  }
  return null;
};

```

## ðŸ“„ src/scraper/extractors/source-specific/politiken.js
*Lines: 8, Size: 391 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/politiken.js (version 1.0.0)
export const politikenExtractor = ($, el, site) => {
  const element = $(el);
  const h = element.find('h2, h3, h4').first().text().trim()
  const a = element.find('a[href*="/art"]').first().attr('href')
  return h && a ? { headline: h, link: a, source: site.name, newspaper: site.name } : null
}

```

## ðŸ“„ src/scraper/extractors/source-specific/verdane.js
*Lines: 16, Size: 516 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/verdane.js (version 1.0.0)
export const verdaneExtractor = ($, el, site) => {
  const element = $(el);
  const linkEl = element.find('a.wp-block-klingit-the-product-block-link')
  const companyName = linkEl.find('h3.wp-block-post-title').text().trim()
  if (companyName) {
    return {
      headline: 'Verdane invests in ' + companyName,
      link: linkEl.attr('href'),
      source: site.name,
      newspaper: site.name,
    }
  }
  return null
}

```

## ðŸ“„ src/scraper/headlineScraper.js
*Lines: 158, Size: 6.15 KB*

```javascript
// packages/scraper-logic/src/scraper/headlineScraper.js (version 5.2.0)
import * as cheerio from 'cheerio'
import axios from 'axios'
import fs from 'fs/promises'
import path from 'path'
import Parser from 'rss-parser'
import { Source } from '../../../models/src/index.js'
import { BROWSER_HEADERS } from './constants.js'
import { fetchPageWithPlaywright } from '../browser.js'
import { getConfig } from '../config.js'
import { extractorRegistry } from './extractors/index.js'
import { dynamicExtractor } from './dynamicExtractor.js'

const rssParser = new Parser({
  customFields: {
    item: [['content:encoded', 'contentEncoded']],
  },
})

async function saveDebugHtml(filename, html) {
  const config = getConfig();
  const DEBUG_HTML_DIR = config.paths?.debugHtmlDir;
  if (!DEBUG_HTML_DIR) return;
  try {
    await fs.mkdir(DEBUG_HTML_DIR, { recursive: true })
    const filePath = path.join(DEBUG_HTML_DIR, filename)
    await fs.writeFile(filePath, html)
  } catch (error) {
    getConfig().logger.error({ err: error, file: filename }, 'Failed to save debug HTML.')
  }
}

async function fetchHeadlinesViaRss(source) {
  try {
    const feed = await rssParser.parseURL(source.rssUrl);
    if (!feed.items || feed.items.length === 0) {
      throw new Error('RSS feed was empty or invalid.');
    }
    const articles = feed.items
      .map((item) => {
        const rssContentHtml = item.contentEncoded || item.contentSnippet || item.content || '';
        const rssContent = cheerio.load(rssContentHtml).text().replace(/\s+/g, ' ').trim();
        return { headline: item.title?.trim(), link: item.link, rssContent: rssContent || null };
      })
      .filter((item) => item.headline && item.link);
    return { articles, error: null };
  } catch (error) {
    const failureReason = error.message || 'Unknown RSS error.';
    getConfig().logger.warn({ url: source.rssUrl, reason: failureReason }, `[RSS] Feed parsing failed for "${source.name}". Auto-disabling.`);
    
    try {
      await Source.updateOne(
        { _id: source._id },
        { 
          $set: { 
            rssUrl: null,
            notes: `${source.notes || ''}\n[${new Date().toISOString()}] RSS URL disabled due to error: ${failureReason}`.trim(),
          } 
        }
      );
    } catch (dbError) {
      getConfig().logger.error({ err: dbError }, 'Failed to auto-disable RSS URL in database.');
    }
    return { articles: [], error: failureReason };
  }
}

async function fetchPageStatic(url) {
  try {
    const { data } = await axios.get(url, { headers: BROWSER_HEADERS, timeout: 25000 })
    return { html: data, error: null }
  } catch (error) {
    getConfig().logger.warn({ url, err: { message: error.message } }, `[Axios] Static fetch failed`)
    return { html: null, error: error.message }
  }
}

async function fetchWithPlaywrightWrapped(source) {
  const html = await fetchPageWithPlaywright(source.sectionUrl, 'HeadlineScraper', {
    timeout: source.playwrightTimeoutMs,
    waitForSelector: source.waitForSelector,
  })
  if (html) {
    return { html, error: null }
  }
  return {
    html: null,
    error: `Playwright failed to fetch content from ${source.sectionUrl}`,
  }
}

export async function scrapeSiteForHeadlines(source) {
  if (source.rssUrl) {
    getConfig().logger.info(`[Scraping] Attempting RSS scrape for "${source.name}"...`);
    const rssResult = await fetchHeadlinesViaRss(source);
    if (rssResult.articles.length > 0) {
      return { articles: rssResult.articles, success: true, resultCount: rssResult.articles.length, error: null };
    }
    getConfig().logger.warn(`RSS scrape failed for "${source.name}". Falling back to HTML scraping.`);
  }

  const fetcher = source.isStatic ? () => fetchPageStatic(source.sectionUrl) : () => fetchWithPlaywrightWrapped(source);
  const fetcherName = source.isStatic ? 'STATIC (fast)' : 'PLAYWRIGHT (full-browser)';
  getConfig().logger.info(`[Scraping] Initiating HTML scrape for "${source.name}" using ${fetcherName}...`);

  const { html, error } = await fetcher();
  if (!html) return { articles: [], success: false, error, debugHtml: null };

  const $ = cheerio.load(html);
  const articles = [];
  
  if (source.extractionMethod === 'json-ld') {
      $('script[type="application/ld+json"]').each((_, el) => {
        try {
          const jsonData = JSON.parse($(el).html())
          const potentialLists = [jsonData, ...(jsonData['@graph'] || [])]
          potentialLists.forEach((list) => {
            const items = list?.itemListElement
            if (items && Array.isArray(items)) {
              items.forEach((item) => {
                const headline = item.name || item.item?.name
                const url = item.url || item.item?.url
                if (headline && url) {
                  articles.push({ headline: headline.trim(), link: new URL(url, source.baseUrl).href });
                }
              })
            }
          })
        } catch (e) { /* Ignore parsing errors */ }
      })
  } else { // 'declarative' or 'custom'
      const selectors = Array.isArray(source.headlineSelector) ? source.headlineSelector : [source.headlineSelector].filter(Boolean);
      const extractorFn = source.extractionMethod === 'custom' ? extractorRegistry[source.extractorKey] : dynamicExtractor;
      if (!extractorFn) {
          return { articles: [], success: false, error: `No valid extractor for method: ${source.extractionMethod}` };
      }
      for (const selector of selectors) {
        $(selector).each((_, el) => {
            const articleData = extractorFn($, el, source);
            if (articleData?.headline && articleData?.link) {
                articleData.link = new URL(articleData.link, source.baseUrl).href;
                articles.push(articleData);
            }
        });
      }
  }

  const uniqueArticles = Array.from(new Map(articles.map((a) => [a.link, a])).values());

  if (uniqueArticles.length === 0) {
    const filename = `${source.name.replace(/[^a-z0-9]/gi, '_').toLowerCase()}_headline_fail.html`;
    await saveDebugHtml(filename, html);
    return { articles: [], success: false, error: 'Extracted 0 headlines.', debugHtml: html };
  }

  return { articles: uniqueArticles, success: true, resultCount: uniqueArticles.length, error: null };
}

```

## ðŸ“„ src/scraper/index.js
*Lines: 12, Size: 395 Bytes*

```javascript
// packages/scraper-logic/src/scraper/index.js (Corrected Exports)
import { scrapeSiteForHeadlines } from './headlineScraper.js'
import { scrapeArticleContent } from './contentScraper.js'
import { testHeadlineExtraction, scrapeArticleContentForTest } from './test-helpers.js'

export {
  scrapeSiteForHeadlines,
  scrapeArticleContent,
  testHeadlineExtraction,
  scrapeArticleContentForTest,
}

```

## ðŸ“„ src/scraper/newsApiScraper.js
*Lines: 133, Size: 4.06 KB*

```javascript
// packages/scraper-logic/src/scraper/newsApiScraper.js (version 2.3.0)
import NewsAPI from 'newsapi'
import { getConfig } from '../config.js';
import { Source, WatchlistEntity } from '../../../models/src/index.js'
import { env } from '../../../config/src/index.js'
import colors from 'ansi-colors';

async function getWatchlist() {
  const [sources, richListTargets] = await Promise.all([
    Source.find({
      status: 'active',
      country: { $in: ['Denmark', 'Global PE', 'M&A Aggregators'] },
    })
      .select('name')
      .lean(),
    WatchlistEntity.find({ status: 'active' }).select('name').lean(),
  ])

  const sourceNames = sources.map((s) => s.name.split('(')[0].trim())
  const richListNames = richListTargets.map((t) => t.name.split('(')[0].trim())
  const watchlist = [...new Set([...sourceNames, ...richListNames])]
  getConfig().logger.trace({ details: watchlist }, 'Full NewsAPI watchlist keywords.')
  return watchlist
}

function buildQueryBatches(watchlist) {
  const MAX_QUERY_LENGTH = 490
  const queries = []
  let currentBatch = []

  for (const keyword of watchlist) {
    const sanitizedKeyword = keyword.replace(/&/g, ' ').replace(/[()]/g, '').trim()
    if (!sanitizedKeyword) continue
    const quotedKeyword = `"${sanitizedKeyword}"`
    const potentialQuery = [...currentBatch, quotedKeyword].join(' OR ')
    if (potentialQuery.length > MAX_QUERY_LENGTH) {
      if (currentBatch.length > 0) {
        queries.push(currentBatch.join(' OR '))
      }
      currentBatch = [quotedKeyword]
    } else {
      currentBatch.push(quotedKeyword)
    }
  }

  if (currentBatch.length > 0) {
    queries.push(currentBatch.join(' OR '))
  }

  const queriesToUse = queries.slice(0, 4);

  if (queries.length > 4) {
    getConfig().logger.warn(
      `[NewsAPI] Watchlist generated ${queries.length} queries, but will only use the first 4 to avoid rate limits.`
    );
    let logMessage = '[NewsAPI] Generated Queries:\n';
    queries.forEach((q, i) => {
        const inUse = i < 4;
        logMessage += inUse ? colors.green(`  [IN USE] Query ${i+1}: ${q}\n`) : colors.gray(`  [SKIPPED] Query ${i+1}: ${q}\n`);
    });
    getConfig().logger.info(logMessage);
  }

  return queriesToUse;
}

export async function scrapeNewsAPI() {
  const newsapi = new NewsAPI(env.NEWSAPI_API_KEY)
  try {
    const watchlist = await getWatchlist()
    const queryBatches = buildQueryBatches(watchlist)

    getConfig().logger.info(
      `ðŸ“° [NewsAPI] Dispatching ${queryBatches.length} batched queries to cover the watchlist.`
    )

    const twentyFourHoursAgo = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString()

    const allPromises = queryBatches.map((query) =>
      newsapi.v2.everything({
        q: query,
        language: 'en,da,sv,no',
        sortBy: 'publishedAt',
        from: twentyFourHoursAgo,
        pageSize: 100,
      })
    )

    const allResponses = await Promise.all(allPromises)
    let allArticles = []

    for (const response of allResponses) {
      if (response.status !== 'ok') {
        getConfig().logger.error(
          `[NewsAPI] Error in a batch query: ${response.code} - ${response.message}`
        )
        continue
      }
      allArticles.push(...response.articles)
    }

    if (allArticles.length === 0) {
      getConfig().logger.info('[NewsAPI] Found no new articles matching the watchlist.')
      return []
    }

    getConfig().logger.info(
      `[NewsAPI] Found a total of ${allArticles.length} potential articles across all batches.`
    )

    const articles = allArticles.map((a) => ({
      headline: a.title,
      link: a.url,
      source: a.source.name,
      newspaper: a.source.name,
    }))

    return Array.from(new Map(articles.map((a) => [a.link, a])).values())
  } catch (error) {
    if (error.name?.includes('rateLimited')) {
      getConfig().logger.warn(
        '[NewsAPI] Rate limit hit, as expected on developer plan. Some watchlist items may have been missed.'
      )
    } else {
      getConfig().logger.error(
        { err: error },
        '[NewsAPI] A critical error occurred during batched scraping.'
      )
    }
    return []
  }
}

```

## ðŸ“„ src/scraper/orchestrator.js
*Lines: 105, Size: 3.46 KB*

```javascript
// packages/scraper-logic/src/scraper/orchestrator.js (version 5.0.0)
import pLimit from 'p-limit'
import { sleep } from '@shared/utils-shared'
import { getConfig } from '../config.js'
import { scrapeSiteForHeadlines } from './headlineScraper.js'
// NEWSAPI REWORK: The direct import of scrapeNewsAPI is removed as it's no longer used for proactive scraping.
// import { scrapeNewsAPI } from './newsApiScraper.js'
import { updateSourceAnalyticsBatch } from '../../../data-access/src/index.js'
import { env } from '../../../config/src/index.js'

async function performStandardScraping(sourcesToScrape) {
  if (sourcesToScrape.length === 0) {
    return { scrapedArticles: [], scraperHealth: [] }
  }

  const limit = pLimit(env.CONCURRENCY_LIMIT || 3)
  getConfig().logger.info(
    `Pipeline will now scrape ${sourcesToScrape.length} active standard sources.`
  )

  let allArticles = []
  const scraperHealthMap = new Map()

  const promises = sourcesToScrape.map((source) =>
    limit(async () => {
      getConfig().logger.info(`[Scraping] -> Starting scrape for "${source.name}"...`)
      const result = await scrapeSiteForHeadlines(source)
      const foundCount = result.resultCount !== undefined ? result.resultCount : 0
      getConfig().logger.info(
        `[Scraping] <- Finished scrape for "${source.name}". Success: ${result.success}, Found: ${foundCount}`
      )
      return { source, result }
    })
  )
  const results = await Promise.all(promises)

  const bulkUpdateOps = []

  for (const { source, result } of results) {
    const healthReport = {
      source: source.name,
      success: result.success && result.resultCount > 0,
      count: result.resultCount || 0,
      error: result.error,
      debugHtml: result.debugHtml,
      failedSelector: result.success ? null : source.headlineSelector,
    }
    scraperHealthMap.set(source.name, healthReport)

    if (healthReport.success) {
      allArticles.push(
        ...result.articles.map((a) => ({
          ...a,
          source: source.name,
          newspaper: source.name,
          country: source.country,
        }))
      )
      bulkUpdateOps.push({
        updateOne: {
          filter: { _id: source._id },
          update: { $set: { lastScrapedAt: new Date(), lastSuccessAt: new Date() } },
        },
      })
    } else {
      getConfig().logger.warn(
        `[Scraping] âŒ FAILED for "${source.name}": ${result.error || 'Extracted 0 headlines.'}.`
      )
      bulkUpdateOps.push({
        updateOne: {
          filter: { _id: source._id },
          update: { $set: { lastScrapedAt: new Date() } },
        },
      })
    }
  }

  if (bulkUpdateOps.length > 0) {
    await updateSourceAnalyticsBatch(bulkUpdateOps)
  }

  return {
    scrapedArticles: allArticles,
    scraperHealth: Array.from(scraperHealthMap.values()),
  }
}

// NEWSAPI REWORK: The main orchestrator is simplified. It no longer calls scrapeNewsAPI.
// Its sole responsibility is now to manage the standard scraping process. This aligns
// with the new strategy of using external APIs only for enrichment, not discovery.
export async function scrapeAllHeadlines(sourcesToScrape) {
  const { scrapedArticles, scraperHealth } =
    await performStandardScraping(sourcesToScrape)

  const uniqueArticles = Array.from(
    new Map(scrapedArticles.map((a) => [a.link, a])).values()
  )

  getConfig().logger.info(
    `Scraping complete. Found ${uniqueArticles.length} unique articles from standard sources.`
  )

  return { allArticles: uniqueArticles, scraperHealth }
}

```

## ðŸ“„ src/scraper/selectorOptimizer.js
*Lines: 98, Size: 3.19 KB*

```javascript
// packages/scraper-logic/src/scraper/selectorOptimizer.js (version 4.2)
import * as cheerio from 'cheerio';

const NEGATIVE_TAGS = ['nav', 'footer', 'aside', 'header', 'form', '.popup-overlay'];

/**
 * Finds clusters of repeated elements by analyzing class name frequency.
 * CRITICALLY, it filters out Tailwind-style classes with colons.
 */
function findRepeatingClassSelectors($) {
    const classCounts = {};
    $('*').each((_, el) => {
        const classes = $(el).attr('class');
        if (classes) {
            classes.trim().split(/\s+/).forEach(cls => {
                // DEFINITIVE FIX: Ignore any class containing a colon to prevent pseudo-class errors.
                if (cls.length > 5 && !cls.includes(':') && !cls.startsWith('js-')) {
                    classCounts[cls] = (classCounts[cls] || 0) + 1;
                }
            });
        }
    });

    return Object.entries(classCounts)
        .filter(([_, count]) => count > 3 && count < 100)
        .sort((a, b) => b[1] - a[1])
        .slice(0, 15) // Widen the search slightly
        .map(([cls]) => `.${cls}`);
}

/**
 * For a given container element, finds the most likely headline text.
 */
function analyzeContainer($container) {
    const headlineEl = $container.find('h1, h2, h3, h4, h5').first();
    let text = headlineEl.text().trim().replace(/\s+/g, ' ');

    if (!text) {
        // Fallback for non-heading elements
        text = $container.text().trim().replace(/\s+/g, ' ');
    }

    // Ensure it's a clickable container
    const isClickable = $container.is('a[href]') || $container.find('a[href]').length > 0 || $container.find('button[data-key]').length > 0;

    if (text && isClickable) {
        return { text };
    }
    return null;
}

export function heuristicallyFindSelectors(html) {
    const $ = cheerio.load(html);
    $(NEGATIVE_TAGS.join(',')).remove();

    const potentialListSelectors = findRepeatingClassSelectors($);
    const clusters = [];

    // Add the CVC-specific selector as a high-priority candidate, as it is a known good pattern.
    potentialListSelectors.unshift('.portfolio__card-holder');

    for (const selector of potentialListSelectors) {
        try {
            const elements = $(selector);
            if (elements.length < 3) continue;

            const samples = [];
            let validItems = 0;

            elements.each((_, el) => {
                const containerData = analyzeContainer($(el));
                if (containerData) {
                    samples.push(containerData.text);
                    validItems++;
                }
            });

            if (validItems > 2 && (validItems / elements.length) > 0.5) {
                clusters.push({
                    selector: selector,
                    score: validItems * (validItems / elements.length),
                    samples: samples,
                });
            }
        } catch (e) {
            // Silently ignore errors from invalid selectors that might still slip through
        }
    }

    if (clusters.length === 0) {
        return [];
    }
    
    const uniqueClusters = [...new Map(clusters.map(item => [item.selector, item])).values()];

    return uniqueClusters.sort((a, b) => b.score - a.score).slice(0, 5);
}

```

## ðŸ“„ src/scraper/test-helpers.js
*Lines: 93, Size: 3.56 KB*

```javascript
// packages/scraper-logic/src/scraper/test-helpers.js (version 2.0)
import * as cheerio from 'cheerio'
import { dynamicExtractor } from './dynamicExtractor.js'
import { extractorRegistry } from './extractors/index.js'
import { fetchPageWithPlaywright } from '../browser.js'

export async function scrapeArticleContentForTest(articleUrl, articleSelectors) {
  if (!articleUrl || !articleSelectors || articleSelectors.length === 0) return ''
  try {
    const html = await fetchPageWithPlaywright(articleUrl, 'TestContentScraper')
    if (!html) return 'Error: Failed to fetch page HTML.';
    
    const $ = cheerio.load(html)
    const selectors = Array.isArray(articleSelectors) ? articleSelectors : [articleSelectors];
    let contentParts = [];
    
    for (const selector of selectors) {
        $(selector).each((_, el) => {
            contentParts.push($(el).text().trim());
        });
    }

    if(contentParts.length > 0) {
        const content = contentParts.join('\\n\\n').replace(/\\s\\s+/g, ' ');
        return content.substring(0, 1000) + (content.length > 1000 ? '...' : '');
    }
    return 'No content found with the provided selectors.';

  } catch (error) {
    console.error(`[Content Scrape Test Error] for ${articleUrl}: ${error.message}`)
    return `Error scraping content: ${error.message}`
  }
}

export async function testHeadlineExtraction(sourceConfig, html) {
  let pageHtml = html
  if (!pageHtml) {
    pageHtml = await fetchPageWithPlaywright(sourceConfig.sectionUrl, 'TestHeadlineScraper')
  }
  const $ = cheerio.load(pageHtml)
  const articles = []
  const selectors = Array.isArray(sourceConfig.headlineSelector) ? sourceConfig.headlineSelector : [sourceConfig.headlineSelector].filter(Boolean);

  for (const selector of selectors) {
    switch (sourceConfig.extractionMethod) {
      case 'json-ld':
        $('script[type="application/ld+json"]').each((_, el) => {
          try {
            const jsonData = JSON.parse($(el).html())
            const potentialLists = [jsonData, ...(jsonData['@graph'] || [])]
            potentialLists.forEach((list) => {
              const items = list?.itemListElement
              if (Array.isArray(items)) {
                items.forEach((item) => {
                  const headline = item.name || item.item?.name
                  const url = item.url || item.item?.url
                  if (headline && url) {
                    articles.push({ headline: headline.trim(), link: new URL(url, sourceConfig.baseUrl).href })
                  }
                })
              }
            })
          } catch (e) {}
        })
        break
      case 'declarative':
        $(selector).each((_, el) => {
          const articleData = dynamicExtractor($, el, sourceConfig)
          if (articleData?.headline && articleData?.link) {
            articleData.link = new URL(articleData.link, sourceConfig.baseUrl).href
            articles.push(articleData)
          }
        })
        break
      case 'custom':
      default:
        const customExtractor = extractorRegistry[sourceConfig.extractorKey]
        if (!customExtractor) {
          throw new Error(`No custom extractor found for key: '${sourceConfig.extractorKey}'`)
        }
        $(selector).each((_, el) => {
          const articleData = customExtractor($(el), sourceConfig)
          if (articleData?.headline && articleData?.link) {
            articleData.link = new URL(articleData.link, sourceConfig.baseUrl).href
            articles.push(articleData)
          }
        })
        break
    }
  }
  return Array.from(new Map(articles.map((a) => [a.link, a])).values())
}

```

## ðŸ“„ src/test-orchestrator.js
*Lines: 52, Size: 1.66 KB*

```javascript
// packages/scraper-logic/src/test-orchestrator.js
import { testHeadlineExtraction, scrapeArticleContentForTest } from './scraper/index.js'
import { Source } from '@headlines/models'
import { browserManager } from './browserManager.js' // Import the manager

export async function testScraperRecipe(sourceConfig, articleUrl = null) {
  await browserManager.initialize() // Ensure browser is running for tests
  try {
    // Mode 1: Test a single article's content
    if (articleUrl && sourceConfig.articleSelector) {
      const content = await scrapeArticleContentForTest(
        articleUrl,
        sourceConfig.articleSelector
      )
      return { success: true, content: { preview: content, sourceUrl: articleUrl } }
    }

    // Mode 2: Test the full source recipe for headlines
    if (sourceConfig && sourceConfig.sectionUrl) {
      const headlines = await testHeadlineExtraction(sourceConfig)

      const success = headlines.length > 0
      await Source.findByIdAndUpdate(
        sourceConfig._id,
        {
          $set: {
            lastScrapedAt: new Date(),
            lastSuccessAt: success ? new Date() : undefined,
          },
          $inc: {
            'analytics.totalRuns': 1,
            'analytics.totalSuccesses': success ? 1 : 0,
            'analytics.totalFailures': success ? 0 : 1,
          },
        },
        { new: true }
      ).lean()

      return {
        success: true,
        headlines: {
          count: headlines.length,
          samples: headlines.slice(0, 10),
        },
      }
    }
    throw new Error('Invalid request payload for testScraperRecipe.')
  } finally {
    await browserManager.close() // Close browser after test
  }
}

```
