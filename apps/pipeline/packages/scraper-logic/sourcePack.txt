# 📁 PROJECT DIRECTORY STRUCTURE

Total: 60 files, 9 directories

```
headlines/
├── 📁 src/
│   ├── 📁 ai/
│   │   ├── 📁 agents/
│   │   │   ├── 📄 articleAgent.js
│   │   │   ├── 📄 articlePreAssessmentAgent.js
│   │   │   ├── 📄 batchArticleAgent.js
│   │   │   ├── 📄 clusteringAgent.js
│   │   │   ├── 📄 contactAgent.js
│   │   │   ├── 📄 emailAgents.js
│   │   │   ├── 📄 entityAgent.js
│   │   │   ├── 📄 executiveSummaryAgent.js
│   │   │   ├── 📄 headlineAgent.js
│   │   │   ├── 📄 judgeAgent.js
│   │   │   ├── 📄 opportunityAgent.js
│   │   │   ├── 📄 sectionClassifierAgent.js
│   │   │   ├── 📄 selectorRepairAgent.js
│   │   │   ├── 📄 synthesisAgent.js
│   │   │   └── 📄 watchlistAgent.js
│   │   ├── 📁 schemas/
│   │   │   ├── 📄 articleAssessmentSchema.js
│   │   │   ├── 📄 articlePreAssessmentSchema.js
│   │   │   ├── 📄 batchArticleAssessmentSchema.js
│   │   │   ├── 📄 canonicalizerSchema.js
│   │   │   ├── 📄 clusterSchema.js
│   │   │   ├── 📄 disambiguationSchema.js
│   │   │   ├── 📄 emailIntroSchema.js
│   │   │   ├── 📄 emailSubjectSchema.js
│   │   │   ├── 📄 enrichContactSchema.js
│   │   │   ├── 📄 entitySchema.js
│   │   │   ├── 📄 findContactSchema.js
│   │   │   ├── 📄 headlineAssessmentSchema.js
│   │   │   ├── 📄 judgeSchema.js
│   │   │   ├── 📄 opportunitySchema.js
│   │   │   ├── 📄 sectionClassifierSchema.js
│   │   │   ├── 📄 selectorRepairSchema.js
│   │   │   ├── 📄 sourceNameSchema.js
│   │   │   ├── 📄 synthesisSchema.js
│   │   │   └── 📄 watchlistSuggestionSchema.js
│   │   ├── 📄 AIAgent.js
│   │   └── 📄 index.js
│   ├── 📁 push/
│   │   └── 📄 client.js
│   ├── 📁 scraper/
│   │   ├── 📁 extractors/
│   │   │   ├── 📁 reusable/
│   │   │   │   └── 📄 simple.js
│   │   │   ├── 📁 source-specific/
│   │   │   │   ├── 📄 cvcPortfolio.js
│   │   │   │   ├── 📄 cvcPortfolioContent.js
│   │   │   │   ├── 📄 finansDk.js
│   │   │   │   ├── 📄 jyllandsPosten.js
│   │   │   │   ├── 📄 okonomiskUgebrev.js
│   │   │   │   ├── 📄 politiken.js
│   │   │   │   └── 📄 verdane.js
│   │   │   └── 📄 index.js
│   │   ├── 📄 constants.js
│   │   ├── 📄 contentScraper.js
│   │   ├── 📄 dynamicExtractor.js
│   │   ├── 📄 extractor.js.bak
│   │   ├── 📄 headlineScraper.js
│   │   ├── 📄 index.js
│   │   ├── 📄 newsApiScraper.js
│   │   ├── 📄 orchestrator.js
│   │   ├── 📄 selectorOptimizer.js
│   │   └── 📄 test-helpers.js
│   ├── 📄 browser.js
│   ├── 📄 config.js
│   └── 📄 index.js
└── 📄 package.json
```

# 📋 PROJECT METADATA

**Generated**: 2025-09-17T10:46:26.452Z
**Repository Path**: /home/mark/Repos/projects/headlines/packages/scraper-logic
**Total Files**: 60
**Package**: @headlines/scraper-logic@1.0.0
**Description**: Shared scraping, AI, and notification logic for the Headlines monorepo.



---


## 📄 package.json
*Lines: 30, Size: 782 Bytes*

```json
{
  "name": "@headlines/scraper-logic",
  "version": "1.0.0",
  "description": "Shared scraping, AI, and notification logic for the Headlines monorepo.",
  "main": "src/index.js",
  "type": "module",
  "license": "ISC",
  "dependencies": {
    "@headlines/ai-services": "1.0.0",
    "@headlines/config": "3.0.0",
    "@headlines/models": "1.0.0",
    "@headlines/prompts": "1.0.0",
    "@headlines/utils": "1.2.0",
    "@mozilla/readability": "^0.5.0",
    "axios": "^1.7.2",
    "cheerio": "^1.0.0-rc.12",
    "date-fns": "*",
    "jsdom": "^24.1.1",
    "lodash": "*",
    "newsapi": "^2.4.1",
    "p-limit": "^5.0.0",
    "playwright": "^1.45.1",
    "pusher": "^5.2.0",
    "rss-parser": "^3.13.0",
    "web-push": "^3.6.7",
    "zod": "^3.23.8"
  },
  "peerDependencies": {}
}

```

## 📄 src/ai/AIAgent.js
*Lines: 64, Size: 1.83 KB*

```javascript
// packages/scraper-logic/src/ai/AIAgent.js (version 3.2.0)
import { callLanguageModel } from '@headlines/ai-services'
import { getConfig } from '../config.js';

export class AIAgent {
  constructor({
    model,
    systemPrompt,
    isJson = true,
    fewShotInputs = [],
    fewShotOutputs = [],
    zodSchema,
  }) {
    if (!model || !systemPrompt) {
      throw new Error('AIAgent requires a model and systemPrompt.')
    }
    this.model = model
    this.systemPrompt = systemPrompt
    this.isJson = isJson
    this.fewShotInputs = fewShotInputs
    this.fewShotOutputs = fewShotOutputs
    this.zodSchema = zodSchema

    getConfig().logger.trace(
      { agentConfig: { model, isJson, hasSchema: !!zodSchema } },
      'Initialized new AIAgent.'
    )
  }

  async execute(userContent) {
    let systemPromptContent = this.systemPrompt
    // If the provided prompt is a function, execute it to get the dynamic prompt object
    if (typeof systemPromptContent === 'function') {
      systemPromptContent = systemPromptContent()
    }

    const response = await callLanguageModel({
      modelName: this.model, // Note: The new function expects modelName
      systemPrompt: systemPromptContent,
      userContent,
      isJson: this.isJson,
      fewShotInputs: this.fewShotInputs,
      fewShotOutputs: this.fewShotOutputs,
    })

    if (this.isJson && this.zodSchema && !response.error) {
      const validationResult = this.zodSchema.safeParse(response)
      if (!validationResult.success) {
        getConfig().logger.error(
          {
            details: validationResult.error.flatten(),
            model: this.model,
          },
          `AI response failed Zod validation.`
        )
        return { error: 'Zod validation failed', details: validationResult.error }
      }
      return validationResult.data
    }

    return response
  }
}

```

## 📄 src/ai/agents/articleAgent.js
*Lines: 90, Size: 3.54 KB*

```javascript
// packages/scraper-logic/src/ai/agents/articleAgent.js (version 3.4.1)
import { truncateString } from '@headlines/utils';
import { AIAgent } from '../AIAgent.js'
import { articleAssessmentSchema } from '../schemas/articleAssessmentSchema.js'
import { env } from '@headlines/config'
import { getInstructionArticle } from '@headlines/prompts'
import { shotsInputArticle } from '@headlines/prompts'
import { shotsOutputArticle } from '@headlines/prompts'
import { getConfig } from '../../config.js'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionArticle,
    fewShotInputs: shotsInputArticle,
    fewShotOutputs: shotsOutputArticle,
    zodSchema: articleAssessmentSchema,
  })

function findWatchlistHits(text, country) {
  const hits = []
  const lowerText = text.toLowerCase()
  const config = getConfig();
  if (!config.configStore) return []

  for (const entity of config.configStore.watchlistEntities.values()) {
      if (entity.country && entity.country !== country && entity.country !== 'Global PE' && entity.country !== 'M&A Aggregators') {
          continue;
      }

      const terms = [entity.name.toLowerCase(), ...(entity.searchTerms || [])]
      for (const term of terms) {
          if (term.length > 3 && lowerText.includes(term)) {
              hits.push(entity)
              break; 
          }
      }
  }
  return [...new Map(hits.map((item) => [item['name'], item])).values()]
}

export async function assessArticleContent(article, isSalvaged = false) {
  const articleAssessmentAgent = getAgent()
  const fullContent = (article.articleContent?.contents || []).join('\n')
  const truncatedContent = truncateString(fullContent, getConfig().settings.LLM_CONTEXT_MAX_CHARS)

  if (fullContent.length > getConfig().settings.LLM_CONTEXT_MAX_CHARS) {
    getConfig().logger.warn(
      {
        originalLength: fullContent.length,
        truncatedLength: truncatedContent.length,
        limit: getConfig().settings.LLM_CONTEXT_MAX_CHARS,
      },
      `Article content for LLM was truncated to prevent context overload.`
    )
  }

  const combinedTextForHitCheck = `${article.headline}\n${truncatedContent}`
  const hits = findWatchlistHits(combinedTextForHitCheck, article.country)
  let articleText = `HEADLINE: ${article.headline}\n\nBODY:\n${truncatedContent}`

  if (hits.length > 0) {
    const hitStrings = hits.map((hit) => `[WATCHLIST HIT: ${hit.name} | CONTEXT: ${hit.context || 'N/A'}]`)
    const hitPrefix = hitStrings.join(' ')
    articleText = `${hitPrefix} ${articleText}`
    getConfig().logger.info({ hits: hits.map((h) => h.name) }, 'Watchlist entities found in article content.')
  }
  
  if (isSalvaged) {
      articleText = `[SALVAGE CONTEXT: The original source for this headline failed to scrape. This content is from an alternative source. Please assess based on this new context.]\n\n${articleText}`;
  }

  const response = await articleAssessmentAgent.execute(articleText)

  if (response.error) {
    getConfig().logger.error(
      { article: { link: article.link }, details: response },
      `Article assessment failed for ${article.link}.`
    )
    return { ...article, error: `AI Error: ${response.error}` }
  }

  if (response.amount > 0 && response.amount < getConfig().settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS) {
      response.relevance_article = 10;
      response.assessment_article = `Dropped: Amount ($${response.amount}M) is below the financial threshold of $${getConfig().settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS}M.`
  }

  return { ...article, ...response, error: null }
}

```

## 📄 src/ai/agents/articlePreAssessmentAgent.js
*Lines: 22, Size: 780 Bytes*

```javascript
// packages/scraper-logic/src/ai/agents/articlePreAssessmentAgent.js (version 2.2.1)
import { AIAgent } from '../AIAgent.js'
import { articlePreAssessmentSchema } from '../schemas/articlePreAssessmentSchema.js'
import { env } from '@headlines/config'
import { instructionArticlePreAssessment } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_UTILITY,
    systemPrompt: instructionArticlePreAssessment,
    zodSchema: articlePreAssessmentSchema,
  })

export async function preAssessArticle(articleContent) {
  const articlePreAssessmentAgent = getAgent()
  const response = await articlePreAssessmentAgent.execute(articleContent)
  if (response.error) {
    return { classification: null, error: response.error }
  }
  return response
}

```

## 📄 src/ai/agents/batchArticleAgent.js
*Lines: 59, Size: 1.98 KB*

```javascript
// packages/scraper-logic/src/ai/agents/batchArticleAgent.js (version 1.0)
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js'
import { batchArticleAssessmentSchema } from '../schemas/batchArticleAssessmentSchema.js'
import { env } from '@headlines/config'
import { getInstructionBatchArticleAssessment } from '@headlines/prompts'
import { assessArticleContent } from './articleAgent.js' // Fallback

const BATCH_SIZE = 5;

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionBatchArticleAssessment,
    zodSchema: batchArticleAssessmentSchema,
  });

export async function batchAssessArticles(articles) {
  if (!articles || articles.length === 0) return [];

  const batchAgent = getAgent();
  const articleBatches = [];
  for (let i = 0; i < articles.length; i += BATCH_SIZE) {
    articleBatches.push(articles.slice(i, i + BATCH_SIZE));
  }

  const allResults = [];

  for (const batch of articleBatches) {
    const payload = batch.map(article => ({
      headline: article.headline,
      content: (article.articleContent?.contents || []).join('\n'),
    }));

    const response = await batchAgent.execute(JSON.stringify(payload));
    
    if (response.error || response.assessments.length !== batch.length) {
      getConfig().logger.error({ 
          details: response,
          expectedCount: batch.length, 
          receivedCount: response.assessments?.length 
      }, "Batch assessment failed or returned mismatched count. Falling back to single-article processing for this batch.");
      
      const fallbackPromises = batch.map(article => assessArticleContent(article));
      const fallbackResults = await Promise.all(fallbackPromises);
      allResults.push(...fallbackResults);
      continue;
    }

    const mergedResults = batch.map((originalArticle, index) => ({
      ...originalArticle,
      ...response.assessments[index],
    }));
    allResults.push(...mergedResults);
  }

  return allResults;
}

```

## 📄 src/ai/agents/clusteringAgent.js
*Lines: 67, Size: 2.27 KB*

```javascript
// packages/scraper-logic/src/ai/agents/clusteringAgent.js (version 2.2.1)
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js'
import { clusterSchema } from '../schemas/clusterSchema.js'
import { env } from '@headlines/config'
import { instructionCluster } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionCluster,
    zodSchema: clusterSchema,
  })

export async function clusterArticlesIntoEvents(articles) {
  const articleClusterAgent = getAgent()
  getConfig().logger.info(`Clustering ${articles.length} articles into unique events...`)
  const CLUSTER_BATCH_SIZE = 25
  const batches = []
  for (let i = 0; i < articles.length; i += CLUSTER_BATCH_SIZE) {
    batches.push(articles.slice(i, i + CLUSTER_BATCH_SIZE))
  }
  getConfig().logger.info(`Processing clusters in ${batches.length} batches.`)

  const allClusters = []
  for (const [index, batch] of batches.entries()) {
    getConfig().logger.info(`Clustering batch ${index + 1} of ${batches.length}...`)
    const articlePayload = batch.map((a) => ({
      id: a._id.toString(),
      headline: a.headline,
      source: a.newspaper,
      summary: (a.topic || a.assessment_article || '').substring(0, 400),
    }))
    const userContent = JSON.stringify(articlePayload)
    const response = await articleClusterAgent.execute(userContent)

    if (response.error || !response.events) {
      getConfig().logger.error(`Failed to cluster articles in batch ${index + 1}.`, { response })
      continue
    }
    allClusters.push(...response.events)
  }

  if (allClusters.length === 0) {
    getConfig().logger.warn('Failed to cluster any articles across all batches.')
    return []
  }

  const finalEventMap = new Map()
  allClusters.forEach((event) => {
    if (finalEventMap.has(event.event_key)) {
      const existing = finalEventMap.get(event.event_key)
      event.article_ids.forEach((id) => existing.article_ids.add(id))
    } else {
      finalEventMap.set(event.event_key, {
        event_key: event.event_key,
        article_ids: new Set(event.article_ids),
      })
    }
  })

  return Array.from(finalEventMap.values()).map((event) => ({
    event_key: event.event_key,
    article_ids: Array.from(event.article_ids),
  }))
}

```

## 📄 src/ai/agents/contactAgent.js
*Lines: 104, Size: 3.25 KB*

```javascript
// packages/scraper-logic/src/ai/agents/contactAgent.js (version 3.1.3)
import { AIAgent } from '../AIAgent.js'
import { enrichContactSchema } from '../schemas/enrichContactSchema.js'
import { findContactSchema } from '../schemas/findContactSchema.js'
import { env } from '@headlines/config'
import { instructionEnrichContact } from '@headlines/prompts'
import { instructionContacts } from '@headlines/prompts'
// DEFINITIVE FIX: Removed the duplicate import of getConfig.
import { getConfig } from '../../config.js'

const getResolverAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionEnrichContact,
    zodSchema: enrichContactSchema,
  })

const getFinderAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionContacts,
    zodSchema: findContactSchema,
  })

export async function resolveVagueContact(contact, article) {
  const contactResolverAgent = getResolverAgent()
  const config = getConfig()
  const researchQuery = `${contact.name} ${contact.company || article.newspaper}`
  const searchResult =
    await getConfig().utilityFunctions.performGoogleSearch(researchQuery)

  if (!searchResult.success) {
    getConfig().logger.warn(
      `[Resolve Agent] Google search failed for "${researchQuery}".`
    )
    return [contact]
  }

  const context = `Initial Contact Profile:\n${JSON.stringify(
    contact
  )}\n\nSource Article Headline: ${
    article.headline
  }\n\nGoogle Search Snippets:\n${searchResult.snippets}`
  const response = await contactResolverAgent.execute(context)

  if (
    response.error ||
    !response.enriched_contacts ||
    response.enriched_contacts.length === 0
  ) {
    getConfig().logger.warn(
      `[Resolve Agent] Failed to resolve contact "${contact.name}".`
    )
    return [contact]
  }

  getConfig().logger.info(
    `[Resolve Agent] Successfully resolved "${contact.name}" -> "${response.enriched_contacts
      .map((c) => c.name)
      .join(', ')}"`
  )
  return response.enriched_contacts
}

export async function findContactDetails(person) {
  const contactFinderAgent = getFinderAgent()
  const config = getConfig()
  getConfig().logger.info(`[Contact Research Agent] Initiated for: ${person.reachOutTo}`)
  const queries = [
    `"${person.reachOutTo}" ${person.contactDetails.company} email address`,
    `"${person.reachOutTo}" contact information`,
  ]

  let combinedSnippets = ''
  for (const query of queries) {
    const searchResult = await getConfig().utilityFunctions.performGoogleSearch(query)
    if (searchResult.success && searchResult.snippets) {
      combinedSnippets += `\n--- Results for query: "${query}" ---\n${searchResult.snippets}`
    }
  }

  if (!combinedSnippets) {
    getConfig().logger.warn(
      `[Contact Research Agent] No search results for "${person.reachOutTo}".`
    )
    return { email: null }
  }

  const response = await contactFinderAgent.execute(combinedSnippets)

  if (response.error || !response.email) {
    getConfig().logger.warn(
      `[Contact Research Agent] LLM failed to extract details for "${person.reachOutTo}".`
    )
    return { email: null }
  }

  getConfig().logger.info(
    { details: response },
    `[Contact Research Agent] Found details for "${person.reachOutTo}".`
  )
  return response
}

```

## 📄 src/ai/agents/emailAgents.js
*Lines: 58, Size: 2.21 KB*

```javascript
// packages/scraper-logic/src/ai/agents/emailAgents.js (version 2.2.1)
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js'
import { emailSubjectSchema } from '../schemas/emailSubjectSchema.js'
import { emailIntroSchema } from '../schemas/emailIntroSchema.js'
import { env } from '@headlines/config'
import { instructionEmailSubject } from '@headlines/prompts'
import { instructionEmailIntro } from '@headlines/prompts'

const getAgent = (systemPrompt, zodSchema) =>
  new AIAgent({
    model: env.LLM_MODEL_SYNTHESIS,
    systemPrompt,
    zodSchema,
  })

export async function generateEmailSubjectLine(events) {
  const subjectLineAgent = getAgent(instructionEmailSubject, emailSubjectSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const response = await subjectLineAgent.execute(JSON.stringify(eventPayload))
    if (response.error || !response.subject_headline) {
      getConfig().logger.warn('AI failed to generate a custom email subject line.', response)
      return 'Key Developments' // Fallback
    }
    return response.subject_headline
  } catch (error) {
    getConfig().logger.error({ err: error }, 'Error in generateEmailSubjectLine')
    return 'Key Developments' // Fallback
  }
}

export async function generatePersonalizedIntro(user, events) {
  const introAgent = getAgent(instructionEmailIntro, emailIntroSchema)
  try {
    const eventPayload = events.map((e) => ({
      headline: e.synthesized_headline,
      summary: e.synthesized_summary,
    }))
    const payload = {
      firstName: user.firstName,
      events: eventPayload,
    }
    const response = await introAgent.execute(JSON.stringify(payload))
    if (response.error || !response.intro_text) {
      getConfig().logger.warn('AI failed to generate a personalized intro.', response)
      return `Dear ${user.firstName}, here are the latest relevant wealth events we have identified.`
    }
    return response.intro_text
  } catch (error) {
    getConfig().logger.error({ err: error }, 'Error in generatePersonalizedIntro')
    return `Dear ${user.firstName}, here are the latest relevant wealth events we have identified.`
  }
}

```

## 📄 src/ai/agents/entityAgent.js
*Lines: 73, Size: 2.37 KB*

```javascript
// packages/scraper-logic/src/ai/agents/entityAgent.js (version 3.1.2)
import { AIAgent } from '../AIAgent.js'
import { entitySchema } from '../schemas/entitySchema.js'
import { canonicalizerSchema } from '../schemas/canonicalizerSchema.js'
import { env } from '@headlines/config'
import { instructionEntity } from '@headlines/prompts'
import { instructionCanonicalizer } from '@headlines/prompts'
// DEFINITIVE FIX: Removed the duplicate import of getConfig.
import { getConfig } from '../../config.js'

const getEntityExtractorAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_UTILITY,
    systemPrompt: instructionEntity,
    zodSchema: entitySchema,
  })

const getEntityCanonicalizerAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_UTILITY,
    systemPrompt: instructionCanonicalizer,
    zodSchema: canonicalizerSchema,
  })

export const entityCanonicalizerAgent = () => getEntityCanonicalizerAgent()

export async function extractEntities(text) {
  const entityExtractorAgent = getEntityExtractorAgent()
  const canonicalizer = getEntityCanonicalizerAgent()

  if (!text) return []
  try {
    const response = await entityExtractorAgent.execute(`Article Text:\n${text}`)

    if (response.error) {
      throw new Error(response.error)
    }

    const { reasoning, entities } = response
    getConfig().logger.info(`[Query Planner Agent] Reasoning: ${reasoning}`)
    if (!entities || !Array.isArray(entities)) return []

    const canonicalizationPromises = entities
      .map((entity) => entity.replace(/\s*\(.*\)\s*/g, '').trim())
      .filter(Boolean)
      .map(async (entity) => {
        const canonResponse = await canonicalizer.execute(entity)
        if (canonResponse && !canonResponse.error && canonResponse.canonical_name) {
          getConfig().logger.trace(
            `Canonicalized "${entity}" -> "${canonResponse.canonical_name}"`
          )
          return canonResponse.canonical_name
        }
        return null
      })

    const canonicalEntities = await Promise.all(canonicalizationPromises)
    const uniqueEntities = [...new Set(canonicalEntities.filter(Boolean))]

    getConfig().logger.info(
      { entities: uniqueEntities },
      `Final list of canonical entities for RAG.`
    )
    return uniqueEntities
  } catch (error) {
    getConfig().logger.warn(
      { err: error },
      'Wikipedia query planning (entity extraction) failed.'
    )
    return []
  }
}

```

## 📄 src/ai/agents/executiveSummaryAgent.js
*Lines: 39, Size: 1.33 KB*

```javascript
// packages/scraper-logic/src/ai/agents/executiveSummaryAgent.js (version 2.0)
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js';
import { env } from '@headlines/config';
import { instructionExecutiveSummary } from '@headlines/prompts';
import { z } from 'zod';

const executiveSummarySchema = z.object({
  summary: z.string(),
});

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionExecutiveSummary,
    zodSchema: executiveSummarySchema,
  });

export async function generateExecutiveSummary(judgeVerdict, runStats) {
  const agent = getAgent();
  try {
    // Create a payload with the crucial context
    const payload = {
      freshHeadlinesFound: runStats.freshHeadlinesFound,
      judgeVerdict: judgeVerdict || { event_judgements: [], opportunity_judgements: [] },
    };

    const response = await agent.execute(JSON.stringify(payload));
    if (response.error || !response.summary) {
      getConfig().logger.warn('AI failed to generate an executive summary.', response);
      return 'AI failed to generate a summary for this run.';
    }
    return response.summary;
  } catch (error) {
    getConfig().logger.error({ err: error }, 'Error in generateExecutiveSummary');
    return 'An unexpected error occurred while generating the executive summary.';
  }
}

```

## 📄 src/ai/agents/headlineAgent.js
*Lines: 89, Size: 3.38 KB*

```javascript
// packages/scraper-logic/src/ai/agents/headlineAgent.js (version 4.1.3)
// DEFINITIVE FIX: Removed p-limit as it uses server-only modules.
// Using Promise.all is sufficient for this use case.
import { AIAgent } from '../AIAgent.js'
import { headlineAssessmentSchema } from '../schemas/headlineAssessmentSchema.js'
import { env } from '@headlines/config'
import { instructionHeadlines } from '@headlines/prompts'
import { shotsInputHeadlines } from '@headlines/prompts'
import { shotsOutputHeadlines } from '@headlines/prompts'
import { getConfig } from '../../config.js'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_HEADLINE_ASSESSMENT,
    systemPrompt: instructionHeadlines,
    fewShotInputs: shotsInputHeadlines,
    fewShotOutputs: shotsOutputHeadlines,
    zodSchema: headlineAssessmentSchema,
  })

function findWatchlistHits(text, country) {
  const hits = new Map()
  const lowerText = text.toLowerCase()
  const config = getConfig();
  if (!getConfig().configStore?.watchlistEntities) return []
  
  const createSearchRegex = (term) => new RegExp(`\\b${term.replace(/[-\/\\^$*+?.()|[\]{}]/g, '\\$&')}\\b`, 'i')
  
  const relevantEntities = Array.from(getConfig().configStore.watchlistEntities.values()).filter(entity => 
      !entity.country || entity.country === country || entity.country === 'Global PE' || entity.country === 'M&A Aggregators'
  );

  for (const entity of relevantEntities) {
      const nameKey = entity.name.toLowerCase();
      if (nameKey.length > 3 && createSearchRegex(nameKey).test(lowerText)) {
          if (!hits.has(entity.name)) hits.set(entity.name, { entity, matchedTerm: nameKey })
      }
      for(const term of (entity.searchTerms || [])) {
          if (term.length > 3 && createSearchRegex(term).test(lowerText)) {
              if (!hits.has(entity.name)) hits.set(entity.name, { entity, matchedTerm: term })
          }
      }
  }
  return Array.from(hits.values())
}

async function assessSingleHeadline(article) {
  const headlineAssessmentAgent = getAgent();
  const hits = findWatchlistHits(article.headline, article.country);
  let headlineWithContext = `[COUNTRY CONTEXT: ${article.country}] ${article.headline}`;
  
  if (hits.length > 0) {
    const hitStrings = hits.map((hit) => `[WATCHLIST HIT: ${hit.entity.name} (matched on '${hit.matchedTerm}')]`).join(' ');
    headlineWithContext = `${hitStrings} ${headlineWithContext}`;
  }

  const response = await headlineAssessmentAgent.execute(headlineWithContext);
  
  let assessment = {
    relevance_headline: 0,
    assessment_headline: 'AI assessment failed.',
    headline_en: article.headline,
  };

  if (response && response.assessment && response.assessment.length > 0) {
    assessment = response.assessment[0];
    let score = assessment.relevance_headline;
    const boost = getConfig().settings.WATCHLIST_SCORE_BOOST;
    if (hits.length > 0 && boost > 0) {
      score = Math.min(100, score + boost);
      assessment.assessment_headline = `Watchlist boost (+${boost}). ${assessment.assessment_headline}`;
    }
    assessment.relevance_headline = score;
  }
  
  return { ...article, ...assessment };
}


export async function assessHeadlinesInBatches(articles) {
  // Using Promise.all provides sufficient concurrency for this operation.
  const assessmentPromises = articles.map(article => 
    assessSingleHeadline(article)
  );
  
  const results = await Promise.all(assessmentPromises);
  return results;
}

```

## 📄 src/ai/agents/judgeAgent.js
*Lines: 54, Size: 1.82 KB*

```javascript
// packages/scraper-logic/src/ai/agents/judgeAgent.js (version 2.3.2)
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js'
import { judgeSchema } from '../schemas/judgeSchema.js'
import { env } from '@headlines/config'
import { instructionJudge } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionJudge,
    zodSchema: judgeSchema,
  })

export async function judgePipelineOutput(events, opportunities) {
  const judgeAgent = getAgent()
  if ((!events || events.length === 0) && (!opportunities || opportunities.length === 0)) {
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }
  getConfig().logger.info('⚖️ [Judge Agent] Reviewing final pipeline output for quality control...')
  const lightweightEvents = (events || []).map((e) => ({
    identifier: `Event: ${e.synthesized_headline}`,
    summary: e.synthesized_summary,
    assessment: e.ai_assessment_reason,
    score: e.highest_relevance_score,
  }))
  const lightweightOpportunities = (opportunities || []).map((o) => ({
    identifier: `Opportunity: ${o.reachOutTo}`,
    reason: o.whyContact,
    wealth_estimate_mm: o.likelyMMDollarWealth,
  }))
  const inputText = JSON.stringify({
    events: lightweightEvents,
    opportunities: lightweightOpportunities,
  })
  const response = await judgeAgent.execute(inputText)
  if (response.error) {
    getConfig().logger.error({ details: response }, 'Judge Agent failed to produce a verdict.')
    // Return a default "empty" verdict on failure to avoid crashing the pipeline
    return {
      event_judgements: [],
      opportunity_judgements: [],
    }
  }
  getConfig().logger.info(
    { details: response },
    '[Judge Agent] Successfully produced quality control verdicts.'
  )
  return response
}

```

## 📄 src/ai/agents/opportunityAgent.js
*Lines: 76, Size: 2.57 KB*

```javascript
// packages/scraper-logic/src/ai/agents/opportunityAgent.js (version 3.2.2)
import { truncateString } from '@headlines/utils';
import { AIAgent } from '../AIAgent.js'
import { opportunitySchema } from '../schemas/opportunitySchema.js'
import { enrichContactSchema } from '../schemas/enrichContactSchema.js'
import { env } from '@headlines/config'
import { getInstructionOpportunities } from '@headlines/prompts'
import { instructionEnrichContact } from '@headlines/prompts'
import { getConfig } from '../../config.js'

const getOppAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: getInstructionOpportunities,
    zodSchema: opportunitySchema,
  })

const getContactResolverAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_ARTICLE_ASSESSMENT,
    systemPrompt: instructionEnrichContact,
    zodSchema: enrichContactSchema,
  })

export async function generateOpportunitiesFromEvent(
  synthesizedEvent,
  articlesInCluster
) {
  const opportunityGeneratorAgent = getOppAgent()

  const highestRelevanceArticle = articlesInCluster.reduce((max, current) =>
    (current.relevance_article || 0) > (max.relevance_article || 0) ? current : max
  )

  const fullText = articlesInCluster
    .map((a) => (a.articleContent?.contents || []).join('\n'))
    .join('\n\n')

  const inputText = `
        Synthesized Event Headline: ${synthesizedEvent.synthesized_headline}
        Synthesized Event Summary: ${synthesizedEvent.synthesized_summary}
        Key Individuals already identified: ${JSON.stringify(synthesizedEvent.key_individuals)}
        Source Article Snippets: ${truncateString(fullText, getConfig().settings.LLM_CONTEXT_MAX_CHARS)}
    `
  const response = await opportunityGeneratorAgent.execute(inputText)

  if (response.error || !response.opportunities) {
    getConfig().logger.warn(
      { event: synthesizedEvent.synthesized_headline, details: response },
      `Opportunity generation failed.`
    )
    return []
  }

  const validOpportunities = (response.opportunities || []).filter(
    (opp) => opp.likelyMMDollarWealth >= getConfig().settings.MINIMUM_EVENT_AMOUNT_USD_MILLIONS
  )

  const opportunitiesWithSource = validOpportunities.map((opp) => ({
    ...opp,
    event_key: synthesizedEvent.event_key,
    sourceArticleId: highestRelevanceArticle._id,
  }))

  getConfig().logger.info(
    { details: opportunitiesWithSource },
    `[Opportunity Agent] Generated ${
      opportunitiesWithSource.length
    } opportunity/ies from event "${truncateString(
      synthesizedEvent.synthesized_headline,
      50
    )}"`
  )
  return opportunitiesWithSource
}

```

## 📄 src/ai/agents/sectionClassifierAgent.js
*Lines: 36, Size: 1.82 KB*

```javascript
// packages/scraper-logic/src/ai/agents/sectionClassifierAgent.js (version 1.0)
import { AIAgent } from '../AIAgent.js';
import { sectionClassifierSchema } from '../schemas/sectionClassifierSchema.js';
import { env } from '@headlines/config';

const INSTRUCTION = `You are a master website navigation analyst. Your task is to analyze a list of hyperlinks (anchor text and href) from a webpage and classify each one into one of four categories.

**Categories:**
1.  **"news_section"**: A link to a major category or section of news (e.g., "Business", "Technology", "World News", "/erhverv", "/økonomi"). These are typically found in main navigation bars.
2.  **"article_headline"**: A link to a specific news article or story. The text is usually a full sentence or a descriptive title.
3.  **"navigation"**: A link to a functional page on the site (e.g., "About Us", "Contact", "Login", "Subscribe").
4.  **"other"**: Any other type of link, such as advertisements, privacy policies, terms of service, or social media links.

**Instructions:**
-   You will receive a JSON array of link objects.
-   You MUST return a JSON object with a single key, "classifications".
-   The "classifications" array MUST contain one classification object for EACH link in the input, in the EXACT SAME ORDER.
-   Base your decision on both the link's text and its URL structure.
`;

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_UTILITY, // Using the cheap and fast model
    systemPrompt: INSTRUCTION,
    zodSchema: sectionClassifierSchema,
  });

export async function classifyLinks(links) {
    const agent = getAgent();
    const response = await agent.execute(JSON.stringify(links));
    if (response.error || !response.classifications || response.classifications.length !== links.length) {
        return null;
    }
    return response.classifications;
}

```

## 📄 src/ai/agents/selectorRepairAgent.js
*Lines: 44, Size: 1.43 KB*

```javascript
// packages/scraper-logic/src/ai/agents/selectorRepairAgent.js (version 3.0.0)
import { truncateString } from '@headlines/utils';
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js'
import { selectorRepairSchema } from '../schemas/selectorRepairSchema.js'
import { env } from '@headlines/config'
import { instructionSelectorRepair } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_UTILITY,
    systemPrompt: instructionSelectorRepair,
    zodSchema: selectorRepairSchema,
  })

export async function suggestNewSelector(url, failedSelector, htmlContent, heuristicSuggestions = []) {
  const selectorRepairAgent = getAgent()
  try {
    const payload = {
      url,
      failed_selector: failedSelector,
      // NEW: Add heuristic suggestions to the payload
      heuristic_suggestions: heuristicSuggestions.map(s => ({
          selector: s.selector,
          samples: s.samples.slice(0, 3)
      })),
      html_content: truncateString(htmlContent, 30000),
    }

    const response = await selectorRepairAgent.execute(JSON.stringify(payload))
    if (response.error || !response.suggested_selectors) {
      getConfig().logger.error('Selector repair agent failed to produce a valid suggestion.', {
        response,
      })
      return null
    }

    return response
  } catch (error) {
    getConfig().logger.error({ err: error }, 'Error in suggestNewSelector')
    return null
  }
}

```

## 📄 src/ai/agents/synthesisAgent.js
*Lines: 102, Size: 3.09 KB*

```javascript
// packages/scraper-logic/src/ai/agents/synthesisAgent.js (version 3.0.2)
import { truncateString } from '@headlines/utils'
import { AIAgent } from '../AIAgent.js'
import { synthesisSchema } from '../schemas/synthesisSchema.js'
import { env } from '@headlines/config'
import { instructionSynthesize } from '@headlines/prompts'
// DEFINITIVE FIX: Removed the duplicate import of getConfig.
import { getConfig } from '../../config.js'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionSynthesize,
    zodSchema: synthesisSchema,
  })

export async function synthesizeEvent(
  articlesInCluster,
  historicalContext,
  wikipediaContext,
  newsApiContext,
  gdeltContext
) {
  const eventSynthesizerAgent = getAgent()
  const config = getConfig()

  const todayPayload = articlesInCluster.map((a) => ({
    headline: a.headline,
    source: a.newspaper,
    full_text: truncateString(
      (a.articleContent?.contents || []).join('\n'),
      getConfig().settings.LLM_CONTEXT_MAX_CHARS / articlesInCluster.length
    ),
    key_individuals: a.key_individuals || [],
  }))

  const historyPayload = historicalContext.map((h) => ({
    headline: h.headline,
    source: h.newspaper,
    published: h.createdAt,
    summary: h.assessment_article || '',
  }))

  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT (Internal Database) ]': historyPayload,
    '[ PUBLIC WIKIPEDIA CONTEXT ]': wikipediaContext || 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': newsApiContext || 'Not available.',
    '[ GLOBAL GDELT CONTEXT ]': gdeltContext || 'Not available.',
  }

  getConfig().logger.trace(
    { synthesis_context: userContent },
    '--- SYNTHESIS CONTEXT ---'
  )

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    getConfig().logger.error('Failed to synthesize event.', { response })
    return { error: 'Synthesis failed' }
  }
  return response
}

export async function synthesizeFromHeadline(article) {
  const eventSynthesizerAgent = getAgent()
  getConfig().logger.warn(
    { headline: article.headline },
    `Salvaging high-signal headline with failed enrichment...`
  )
  const todayPayload = [
    {
      headline: article.headline,
      source: article.newspaper,
      full_text:
        "NOTE: Full article text could not be retrieved. Synthesize based on the headline's explicit claims and your general knowledge.",
      key_individuals: article.key_individuals || [],
    },
  ]
  const userContent = {
    "[ TODAY'S NEWS ]": todayPayload,
    '[ HISTORICAL CONTEXT ]': [],
    '[ PUBLIC WIKIPEDIA CONTEXT ]': 'Not available.',
    '[ LATEST NEWS CONTEXT (NewsAPI) ]': 'Not available.',
    '[ GLOBAL GDELT CONTEXT ]': 'Not available.',
  }

  getConfig().logger.trace(
    { synthesis_context: userContent },
    '--- SALVAGE SYNTHESIS CONTEXT ---'
  )

  const response = await eventSynthesizerAgent.execute(JSON.stringify(userContent))

  if (response.error) {
    getConfig().logger.error('Failed to salvage headline.', { response })
    return null
  }
  return response
}

```

## 📄 src/ai/agents/watchlistAgent.js
*Lines: 43, Size: 1.59 KB*

```javascript
// packages/scraper-logic/src/ai/agents/watchlistAgent.js (version 2.3.1)
import { getConfig } from '../../config.js';
import { AIAgent } from '../AIAgent.js'
import { watchlistSuggestionSchema } from '../schemas/watchlistSuggestionSchema.js'
import { env } from '@headlines/config'
import { instructionWatchlistSuggestion } from '@headlines/prompts'

const getAgent = () =>
  new AIAgent({
    model: env.LLM_MODEL_SYNTHESIS,
    systemPrompt: instructionWatchlistSuggestion,
    zodSchema: watchlistSuggestionSchema,
  })

/**
 * Analyzes events to generate new watchlist suggestions.
 * @param {Array<object>} events - High-quality synthesized events.
 * @param {Set<string>} existingWatchlistNames - A set of lowercase names already on the watchlist.
 * @returns {Promise<Array<object>>} An array of new WatchlistSuggestion documents.
 */
export async function generateWatchlistSuggestions(events, existingWatchlistNames) {
  const watchlistSuggestionAgent = getAgent()
  try {
    const payload = { events }
    const response = await watchlistSuggestionAgent.execute(JSON.stringify(payload))

    if (response.error || !Array.isArray(response.suggestions)) {
      getConfig().logger.warn('AI failed to generate watchlist suggestions.', response)
      return []
    }

    // Post-filter to ensure we don't suggest entities that already exist
    const newSuggestions = response.suggestions.filter(
      (s) => !existingWatchlistNames.has(s.name.toLowerCase())
    )

    return newSuggestions
  } catch (error) {
    getConfig().logger.error({ err: error }, 'Error in generateWatchlistSuggestions')
    return []
  }
}

```

## 📄 src/ai/index.js
*Lines: 136, Size: 5.31 KB*

```javascript
// packages/scraper-logic/src/ai/index.js (version 6.1.0)
import { getConfig } from '../config.js'
import { callLanguageModel } from '@headlines/ai-services'
import { AIAgent } from './AIAgent.js'
import { assessArticleContent } from './agents/articleAgent.js'
import { articleAssessmentSchema } from './schemas/articleAssessmentSchema.js'
import { preAssessArticle } from './agents/articlePreAssessmentAgent.js'
import { articlePreAssessmentSchema } from './schemas/articlePreAssessmentSchema.js'
import { clusterArticlesIntoEvents } from './agents/clusteringAgent.js'
import { clusterSchema } from './schemas/clusterSchema.js'
import { resolveVagueContact, findContactDetails } from './agents/contactAgent.js'
import { enrichContactSchema } from './schemas/enrichContactSchema.js'
import { findContactSchema } from './schemas/findContactSchema.js'
import {
  generateEmailSubjectLine,
  generatePersonalizedIntro,
} from './agents/emailAgents.js'
import { emailSubjectSchema } from './schemas/emailSubjectSchema.js'
import { emailIntroSchema } from './schemas/emailIntroSchema.js'
import {
  extractEntities,
  entityCanonicalizerAgent as getEntityCanonicalizerAgent,
} from './agents/entityAgent.js'
import { entitySchema } from './schemas/entitySchema.js'
import { canonicalizerSchema } from './schemas/canonicalizerSchema.js'
import { assessHeadlinesInBatches } from './agents/headlineAgent.js'
import { headlineAssessmentSchema } from './schemas/headlineAssessmentSchema.js'
import { judgePipelineOutput } from './agents/judgeAgent.js'
import { judgeSchema } from './schemas/judgeSchema.js'
import { generateOpportunitiesFromEvent } from './agents/opportunityAgent.js'
import { opportunitySchema } from './schemas/opportunitySchema.js'
import { suggestNewSelector } from './agents/selectorRepairAgent.js'
import { selectorRepairSchema } from './schemas/selectorRepairSchema.js'
import { synthesizeEvent, synthesizeFromHeadline } from './agents/synthesisAgent.js'
import { synthesisSchema } from './schemas/synthesisSchema.js'
import { generateWatchlistSuggestions } from './agents/watchlistAgent.js'
import { watchlistSuggestionSchema } from './schemas/watchlistSuggestionSchema.js'
import { disambiguationSchema } from './schemas/disambiguationSchema.js'
import { batchAssessArticles } from './agents/batchArticleAgent.js'
import { batchArticleAssessmentSchema } from './schemas/batchArticleAssessmentSchema.js'
import { classifyLinks as sectionClassifierAgent } from './agents/sectionClassifierAgent.js'
import { generateExecutiveSummary } from './agents/executiveSummaryAgent.js'

let isApiKeyInvalid = false
export async function performAiSanityCheck() {
  try {
    getConfig().logger.info('🔬 Performing AI service sanity check (OpenAI)...')
    const answer = await callLanguageModel({
      modelName: 'gpt-3.5-turbo', // Use a standard, widely available model for the check
      prompt: 'What is in one word the name of the capital of France',
      isJson: false,
    })
    if (
      answer &&
      typeof answer === 'string' &&
      answer.trim().toLowerCase().includes('paris')
    ) {
      getConfig().logger.info('✅ AI service sanity check passed.')
      return true
    } else {
      getConfig().logger.fatal(
        { details: { expected: 'paris', received: answer } },
        `OpenAI sanity check failed.`
      )
      return false
    }
  } catch (error) {
    if (error.status === 401 || error.message?.includes('Incorrect API key')) {
      getConfig().logger.fatal(`OpenAI sanity check failed due to INVALID API KEY (401).`)
    } else {
      getConfig().logger.fatal(
        { err: error },
        'OpenAI sanity check failed with an unexpected API error.'
      )
    }
    isApiKeyInvalid = true
    return false
  }
}
export async function checkModelPermissions(requiredModels) {
  getConfig().logger.info('🔬 Verifying permissions for configured OpenAI models...')
  try {
    // DEFINITIVE FIX: The OpenAI client for checking models is part of the ai-services package, not here.
    // This function is also not strictly necessary for the app to run, so we can simplify.
    // For now, we will assume permissions are correct if the sanity check passes.
    getConfig().logger.warn(
      'Model permission check is currently a no-op, relying on sanity check.'
    )
    return true
  } catch (error) {
    getConfig().logger.fatal({ err: error }, 'Failed to verify model permissions.')
    isApiKeyInvalid = true
    return false
  }
}
export {
  AIAgent,
  callLanguageModel,
  assessArticleContent,
  articleAssessmentSchema,
  preAssessArticle,
  articlePreAssessmentSchema,
  clusterArticlesIntoEvents,
  clusterSchema,
  resolveVagueContact,
  findContactDetails,
  enrichContactSchema,
  findContactSchema,
  generateEmailSubjectLine,
  generatePersonalizedIntro,
  emailSubjectSchema,
  emailIntroSchema,
  extractEntities,
  getEntityCanonicalizerAgent as entityCanonicalizerAgent,
  entitySchema,
  canonicalizerSchema,
  assessHeadlinesInBatches,
  headlineAssessmentSchema,
  judgePipelineOutput,
  judgeSchema,
  generateOpportunitiesFromEvent,
  opportunitySchema,
  suggestNewSelector,
  selectorRepairSchema,
  synthesizeEvent,
  synthesizeFromHeadline,
  synthesisSchema,
  generateWatchlistSuggestions,
  watchlistSuggestionSchema,
  disambiguationSchema,
  batchAssessArticles,
  batchArticleAssessmentSchema,
  sectionClassifierAgent,
  generateExecutiveSummary,
}

```

## 📄 src/ai/schemas/articleAssessmentSchema.js
*Lines: 30, Size: 862 Bytes*

```javascript
// packages/scraper-logic/src/ai/schemas/articleAssessmentSchema.js (version 1.1.0)
import { z } from 'zod'

export const articleAssessmentSchema = z.object({
  reasoning: z.object({
    event_type: z.string(),
    is_liquidity_event: z.boolean(),
    beneficiary: z.string(),
  }),
  relevance_article: z
    .number()
    .min(0)
    .max(100)
    .describe('The relevance score of the article content.'),
  assessment_article: z
    .string()
    .min(1)
    .describe('A single, concise sentence assessing the article.'),
  amount: z.number().nullable().optional(),
  key_individuals: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      // ROBUSTNESS FIX: Allow company to be null, as the AI sometimes cannot determine it.
      company: z.string().nullable(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/ai/schemas/articlePreAssessmentSchema.js
*Lines: 7, Size: 213 Bytes*

```javascript
// src/modules/ai/schemas/articlePreAssessmentSchema.js (version 1.0)
import { z } from 'zod'

export const articlePreAssessmentSchema = z.object({
  classification: z.enum(['private', 'public', 'corporate']),
})

```

## 📄 src/ai/schemas/batchArticleAssessmentSchema.js
*Lines: 8, Size: 290 Bytes*

```javascript
// packages/scraper-logic/src/ai/schemas/batchArticleAssessmentSchema.js (version 1.0)
import { z } from 'zod'
import { articleAssessmentSchema } from './articleAssessmentSchema.js'

export const batchArticleAssessmentSchema = z.object({
  assessments: z.array(articleAssessmentSchema),
})

```

## 📄 src/ai/schemas/canonicalizerSchema.js
*Lines: 7, Size: 178 Bytes*

```javascript
// src/modules/ai/schemas/canonicalizerSchema.js (version 1.0)
import { z } from 'zod'

export const canonicalizerSchema = z.object({
  canonical_name: z.string().nullable(),
})

```

## 📄 src/ai/schemas/clusterSchema.js
*Lines: 12, Size: 240 Bytes*

```javascript
// src/modules/ai/schemas/clusterSchema.js (version 1.0)
import { z } from 'zod'

export const clusterSchema = z.object({
  events: z.array(
    z.object({
      event_key: z.string(),
      article_ids: z.array(z.string()),
    })
  ),
})

```

## 📄 src/ai/schemas/disambiguationSchema.js
*Lines: 7, Size: 176 Bytes*

```javascript
// src/modules/ai/schemas/disambiguationSchema.js (version 1.0)
import { z } from 'zod'

export const disambiguationSchema = z.object({
  best_title: z.string().nullable(),
})

```

## 📄 src/ai/schemas/emailIntroSchema.js
*Lines: 7, Size: 164 Bytes*

```javascript
// src/modules/ai/schemas/emailIntroSchema.js (version 1.0)
import { z } from 'zod'

export const emailIntroSchema = z.object({
  intro_text: z.string().min(1),
})

```

## 📄 src/ai/schemas/emailSubjectSchema.js
*Lines: 7, Size: 174 Bytes*

```javascript
// src/modules/ai/schemas/emailSubjectSchema.js (version 1.0)
import { z } from 'zod'

export const emailSubjectSchema = z.object({
  subject_headline: z.string().min(1),
})

```

## 📄 src/ai/schemas/enrichContactSchema.js
*Lines: 14, Size: 325 Bytes*

```javascript
// src/modules/ai/schemas/enrichContactSchema.js (version 1.0)
import { z } from 'zod'

export const enrichContactSchema = z.object({
  enriched_contacts: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      company: z.string(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/ai/schemas/entitySchema.js
*Lines: 8, Size: 181 Bytes*

```javascript
// src/modules/ai/schemas/entitySchema.js (version 1.0)
import { z } from 'zod'

export const entitySchema = z.object({
  reasoning: z.string(),
  entities: z.array(z.string()),
})

```

## 📄 src/ai/schemas/findContactSchema.js
*Lines: 7, Size: 173 Bytes*

```javascript
// src/modules/ai/schemas/findContactSchema.js (version 1.0)
import { z } from 'zod'

export const findContactSchema = z.object({
  email: z.string().email().nullable(),
})

```

## 📄 src/ai/schemas/headlineAssessmentSchema.js
*Lines: 13, Size: 352 Bytes*

```javascript
// src/modules/ai/schemas/headlineAssessmentSchema.js (version 1.0)
import { z } from 'zod'

const singleAssessmentSchema = z.object({
  headline_en: z.string(),
  relevance_headline: z.number().min(0).max(100),
  assessment_headline: z.string(),
})

export const headlineAssessmentSchema = z.object({
  assessment: z.array(singleAssessmentSchema),
})

```

## 📄 src/ai/schemas/judgeSchema.js
*Lines: 14, Size: 394 Bytes*

```javascript
// src/modules/ai/schemas/judgeSchema.js (version 1.0)
import { z } from 'zod'

const verdictSchema = z.object({
  identifier: z.string(),
  quality: z.enum(['Excellent', 'Good', 'Acceptable', 'Marginal', 'Poor', 'Irrelevant']),
  commentary: z.string(),
})

export const judgeSchema = z.object({
  event_judgements: z.array(verdictSchema),
  opportunity_judgements: z.array(verdictSchema),
})

```

## 📄 src/ai/schemas/opportunitySchema.js
*Lines: 19, Size: 490 Bytes*

```javascript
// src/modules/ai/schemas/opportunitySchema.js (version 1.0)
import { z } from 'zod'

export const opportunitySchema = z.object({
  opportunities: z.array(
    z.object({
      reachOutTo: z.string(),
      contactDetails: z.object({
        email: z.string().email().nullable(),
        role: z.string().nullable(),
        company: z.string().nullable(),
      }),
      basedIn: z.string(),
      whyContact: z.string(),
      likelyMMDollarWealth: z.number().nullable(),
    })
  ),
})

```

## 📄 src/ai/schemas/sectionClassifierSchema.js
*Lines: 17, Size: 443 Bytes*

```javascript
// packages/scraper-logic/src/ai/schemas/sectionClassifierSchema.js (version 1.0)
import { z } from 'zod';

export const sectionClassifierSchema = z.object({
  classifications: z.array(
    z.object({
      classification: z.enum([
        "news_section", 
        "article_headline", 
        "navigation", 
        "other"
      ]),
      reasoning: z.string().describe("A brief explanation for the classification choice."),
    })
  ),
});

```

## 📄 src/ai/schemas/selectorRepairSchema.js
*Lines: 13, Size: 405 Bytes*

```javascript
// src/modules/ai/schemas/selectorRepairSchema.js (version 1.0)
import { z } from 'zod'

export const selectorRepairSchema = z.object({
  reasoning: z.string(),
  suggested_selectors: z.object({
    headlineSelector: z.string().optional(),
    linkSelector: z.string().optional().nullable(),
    headlineTextSelector: z.string().optional().nullable(),
    articleSelector: z.string().optional(),
  }),
})

```

## 📄 src/ai/schemas/sourceNameSchema.js
*Lines: 7, Size: 213 Bytes*

```javascript
// src/modules/ai/schemas/sourceNameSchema.js (version 1.0)
import { z } from 'zod'

export const sourceNameSchema = z.object({
  name: z.string().min(1).describe('The official name of the news publication.'),
})

```

## 📄 src/ai/schemas/synthesisSchema.js
*Lines: 18, Size: 543 Bytes*

```javascript
// packages/scraper-logic/src/ai/schemas/synthesisSchema.js (version 1.1.0)
import { z } from 'zod'

export const synthesisSchema = z.object({
  headline: z.string().min(1),
  summary: z.string().min(1),
  country: z.string().min(1),
  key_individuals: z.array(
    z.object({
      name: z.string(),
      role_in_event: z.string(),
      // ROBUSTNESS FIX: Allow company to be null, as the AI sometimes cannot determine it during synthesis.
      company: z.string().nullable(),
      email_suggestion: z.string().nullable(),
    })
  ),
})

```

## 📄 src/ai/schemas/watchlistSuggestionSchema.js
*Lines: 15, Size: 364 Bytes*

```javascript
// src/modules/ai/schemas/watchlistSuggestionSchema.js (version 1.0)
import { z } from 'zod'

export const watchlistSuggestionSchema = z.object({
  suggestions: z.array(
    z.object({
      name: z.string(),
      type: z.enum(['person', 'family', 'company']),
      country: z.string(),
      rationale: z.string(),
      sourceEvent: z.string(),
    })
  ),
})

```

## 📄 src/browser.js
*Lines: 208, Size: 6.38 KB*

```javascript
// packages/scraper-logic/src/browser.js (version 4.1.0)
import playwright from 'playwright'
import fs from 'fs/promises'
import path from 'path'
import { getConfig } from './config.js'

const BROWSER_HEADERS = {
  Accept:
    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9,nl-NL;q=0.8,nl;q=0.7',
  'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}

const CONSENT_SELECTORS = [
  'button:has-text("Accepteer alles")',
  'button:has-text("Alles accepteren")',
  'button:has-text("Toestemming geven")',
  'button:has-text("Akkoord")',
  'button:has-text("Accept all")',
  'button:has-text("Accept All")',
  'button:has-text("I accept")',
  'button:has-text("Accept")',
  'button:has-text("Godkend alle")',
  'button:has-text("Tillad alle")',
]

async function ensureDebugDirectory() {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) {
    getConfig().logger.warn('Debug HTML directory not configured. Saving disabled.')
    return null
  }
  try {
    await fs.mkdir(debugDir, { recursive: true })
    return debugDir
  } catch (error) {
    getConfig().logger.warn('Failed to create debug directory: ' + error.message)
    return null
  }
}

async function saveDebugHtml(page, caller, prefix, url) {
  const debugDir = await ensureDebugDirectory()
  if (!debugDir) return null
  try {
    const html = await page.content()
    const urlPart = new URL(url).hostname.replace(/[^a-z0-9]/gi, '_')
    const filename = prefix + '_' + caller + '_' + urlPart + '.html'
    const filePath = path.join(debugDir, filename)
    await fs.writeFile(filePath, html)
    getConfig().logger.warn('[Playwright:' + caller + '] Saved debug HTML to ' + filePath)
    return filePath
  } catch (error) {
    getConfig().logger.error(
      '[Playwright:' + caller + '] Failed to save debug HTML: ' + error.message
    )
    return null
  }
}

async function handleConsent(page, caller) {
  for (const selector of CONSENT_SELECTORS) {
    try {
      const button = page.locator(selector).first()
      if (await button.isVisible({ timeout: 1500 })) {
        await button.click({ timeout: 2000 })
        getConfig().logger.info(
          '[Playwright:' +
            caller +
            '] Clicked consent button with selector: "' +
            selector +
            '"'
        )
        await page.waitForTimeout(1500)
        return true
      }
    } catch (e) {
      // Selector not found, continue
    }
  }
  getConfig().logger.trace(
    '[Playwright:' + caller + '] No actionable consent modal found.'
  )
  return false
}

export async function fetchPageWithPlaywright(url, caller = 'Unknown', options = {}) {
  const { timeout = 60000, waitForSelector } = options

  let browser = null
  let page = null
  try {
    getConfig().logger.trace(
      '[Playwright:' +
        caller +
        '] Launching browser for: ' +
        url +
        ' (Timeout: ' +
        timeout +
        'ms)'
    )
    browser = await playwright.chromium.launch({
      headless: true,
      args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage'],
    })
    const context = await browser.newContext({
      userAgent: BROWSER_HEADERS['User-Agent'],
      extraHTTPHeaders: BROWSER_HEADERS,
      viewport: { width: 1920, height: 1080 },
    })
    page = await context.newPage()

    await page.goto(url, { waitUntil: 'domcontentloaded', timeout })

    await handleConsent(page, caller)

    if (waitForSelector) {
      getConfig().logger.info(
        '[Playwright:' + caller + '] Waiting for selector "' + waitForSelector + '"...'
      )
      await page.waitForSelector(waitForSelector, { timeout: timeout - 5000 })
      getConfig().logger.info(
        '[Playwright:' + caller + '] Selector found. Page is ready.'
      )
    } else {
      await page
        .waitForLoadState('networkidle', { timeout: 5000 })
        .catch(() =>
          getConfig().logger.trace(
            '[Playwright:' + caller + '] Network idle timeout reached, proceeding anyway.'
          )
        )
    }

    return await page.content()
  } catch (error) {
    // ENHANCED ERROR LOGGING: Provide more specific reasons for failures.
    let reason = error.message.split('\n')[0]
    if (error.message.includes('net::ERR')) {
      reason = `Network Error: ${reason}`
    } else if (error.name === 'TimeoutError') {
      reason = `Timeout after ${timeout / 1000}s. The page may be too slow or blocked.`
    } else if (page) {
      const pageContent = await page.content()
      if (pageContent.includes('captcha') || pageContent.includes('challenge-platform')) {
        reason = 'Potential CAPTCHA or bot detection wall encountered.'
      }
    }

    getConfig().logger.error(
      '[Playwright:' +
        caller +
        '] Critical failure during fetch for ' +
        url +
        ': ' +
        reason
    )
    if (page) {
      await saveDebugHtml(page, caller, 'CRITICAL_FAIL', url)
    }
    return null
  } finally {
    if (browser) {
      await browser.close()
      getConfig().logger.trace('[Playwright:' + caller + '] Browser closed for: ' + url)
    }
  }
}

export async function fetchPageContentFromPopup(pageUrl, buttonSelector) {
  let browser = null
  try {
    browser = await playwright.chromium.launch({ headless: true })
    const context = await browser.newContext({ userAgent: BROWSER_HEADERS['User-Agent'] })
    const page = await context.newPage()
    await page.goto(pageUrl, { waitUntil: 'networkidle' })
    await handleConsent(page, 'PopupFetcher')

    await page.waitForSelector(buttonSelector, { timeout: 10000 })
    const button = page.locator(buttonSelector).first()
    await button.click()

    // Wait for the popup overlay to become visible
    await page.waitForSelector('.popup-overlay--opened', {
      state: 'visible',
      timeout: 5000,
    })

    // Extract the HTML of the now-visible popup
    const popupElement = await page.locator('.popup__box')
    const popupHtml = await popupElement.innerHTML()
    return popupHtml
  } catch (error) {
    getConfig().logger.error(
      { err: error, url: pageUrl, selector: buttonSelector },
      'Failed to fetch content from popup.'
    )
    return null
  } finally {
    if (browser) {
      await browser.close()
    }
  }
}

```

## 📄 src/config.js
*Lines: 23, Size: 567 Bytes*

```javascript
// packages/scraper-logic/src/config.js (version 1.0.0)
// This module holds the shared configuration for the scraper logic,
// which will be injected by the consuming application (pipeline or admin).

let _config = {
  // A simple console logger as a fallback.
  logger: console,
  paths: {
    debugHtmlDir: null,
  },
  configStore: null,
  utilityFunctions: null,
};

export function configure(appConfig) {
  // Merge the provided app config with the existing config.
  _config = { ..._config, ...appConfig };
}

export function getConfig() {
  return _config;
}

```

## 📄 src/index.js
*Lines: 5, Size: 305 Bytes*

```javascript
// packages/scraper-logic/src/index.js (version 1.0)
// This file will serve as the public API for our shared scraper logic package.
// For now, we will leave it empty as direct imports from subdirectories are clear enough.
// In the future, we can export key functions from here for a cleaner interface.

```

## 📄 src/push/client.js
*Lines: 26, Size: 929 Bytes*

```javascript
// packages/scraper-logic/src/push/client.js (version 2.0.0)
import webpush from 'web-push'
import { getConfig } from '../config.js';
import { env } from '@headlines/config'

let isPushConfigured = false

function configurePush() {
    if (isPushConfigured) return;

    const { VAPID_SUBJECT, VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY } = env
    if (VAPID_PUBLIC_KEY && VAPID_PRIVATE_KEY && VAPID_SUBJECT) {
      try {
        webpush.setVapidDetails(VAPID_SUBJECT, VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY)
        isPushConfigured = true
        getConfig().logger.info('✅ Centralized push notification service (VAPID) configured.')
      } catch (error) {
        getConfig().logger.error({ err: error }, '❌ Failed to configure VAPID details.')
      }
    } else {
      getConfig().logger.warn('VAPID keys not fully configured. Push notifications will be disabled.')
    }
}

export { webpush, isPushConfigured, configurePush }

```

## 📄 src/scraper/constants.js
*Lines: 10, Size: 454 Bytes*

```javascript
// src/modules/scraper/constants.js (version 1.0)
export const BROWSER_HEADERS = {
  Accept:
    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
  'Accept-Encoding': 'gzip, deflate, br',
  'Accept-Language': 'en-US,en;q=0.9',
  'User-Agent':
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
}

```

## 📄 src/scraper/contentScraper.js
*Lines: 102, Size: 3.52 KB*

```javascript
// packages/scraper-logic/src/scraper/contentScraper.js (version 4.5.0)
import * as cheerio from 'cheerio'
import fs from 'fs/promises'
import path from 'path'
import { JSDOM, VirtualConsole } from 'jsdom'
import { Readability } from '@mozilla/readability'

import { getConfig } from '../config.js'
import { fetchPageWithPlaywright } from '../browser.js'
import { contentExtractorRegistry } from './extractors/index.js'

async function saveDebugHtml(filename, html) {
  const config = getConfig()
  const debugDir = config.paths?.debugHtmlDir
  if (!debugDir) return null
  try {
    await fs.mkdir(debugDir, { recursive: true })
    const filePath = path.join(debugDir, filename)
    await fs.writeFile(filePath, html)
    return filePath
  } catch (error) {
    getConfig().logger.error({ err: error, file: filename }, 'Failed to save debug HTML.')
    return null
  }
}

function extractWithReadability(url, html) {
  try {
    const virtualConsole = new VirtualConsole();
    virtualConsole.on('cssParseError', () => {}); 

    const doc = new JSDOM(html, { url, virtualConsole });
    const reader = new Readability(doc.window.document);
    const article = reader.parse();
    return article?.textContent || null;
  } catch (e) {
    getConfig().logger.warn({ err: e, url }, 'Readability.js failed to parse article.');
    return null;
  }
}

export async function scrapeArticleContent(article, source) {
  if (source.extractionMethod === 'custom' && contentExtractorRegistry[source.extractorKey]) {
    const customContentExtractor = contentExtractorRegistry[source.extractorKey];
    return await customContentExtractor(article, source);
  }

  if (article.rssContent && article.rssContent.length >= getConfig().settings.MIN_ARTICLE_CHARS) {
    article.articleContent = { contents: [article.rssContent] }
    return article
  }

  const html = await fetchPageWithPlaywright(article.link, 'ContentScraper')
  if (!html) {
    return { ...article, enrichment_error: 'Playwright failed to fetch page HTML' }
  }

  let contentText = extractWithReadability(article.link, html)
  let extractionMethod = 'Readability.js'

  if (!contentText || contentText.length < getConfig().settings.MIN_ARTICLE_CHARS) {
    const $ = cheerio.load(html)
    const selectors = Array.isArray(source.articleSelector)
      ? source.articleSelector
      : [source.articleSelector].filter(Boolean)
    if (selectors.length > 0) {
      const contentParts = []
      for (const selector of selectors) {
        const extractedText = $(selector).text().replace(/\s+/g, ' ').trim()
        if (extractedText) contentParts.push(extractedText)
      }
      if (contentParts.length > 0) {
        contentText = contentParts.join('\n\n')
        extractionMethod = 'Cheerio Selector'
      }
    }
  }

  if (contentText && contentText.length >= getConfig().settings.MIN_ARTICLE_CHARS) {
    article.articleContent = { contents: [contentText] }
    getConfig().logger.trace(
      {
        article: {
          headline: article.headline,
          chars: contentText.length,
          method: extractionMethod,
        },
      },
      `✅ Content enrichment successful.`
    )
  } else {
    const reason = !contentText
      ? `All extraction methods failed.`
      : `Content too short (` + (contentText ? contentText.length : 0) + ` chars).`
    article.enrichment_error = reason
    article.contentPreview = contentText ? contentText.substring(0, 100) : ''
    const filename = source.name.replace(/[^a-z0-9]/gi, '_').toLowerCase() + '_content_fail.html'
    await saveDebugHtml(filename, html)
  }
  return article
}

```

## 📄 src/scraper/dynamicExtractor.js
*Lines: 51, Size: 1.83 KB*

```javascript
// packages/scraper-logic/src/scraper/dynamicExtractor.js (version 2.0.1)
import { getConfig } from '../config.js';

/**
 * A generic, data-driven extractor that uses declarative fields from a Source document
 * to extract headline and link information from a Cheerio element.
 * @param {import('cheerio').CheerioAPI} $ - The Cheerio instance.
 * @param {import('cheerio').Element} el - The current DOM element matching the headlineSelector.
 * @param {object} source - The full Source document from the database.
 * @returns {{headline: string, link: string}|null} The extracted article data or null if invalid.
 */
export function dynamicExtractor($, el, source) {
  try {
    const mainElement = $(el)

    // 1. Find the link element and extract the href.
    // If linkSelector is null, the mainElement itself is the link.
    const linkElement = source.linkSelector
      ? mainElement.find(source.linkSelector).first()
      : mainElement
    const link = linkElement.attr('href')

    if (!link) {
      return null // A link is mandatory
    }

    // 2. Find the headline text element and extract the text.
    // If headlineTextSelector is null, the mainElement contains the text.
    const textElement = source.headlineTextSelector
      ? mainElement.find(source.headlineTextSelector).first()
      : mainElement

    // 3. Extract the text and clean it by removing any nested HTML tags.
    let headline = textElement.text().trim().replace(/\s+/g, ' ')

    if (!headline) {
      return null // A headline is mandatory
    }

    // 4. Apply the headline template if it exists
    if (source.headlineTemplate) {
      headline = source.headlineTemplate.replace('{{TEXT}}', headline)
    }

    return { headline, link }
  } catch (error) {
    getConfig().logger.error({ err: error, source: source.name }, 'Error during dynamic extraction.')
    return null
  }
}

```

## 📄 src/scraper/extractor.js.bak
*Lines: 122, Size: 3.76 KB*

```
// packages/scraper-logic/src/scraper/extractor.js (version 3.3.0)
import * as cheerio from 'cheerio'
import { fetchPageWithPlaywright } from '../browser.js'

const cvcHeadlineExtractor = ($, el, site) => {
  const element = $(el);
  if (element.hasClass('portfolio__card-holder--spotlight')) {
    return null;
  }
  const headingElement = element.find('h2.portfolio__card-heading');
  const companyName = headingElement.text().trim();
  const button = element.find('button.js-portfolio-card');
  
  if (companyName && button.length) {
    return { 
        headline: 'CVC Portfolio Company: ' + companyName, 
        link: site.sectionUrl, 
        source: site.name, 
        newspaper: site.name,
        customData: { dataKey: button.attr('data-key') } 
    };
  }
  return null;
};

async function cvcContentExtractor(article, source) {
    const { getConfig } = await import('../config.js');
    const { fetchPageContentFromPopup } = await import('../browser.js');
    const cheerio = await import('cheerio');

    if (!article.customData?.dataKey) {
        return { ...article, enrichment_error: 'Missing data-key for popup interaction.' };
    }

    const buttonSelector = 'button[data-key="' + article.customData.dataKey + '"]';
    const popupHtml = await fetchPageContentFromPopup(source.sectionUrl, buttonSelector);

    if (!popupHtml) {
        return { ...article, enrichment_error: 'Failed to fetch popup HTML for content.' };
    }

    const $ = cheerio.load(popupHtml);
    const content = $('.rte').text().trim().replace(/\s+/g, ' ');

    if (content) {
        article.articleContent = { contents: [content] };
        getConfig().logger.trace({ article: { headline: article.headline } }, '✅ CVC custom content extraction successful.');
    } else {
        article.enrichment_error = 'Could not find content in the CVC popup.';
    }

    return article;
}

const simpleExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.text().trim().replace(/\s+/g, ' ')
  const link = element.attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

const finansDkExtractor = ($, el, site) => ({
  headline: $(el).text().trim(),
  link: $(el).closest('a').attr('href'),
  source: site.name,
  newspaper: site.name,
})

const politikenExtractor = ($, el, site) => {
  const element = $(el);
  const h = element.find('h2, h3, h4').first().text().trim()
  const a = element.find('a[href*="/art"]').first().attr('href')
  return h && a ? { headline: h, link: a, source: site.name, newspaper: site.name } : null
}

const verdane = ($, el, site) => {
  const element = $(el);
  const linkEl = element.find('a.wp-block-klingit-the-product-block-link')
  const companyName = linkEl.find('h3.wp-block-post-title').text().trim()
  if (companyName) {
    return {
      headline: 'Verdane invests in ' + companyName,
      link: linkEl.attr('href'),
      source: site.name,
      newspaper: site.name,
    }
  }
  return null
}

const jyllandsPostenHeadlineExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h3').text().trim()
  const link = element.find('a').attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

export const extractorRegistry = {
  cvc_portfolio: cvcHeadlineExtractor,
  okonomisk_ugebrev: simpleExtractor,
  gro_capital: simpleExtractor,
  eifo_dk: simpleExtractor,
  clearwater_dk: simpleExtractor,
  finans_dk: finansDkExtractor,
  politiken: politikenExtractor,
  e24: simpleExtractor,
  verdane: verdane,
  quotenet_nl: simpleExtractor,
  jyllands_posten: jyllandsPostenHeadlineExtractor,
  simple: simpleExtractor,
}

export const contentExtractorRegistry = {
  cvc_portfolio: cvcContentExtractor,
}

```

## 📄 src/scraper/extractors/index.js
*Lines: 45, Size: 1.66 KB*

```javascript
// packages/scraper-logic/src/scraper/extractors/index.js (version 2.0.0)
// This file uses static imports to be compatible with both Node.js (pipeline) and Webpack (Next.js apps).

// Reusable Extractors
import { simpleExtractor } from './reusable/simple.js';

// Source-Specific Headline Extractors
import { cvcPortfolioExtractor } from './source-specific/cvcPortfolio.js';
import { finansDkExtractor } from './source-specific/finansDk.js';
import { jyllandsPostenExtractor } from './source-specific/jyllandsPosten.js';
import { okonomiskUgebrevExtractor } from './source-specific/okonomiskUgebrev.js';
import { politikenExtractor } from './source-specific/politiken.js';
import { verdaneExtractor } from './source-specific/verdane.js';

// Source-Specific Content Extractors
import { cvcPortfolioContentExtractor } from './source-specific/cvcPortfolioContent.js';

// --- Build Registries ---

export const extractorRegistry = {
  // Reusable
  simple: simpleExtractor,

  // Source-specific
  cvc_portfolio: cvcPortfolioExtractor,
  finans_dk: finansDkExtractor,
  jyllands_posten: jyllandsPostenExtractor,
  okonomisk_ugebrev: okonomiskUgebrevExtractor,
  politiken: politikenExtractor,
  verdane: verdaneExtractor,

  // Manual mapping for legacy keys
  gro_capital: simpleExtractor,
  eifo_dk: simpleExtractor,
  clearwater_dk: simpleExtractor,
  e24: simpleExtractor,
  quotenet_nl: simpleExtractor,
};

export const contentExtractorRegistry = {
  cvc_portfolio_content: cvcPortfolioContentExtractor,
};

console.log(`[Extractor Registry] Statically loaded ${Object.keys(extractorRegistry).length} headline extractors and ${Object.keys(contentExtractorRegistry).length} content extractors.`);

```

## 📄 src/scraper/extractors/reusable/simple.js
*Lines: 11, Size: 374 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/reusable/simple.js (version 1.0.0)
export const simpleExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.text().trim().replace(/\s+/g, ' ')
  const link = element.attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

```

## 📄 src/scraper/extractors/source-specific/cvcPortfolio.js
*Lines: 22, Size: 734 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/cvcPortfolio.js (version 1.0.0)
export const cvcPortfolioExtractor = ($, el, site) => {
  const element = $(el);
  if (element.hasClass('portfolio__card-holder--spotlight')) {
    return null;
  }
  const headingElement = element.find('h2.portfolio__card-heading');
  const companyName = headingElement.text().trim();
  const button = element.find('button.js-portfolio-card');
  
  if (companyName && button.length) {
    return { 
        headline: 'CVC Portfolio Company: ' + companyName, 
        link: site.sectionUrl, 
        source: site.name, 
        newspaper: site.name,
        customData: { dataKey: button.attr('data-key') } 
    };
  }
  return null;
};

```

## 📄 src/scraper/extractors/source-specific/cvcPortfolioContent.js
*Lines: 30, Size: 1.17 KB*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/cvcPortfolioContent.js (version 1.0.0)
import { fetchPageContentFromPopup } from '../../../browser.js';
import { getConfig } from '../../../config.js';
import * as cheerio from 'cheerio';

export const cvcPortfolioContentExtractor = async (article, source) => {
    if (!article.customData?.dataKey) {
        return { ...article, enrichment_error: 'Missing data-key for popup interaction.' };
    }

    const buttonSelector = 'button[data-key="' + article.customData.dataKey + '"]';
    const popupHtml = await fetchPageContentFromPopup(source.sectionUrl, buttonSelector);

    if (!popupHtml) {
        return { ...article, enrichment_error: 'Failed to fetch popup HTML for content.' };
    }

    const $ = cheerio.load(popupHtml);
    const content = $('.rte').text().trim().replace(/\s+/g, ' ');

    if (content) {
        article.articleContent = { contents: [content] };
        getConfig().logger.trace({ article: { headline: article.headline } }, '✅ CVC custom content extraction successful.');
    } else {
        article.enrichment_error = 'Could not find content in the CVC popup.';
    }

    return article;
}

```

## 📄 src/scraper/extractors/source-specific/finansDk.js
*Lines: 8, Size: 268 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/finansDk.js (version 1.0.0)
export const finansDkExtractor = ($, el, site) => ({
  headline: $(el).text().trim(),
  link: $(el).closest('a').attr('href'),
  source: site.name,
  newspaper: site.name,
})

```

## 📄 src/scraper/extractors/source-specific/jyllandsPosten.js
*Lines: 11, Size: 397 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/jyllandsPosten.js (version 1.0.0)
export const jyllandsPostenExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h3').text().trim()
  const link = element.find('a').attr('href')
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name }
  }
  return null
}

```

## 📄 src/scraper/extractors/source-specific/okonomiskUgebrev.js
*Lines: 12, Size: 444 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/okonomiskUgebrev.js (version 1.0.0)
export const okonomiskUgebrevExtractor = ($, el, site) => {
  const element = $(el);
  const headline = element.find('h5.elementor-heading-title').text().trim().replace(/\s+/g, ' ');
  const link = element.attr('href');
  
  if (headline && link) {
    return { headline, link, source: site.name, newspaper: site.name };
  }
  return null;
};

```

## 📄 src/scraper/extractors/source-specific/politiken.js
*Lines: 8, Size: 391 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/politiken.js (version 1.0.0)
export const politikenExtractor = ($, el, site) => {
  const element = $(el);
  const h = element.find('h2, h3, h4').first().text().trim()
  const a = element.find('a[href*="/art"]').first().attr('href')
  return h && a ? { headline: h, link: a, source: site.name, newspaper: site.name } : null
}

```

## 📄 src/scraper/extractors/source-specific/verdane.js
*Lines: 16, Size: 516 Bytes*

```javascript
// packages/scraper-logic/src/scraper/extractors/source-specific/verdane.js (version 1.0.0)
export const verdaneExtractor = ($, el, site) => {
  const element = $(el);
  const linkEl = element.find('a.wp-block-klingit-the-product-block-link')
  const companyName = linkEl.find('h3.wp-block-post-title').text().trim()
  if (companyName) {
    return {
      headline: 'Verdane invests in ' + companyName,
      link: linkEl.attr('href'),
      source: site.name,
      newspaper: site.name,
    }
  }
  return null
}

```

## 📄 src/scraper/headlineScraper.js
*Lines: 158, Size: 6.14 KB*

```javascript
// packages/scraper-logic/src/scraper/headlineScraper.js (version 5.2.0)
import * as cheerio from 'cheerio'
import axios from 'axios'
import fs from 'fs/promises'
import path from 'path'
import Parser from 'rss-parser'
import { Source } from '@headlines/models'
import { BROWSER_HEADERS } from './constants.js'
import { fetchPageWithPlaywright } from '../browser.js'
import { getConfig } from '../config.js'
import { extractorRegistry } from './extractors/index.js'
import { dynamicExtractor } from './dynamicExtractor.js'

const rssParser = new Parser({
  customFields: {
    item: [['content:encoded', 'contentEncoded']],
  },
})

async function saveDebugHtml(filename, html) {
  const config = getConfig();
  const DEBUG_HTML_DIR = config.paths?.debugHtmlDir;
  if (!DEBUG_HTML_DIR) return;
  try {
    await fs.mkdir(DEBUG_HTML_DIR, { recursive: true })
    const filePath = path.join(DEBUG_HTML_DIR, filename)
    await fs.writeFile(filePath, html)
  } catch (error) {
    getConfig().logger.error({ err: error, file: filename }, 'Failed to save debug HTML.')
  }
}

async function fetchHeadlinesViaRss(source) {
  try {
    const feed = await rssParser.parseURL(source.rssUrl);
    if (!feed.items || feed.items.length === 0) {
      throw new Error('RSS feed was empty or invalid.');
    }
    const articles = feed.items
      .map((item) => {
        const rssContentHtml = item.contentEncoded || item.contentSnippet || item.content || '';
        const rssContent = cheerio.load(rssContentHtml).text().replace(/\s+/g, ' ').trim();
        return { headline: item.title?.trim(), link: item.link, rssContent: rssContent || null };
      })
      .filter((item) => item.headline && item.link);
    return { articles, error: null };
  } catch (error) {
    const failureReason = error.message || 'Unknown RSS error.';
    getConfig().logger.warn({ url: source.rssUrl, reason: failureReason }, `[RSS] Feed parsing failed for "${source.name}". Auto-disabling.`);
    
    try {
      await Source.updateOne(
        { _id: source._id },
        { 
          $set: { 
            rssUrl: null,
            notes: `${source.notes || ''}\n[${new Date().toISOString()}] RSS URL disabled due to error: ${failureReason}`.trim(),
          } 
        }
      );
    } catch (dbError) {
      getConfig().logger.error({ err: dbError }, 'Failed to auto-disable RSS URL in database.');
    }
    return { articles: [], error: failureReason };
  }
}

async function fetchPageStatic(url) {
  try {
    const { data } = await axios.get(url, { headers: BROWSER_HEADERS, timeout: 25000 })
    return { html: data, error: null }
  } catch (error) {
    getConfig().logger.warn({ url, err: { message: error.message } }, `[Axios] Static fetch failed`)
    return { html: null, error: error.message }
  }
}

async function fetchWithPlaywrightWrapped(source) {
  const html = await fetchPageWithPlaywright(source.sectionUrl, 'HeadlineScraper', {
    timeout: source.playwrightTimeoutMs,
    waitForSelector: source.waitForSelector,
  })
  if (html) {
    return { html, error: null }
  }
  return {
    html: null,
    error: `Playwright failed to fetch content from ${source.sectionUrl}`,
  }
}

export async function scrapeSiteForHeadlines(source) {
  if (source.rssUrl) {
    getConfig().logger.info(`[Scraping] Attempting RSS scrape for "${source.name}"...`);
    const rssResult = await fetchHeadlinesViaRss(source);
    if (rssResult.articles.length > 0) {
      return { articles: rssResult.articles, success: true, resultCount: rssResult.articles.length, error: null };
    }
    getConfig().logger.warn(`RSS scrape failed for "${source.name}". Falling back to HTML scraping.`);
  }

  const fetcher = source.isStatic ? () => fetchPageStatic(source.sectionUrl) : () => fetchWithPlaywrightWrapped(source);
  const fetcherName = source.isStatic ? 'STATIC (fast)' : 'PLAYWRIGHT (full-browser)';
  getConfig().logger.info(`[Scraping] Initiating HTML scrape for "${source.name}" using ${fetcherName}...`);

  const { html, error } = await fetcher();
  if (!html) return { articles: [], success: false, error, debugHtml: null };

  const $ = cheerio.load(html);
  const articles = [];
  
  if (source.extractionMethod === 'json-ld') {
      $('script[type="application/ld+json"]').each((_, el) => {
        try {
          const jsonData = JSON.parse($(el).html())
          const potentialLists = [jsonData, ...(jsonData['@graph'] || [])]
          potentialLists.forEach((list) => {
            const items = list?.itemListElement
            if (items && Array.isArray(items)) {
              items.forEach((item) => {
                const headline = item.name || item.item?.name
                const url = item.url || item.item?.url
                if (headline && url) {
                  articles.push({ headline: headline.trim(), link: new URL(url, source.baseUrl).href });
                }
              })
            }
          })
        } catch (e) { /* Ignore parsing errors */ }
      })
  } else { // 'declarative' or 'custom'
      const selectors = Array.isArray(source.headlineSelector) ? source.headlineSelector : [source.headlineSelector].filter(Boolean);
      const extractorFn = source.extractionMethod === 'custom' ? extractorRegistry[source.extractorKey] : dynamicExtractor;
      if (!extractorFn) {
          return { articles: [], success: false, error: `No valid extractor for method: ${source.extractionMethod}` };
      }
      for (const selector of selectors) {
        $(selector).each((_, el) => {
            const articleData = extractorFn($, el, source);
            if (articleData?.headline && articleData?.link) {
                articleData.link = new URL(articleData.link, source.baseUrl).href;
                articles.push(articleData);
            }
        });
      }
  }

  const uniqueArticles = Array.from(new Map(articles.map((a) => [a.link, a])).values());

  if (uniqueArticles.length === 0) {
    const filename = `${source.name.replace(/[^a-z0-9]/gi, '_').toLowerCase()}_headline_fail.html`;
    await saveDebugHtml(filename, html);
    return { articles: [], success: false, error: 'Extracted 0 headlines.', debugHtml: html };
  }

  return { articles: uniqueArticles, success: true, resultCount: uniqueArticles.length, error: null };
}

```

## 📄 src/scraper/index.js
*Lines: 12, Size: 389 Bytes*

```javascript
// packages/scraper-logic/src/scraper/index.js (version 1.0)
import { scrapeSiteForHeadlines } from './headlineScraper.js'
import { scrapeArticleContent } from './contentScraper.js'
import { testHeadlineExtraction, scrapeArticleContentForTest } from './test-helpers.js'

export {
  scrapeSiteForHeadlines,
  scrapeArticleContent,
  testHeadlineExtraction,
  scrapeArticleContentForTest,
}

```

## 📄 src/scraper/newsApiScraper.js
*Lines: 133, Size: 4.04 KB*

```javascript
// packages/scraper-logic/src/scraper/newsApiScraper.js (version 2.3.0)
import NewsAPI from 'newsapi'
import { getConfig } from '../config.js';
import { Source, WatchlistEntity } from '@headlines/models'
import { env } from '@headlines/config'
import colors from 'ansi-colors';

async function getWatchlist() {
  const [sources, richListTargets] = await Promise.all([
    Source.find({
      status: 'active',
      country: { $in: ['Denmark', 'Global PE', 'M&A Aggregators'] },
    })
      .select('name')
      .lean(),
    WatchlistEntity.find({ status: 'active' }).select('name').lean(),
  ])

  const sourceNames = sources.map((s) => s.name.split('(')[0].trim())
  const richListNames = richListTargets.map((t) => t.name.split('(')[0].trim())
  const watchlist = [...new Set([...sourceNames, ...richListNames])]
  getConfig().logger.trace({ details: watchlist }, 'Full NewsAPI watchlist keywords.')
  return watchlist
}

function buildQueryBatches(watchlist) {
  const MAX_QUERY_LENGTH = 490
  const queries = []
  let currentBatch = []

  for (const keyword of watchlist) {
    const sanitizedKeyword = keyword.replace(/&/g, ' ').replace(/[()]/g, '').trim()
    if (!sanitizedKeyword) continue
    const quotedKeyword = `"${sanitizedKeyword}"`
    const potentialQuery = [...currentBatch, quotedKeyword].join(' OR ')
    if (potentialQuery.length > MAX_QUERY_LENGTH) {
      if (currentBatch.length > 0) {
        queries.push(currentBatch.join(' OR '))
      }
      currentBatch = [quotedKeyword]
    } else {
      currentBatch.push(quotedKeyword)
    }
  }

  if (currentBatch.length > 0) {
    queries.push(currentBatch.join(' OR '))
  }

  const queriesToUse = queries.slice(0, 4);

  if (queries.length > 4) {
    getConfig().logger.warn(
      `[NewsAPI] Watchlist generated ${queries.length} queries, but will only use the first 4 to avoid rate limits.`
    );
    let logMessage = '[NewsAPI] Generated Queries:\n';
    queries.forEach((q, i) => {
        const inUse = i < 4;
        logMessage += inUse ? colors.green(`  [IN USE] Query ${i+1}: ${q}\n`) : colors.gray(`  [SKIPPED] Query ${i+1}: ${q}\n`);
    });
    getConfig().logger.info(logMessage);
  }

  return queriesToUse;
}

export async function scrapeNewsAPI() {
  const newsapi = new NewsAPI(env.NEWSAPI_API_KEY)
  try {
    const watchlist = await getWatchlist()
    const queryBatches = buildQueryBatches(watchlist)

    getConfig().logger.info(
      `📰 [NewsAPI] Dispatching ${queryBatches.length} batched queries to cover the watchlist.`
    )

    const twentyFourHoursAgo = new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString()

    const allPromises = queryBatches.map((query) =>
      newsapi.v2.everything({
        q: query,
        language: 'en,da,sv,no',
        sortBy: 'publishedAt',
        from: twentyFourHoursAgo,
        pageSize: 100,
      })
    )

    const allResponses = await Promise.all(allPromises)
    let allArticles = []

    for (const response of allResponses) {
      if (response.status !== 'ok') {
        getConfig().logger.error(
          `[NewsAPI] Error in a batch query: ${response.code} - ${response.message}`
        )
        continue
      }
      allArticles.push(...response.articles)
    }

    if (allArticles.length === 0) {
      getConfig().logger.info('[NewsAPI] Found no new articles matching the watchlist.')
      return []
    }

    getConfig().logger.info(
      `[NewsAPI] Found a total of ${allArticles.length} potential articles across all batches.`
    )

    const articles = allArticles.map((a) => ({
      headline: a.title,
      link: a.url,
      source: a.source.name,
      newspaper: a.source.name,
    }))

    return Array.from(new Map(articles.map((a) => [a.link, a])).values())
  } catch (error) {
    if (error.name?.includes('rateLimited')) {
      getConfig().logger.warn(
        '[NewsAPI] Rate limit hit, as expected on developer plan. Some watchlist items may have been missed.'
      )
    } else {
      getConfig().logger.error(
        { err: error },
        '[NewsAPI] A critical error occurred during batched scraping.'
      )
    }
    return []
  }
}

```

## 📄 src/scraper/orchestrator.js
*Lines: 105, Size: 3.44 KB*

```javascript
// packages/scraper-logic/src/scraper/orchestrator.js (version 5.0.0)
import pLimit from 'p-limit'
import { sleep } from '@headlines/utils'
import { getConfig } from '../config.js'
import { scrapeSiteForHeadlines } from './headlineScraper.js'
// NEWSAPI REWORK: The direct import of scrapeNewsAPI is removed as it's no longer used for proactive scraping.
// import { scrapeNewsAPI } from './newsApiScraper.js'
import { updateSourceAnalyticsBatch } from '@headlines/data-access'
import { env } from '@headlines/config'

async function performStandardScraping(sourcesToScrape) {
  if (sourcesToScrape.length === 0) {
    return { scrapedArticles: [], scraperHealth: [] }
  }

  const limit = pLimit(env.CONCURRENCY_LIMIT || 3)
  getConfig().logger.info(
    `Pipeline will now scrape ${sourcesToScrape.length} active standard sources.`
  )

  let allArticles = []
  const scraperHealthMap = new Map()

  const promises = sourcesToScrape.map((source) =>
    limit(async () => {
      getConfig().logger.info(`[Scraping] -> Starting scrape for "${source.name}"...`)
      const result = await scrapeSiteForHeadlines(source)
      const foundCount = result.resultCount !== undefined ? result.resultCount : 0
      getConfig().logger.info(
        `[Scraping] <- Finished scrape for "${source.name}". Success: ${result.success}, Found: ${foundCount}`
      )
      return { source, result }
    })
  )
  const results = await Promise.all(promises)

  const bulkUpdateOps = []

  for (const { source, result } of results) {
    const healthReport = {
      source: source.name,
      success: result.success && result.resultCount > 0,
      count: result.resultCount || 0,
      error: result.error,
      debugHtml: result.debugHtml,
      failedSelector: result.success ? null : source.headlineSelector,
    }
    scraperHealthMap.set(source.name, healthReport)

    if (healthReport.success) {
      allArticles.push(
        ...result.articles.map((a) => ({
          ...a,
          source: source.name,
          newspaper: source.name,
          country: source.country,
        }))
      )
      bulkUpdateOps.push({
        updateOne: {
          filter: { _id: source._id },
          update: { $set: { lastScrapedAt: new Date(), lastSuccessAt: new Date() } },
        },
      })
    } else {
      getConfig().logger.warn(
        `[Scraping] ❌ FAILED for "${source.name}": ${result.error || 'Extracted 0 headlines.'}.`
      )
      bulkUpdateOps.push({
        updateOne: {
          filter: { _id: source._id },
          update: { $set: { lastScrapedAt: new Date() } },
        },
      })
    }
  }

  if (bulkUpdateOps.length > 0) {
    await updateSourceAnalyticsBatch(bulkUpdateOps)
  }

  return {
    scrapedArticles: allArticles,
    scraperHealth: Array.from(scraperHealthMap.values()),
  }
}

// NEWSAPI REWORK: The main orchestrator is simplified. It no longer calls scrapeNewsAPI.
// Its sole responsibility is now to manage the standard scraping process. This aligns
// with the new strategy of using external APIs only for enrichment, not discovery.
export async function scrapeAllHeadlines(sourcesToScrape) {
  const { scrapedArticles, scraperHealth } =
    await performStandardScraping(sourcesToScrape)

  const uniqueArticles = Array.from(
    new Map(scrapedArticles.map((a) => [a.link, a])).values()
  )

  getConfig().logger.info(
    `Scraping complete. Found ${uniqueArticles.length} unique articles from standard sources.`
  )

  return { allArticles: uniqueArticles, scraperHealth }
}

```

## 📄 src/scraper/selectorOptimizer.js
*Lines: 98, Size: 3.19 KB*

```javascript
// packages/scraper-logic/src/scraper/selectorOptimizer.js (version 4.2)
import * as cheerio from 'cheerio';

const NEGATIVE_TAGS = ['nav', 'footer', 'aside', 'header', 'form', '.popup-overlay'];

/**
 * Finds clusters of repeated elements by analyzing class name frequency.
 * CRITICALLY, it filters out Tailwind-style classes with colons.
 */
function findRepeatingClassSelectors($) {
    const classCounts = {};
    $('*').each((_, el) => {
        const classes = $(el).attr('class');
        if (classes) {
            classes.trim().split(/\s+/).forEach(cls => {
                // DEFINITIVE FIX: Ignore any class containing a colon to prevent pseudo-class errors.
                if (cls.length > 5 && !cls.includes(':') && !cls.startsWith('js-')) {
                    classCounts[cls] = (classCounts[cls] || 0) + 1;
                }
            });
        }
    });

    return Object.entries(classCounts)
        .filter(([_, count]) => count > 3 && count < 100)
        .sort((a, b) => b[1] - a[1])
        .slice(0, 15) // Widen the search slightly
        .map(([cls]) => `.${cls}`);
}

/**
 * For a given container element, finds the most likely headline text.
 */
function analyzeContainer($container) {
    const headlineEl = $container.find('h1, h2, h3, h4, h5').first();
    let text = headlineEl.text().trim().replace(/\s+/g, ' ');

    if (!text) {
        // Fallback for non-heading elements
        text = $container.text().trim().replace(/\s+/g, ' ');
    }

    // Ensure it's a clickable container
    const isClickable = $container.is('a[href]') || $container.find('a[href]').length > 0 || $container.find('button[data-key]').length > 0;

    if (text && isClickable) {
        return { text };
    }
    return null;
}

export function heuristicallyFindSelectors(html) {
    const $ = cheerio.load(html);
    $(NEGATIVE_TAGS.join(',')).remove();

    const potentialListSelectors = findRepeatingClassSelectors($);
    const clusters = [];

    // Add the CVC-specific selector as a high-priority candidate, as it is a known good pattern.
    potentialListSelectors.unshift('.portfolio__card-holder');

    for (const selector of potentialListSelectors) {
        try {
            const elements = $(selector);
            if (elements.length < 3) continue;

            const samples = [];
            let validItems = 0;

            elements.each((_, el) => {
                const containerData = analyzeContainer($(el));
                if (containerData) {
                    samples.push(containerData.text);
                    validItems++;
                }
            });

            if (validItems > 2 && (validItems / elements.length) > 0.5) {
                clusters.push({
                    selector: selector,
                    score: validItems * (validItems / elements.length),
                    samples: samples,
                });
            }
        } catch (e) {
            // Silently ignore errors from invalid selectors that might still slip through
        }
    }

    if (clusters.length === 0) {
        return [];
    }
    
    const uniqueClusters = [...new Map(clusters.map(item => [item.selector, item])).values()];

    return uniqueClusters.sort((a, b) => b.score - a.score).slice(0, 5);
}

```

## 📄 src/scraper/test-helpers.js
*Lines: 93, Size: 3.56 KB*

```javascript
// packages/scraper-logic/src/scraper/test-helpers.js (version 2.0)
import * as cheerio from 'cheerio'
import { dynamicExtractor } from './dynamicExtractor.js'
import { extractorRegistry } from './extractors/index.js'
import { fetchPageWithPlaywright } from '../browser.js'

export async function scrapeArticleContentForTest(articleUrl, articleSelectors) {
  if (!articleUrl || !articleSelectors || articleSelectors.length === 0) return ''
  try {
    const html = await fetchPageWithPlaywright(articleUrl, 'TestContentScraper')
    if (!html) return 'Error: Failed to fetch page HTML.';
    
    const $ = cheerio.load(html)
    const selectors = Array.isArray(articleSelectors) ? articleSelectors : [articleSelectors];
    let contentParts = [];
    
    for (const selector of selectors) {
        $(selector).each((_, el) => {
            contentParts.push($(el).text().trim());
        });
    }

    if(contentParts.length > 0) {
        const content = contentParts.join('\\n\\n').replace(/\\s\\s+/g, ' ');
        return content.substring(0, 1000) + (content.length > 1000 ? '...' : '');
    }
    return 'No content found with the provided selectors.';

  } catch (error) {
    console.error(`[Content Scrape Test Error] for ${articleUrl}: ${error.message}`)
    return `Error scraping content: ${error.message}`
  }
}

export async function testHeadlineExtraction(sourceConfig, html) {
  let pageHtml = html
  if (!pageHtml) {
    pageHtml = await fetchPageWithPlaywright(sourceConfig.sectionUrl, 'TestHeadlineScraper')
  }
  const $ = cheerio.load(pageHtml)
  const articles = []
  const selectors = Array.isArray(sourceConfig.headlineSelector) ? sourceConfig.headlineSelector : [sourceConfig.headlineSelector].filter(Boolean);

  for (const selector of selectors) {
    switch (sourceConfig.extractionMethod) {
      case 'json-ld':
        $('script[type="application/ld+json"]').each((_, el) => {
          try {
            const jsonData = JSON.parse($(el).html())
            const potentialLists = [jsonData, ...(jsonData['@graph'] || [])]
            potentialLists.forEach((list) => {
              const items = list?.itemListElement
              if (Array.isArray(items)) {
                items.forEach((item) => {
                  const headline = item.name || item.item?.name
                  const url = item.url || item.item?.url
                  if (headline && url) {
                    articles.push({ headline: headline.trim(), link: new URL(url, sourceConfig.baseUrl).href })
                  }
                })
              }
            })
          } catch (e) {}
        })
        break
      case 'declarative':
        $(selector).each((_, el) => {
          const articleData = dynamicExtractor($, el, sourceConfig)
          if (articleData?.headline && articleData?.link) {
            articleData.link = new URL(articleData.link, sourceConfig.baseUrl).href
            articles.push(articleData)
          }
        })
        break
      case 'custom':
      default:
        const customExtractor = extractorRegistry[sourceConfig.extractorKey]
        if (!customExtractor) {
          throw new Error(`No custom extractor found for key: '${sourceConfig.extractorKey}'`)
        }
        $(selector).each((_, el) => {
          const articleData = customExtractor($(el), sourceConfig)
          if (articleData?.headline && articleData?.link) {
            articleData.link = new URL(articleData.link, sourceConfig.baseUrl).href
            articles.push(articleData)
          }
        })
        break
    }
  }
  return Array.from(new Map(articles.map((a) => [a.link, a])).values())
}

```
